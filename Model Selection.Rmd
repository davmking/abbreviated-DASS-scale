---
title: "Model Selection"
author: "Dav King"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r Libraries and Data}
library(MASS)
library(tidyverse)
library(tidymodels)
library(readr)
library(GGally)
library(corrplot)
library(glmnet)
library(tree)
library(randomForest)
library(gbm)
library(splines)
library(class)
library(FNN)
library(e1071)

grid <- 10^seq(10, -2, length = 100)
data <- read_delim("data.csv", delim = "\t", 
                   escape_double = FALSE, trim_ws = TRUE)
test_thr <- seq(.5, .7, by = .01)
```

# Data Transformations

## Creating Scales

```{r DT Creating Scales}
data <- data %>% 
  mutate(stress_score = Q1A + Q6A + Q8A + Q11A + Q12A + Q14A + Q18A + 
           Q22A + Q27A + Q29A + Q32A + Q33A + Q35A + Q39A,
         anxiety_score = Q2A + Q4A + Q7A + Q9A + Q15A + Q19A + Q20A +
           Q23A + Q25A + Q28A + Q30A + Q36A + Q40A + Q41A,
         depression_score = Q3A + Q5A + Q10A + Q13A + Q16A + Q17A + Q21A + 
           Q24A + Q26A + Q31A + Q34A + Q37A + Q38A + Q42A) %>% 
  mutate(stress_score_reduced = Q1A + Q6A + Q8A + Q12A + Q14A + Q18A + Q22A + 
           Q32A + Q33A + Q35A + Q39A,
         anxiety_score_reduced = Q2A + Q4A + Q7A + Q15A + Q19A + Q20A + Q23A +
           Q25A + Q30A + Q36A + Q41A,
         depression_score_reduced = Q3A + Q5A + Q16A + Q17A + Q24A + Q26A +
           Q31A + Q34A + Q37A + Q38A + Q42A) %>% 
  mutate(stress_score = stress_score - 14, # Original responses are based on 0-3 scale
         anxiety_score = anxiety_score - 14, # Dataset is based on 1-4 scale
         depression_score = depression_score - 14) %>% # 14 questions per scale
  mutate(stress_score_reduced = stress_score_reduced - 11,
         anxiety_score_reduced = anxiety_score_reduced - 11, # Same idea here
         depression_score_reduced = depression_score_reduced - 11) %>% 
  mutate(anxiety_trans = sqrt(anxiety_score),
         anxiety_trans_reduced = sqrt(anxiety_score_reduced)) %>% 
  mutate(stress_cat = case_when(
    stress_score <= 14 ~ "Normal",
    stress_score <= 18 ~ "Mild",
    stress_score <= 25 ~ "Moderate",
    stress_score <= 33 ~ "Severe",
    T ~ "Extremely Severe"
  ), anxiety_cat = case_when(
    anxiety_score <= 7 ~ "Normal",
    anxiety_score <= 9 ~ "Mild",
    anxiety_score <= 14 ~ "Moderate",
    anxiety_score <= 19 ~ "Severe",
    T ~ "Extremely Severe"
  ), depression_cat = case_when(
    depression_score <= 9 ~ "Normal",
    depression_score <= 13 ~ "Mild",
    depression_score <= 20 ~ "Moderate",
    depression_score <= 27 ~ "Severe",
    T ~ "Extremely Severe"
  )) %>% 
  mutate(stress_cat = factor(stress_cat,
                             levels = c("Normal", "Mild", "Moderate",
                                        "Severe", "Extremely Severe")),
         anxiety_cat = factor(anxiety_cat,
                              levels = c("Normal", "Mild", "Moderate",
                                        "Severe", "Extremely Severe")),
         depression_cat = factor(depression_cat,
                                 levels = c("Normal", "Mild", "Moderate",
                                        "Severe", "Extremely Severe"))) %>%
  mutate(stress_bin = factor(if_else(stress_score > 18, "High", "Low")),
         anxiety_bin = factor(if_else(anxiety_score > 9, "High", "Low")),
         depression_bin = factor(if_else(depression_score > 13, "High", "Low"))) %>% 
  mutate(TIPI4 = if_else(TIPI4 == 0, NA, TIPI4)) %>% 
  mutate(TIPI4 = if_else(is.na(TIPI4), 4, TIPI4)) # Justification: A) this is a very small subset of the data, and it allows us to use this data instead of it being missing, B) TIPI4 is already our weakest predictor anyway, and C) "neither agree nor disagree" seems to be a reasonable, neutral midpoint to impute

data %>% 
  select(stress_bin) %>% 
  group_by(stress_bin) %>% 
  count()

data %>% 
  select(anxiety_bin) %>% 
  group_by(anxiety_bin) %>% 
  count()

data %>% 
  select(depression_bin) %>% 
  group_by(depression_bin) %>% 
  count()
```

These counts are high, but we'll stick with it.

## BoxCox Transformations

```{r DT BoxCox}
stress_boxcox <- boxcox(lm(data$stress_score[data$stress_score > 0] ~ 1))
best_stress <- stress_boxcox$x[which.max(stress_boxcox$y)]
data <- data %>% 
  mutate(stress_trans = (stress_score^best_stress - 1) / best_stress)

data <- data %>% 
  mutate(anxiety_trans = sqrt(anxiety_score))

depression_boxcox <- boxcox(lm(data$depression_score[data$depression_score > 0] ~ 1))
best_depression <- depression_boxcox$x[which.max(depression_boxcox$y)]
data <- data %>% 
  mutate(depression_trans = (depression_score^best_depression - 1) / best_depression) %>% 
  mutate(depression_trans_2 = sqrt(depression_score))
```

stress_t = stress_score^best_s - 1 / best_s
stress_t * best_s = stress_score^best_s - 1
stress_t * best_s + 1 = stress_score^best_s
stress_score = (stress_t * best_s + 1)^(1 / best_s)

# Splitting Data

```{r Data Splits}
set.seed(322)
split <- initial_split(data, prop = .75)
trainData <- training(split)
testData <- testing(split)
training_folds <- vfold_cv(trainData, v = 10)
```

# Selecting Predictor Subset

```{r Final Predictors}
trainSubset <- trainData %>% 
  select(stress_score, stress_trans, stress_bin, anxiety_score, anxiety_trans,
         anxiety_bin, depression_score, depression_trans, depression_bin,
         stress_score_reduced, anxiety_score_reduced, depression_score_reduced,
         Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4,
         anxiety_trans_reduced)
```

# EDA

## Predictors

```{r EDA Predictors}
plot(density(trainSubset$Q11A)) # 4 is the highest, 1 smallest, 2-3 about identical
plot(density(trainSubset$Q27A)) # 2 is by far the highest, still more mass on right
plot(density(trainSubset$Q29A)) # Same
plot(density(trainSubset$Q9A)) # 2 and 4 are about the same peaks, 3 higher than 1
plot(density(trainSubset$Q28A)) # Heavy right skew - 1-2 identical, much higher than 3-4 identical
plot(density(trainSubset$Q40A)) # 4 and then 2
plot(density(trainSubset$Q10A)) # Everything high/ident except 3
plot(density(trainSubset$Q13A)) # Left-skew
plot(density(trainSubset$Q21A)) # Most mass at 1
plot(density(na.omit(trainSubset$TIPI4))) # Heavily skews right
plot(density(na.omit(trainSubset$TIPI4^2))) # Not much better

boxcox(lm(trainSubset$TIPI4[trainSubset$TIPI4 > 0] ~ 1)) # Does suggest x^2
```


## Stress

```{r EDA Stress}
scatter.smooth(trainSubset$Q11A, trainSubset$stress_score) # More or less linear
scatter.smooth(trainSubset$Q27A, trainSubset$stress_score) # Same
scatter.smooth(trainSubset$Q29A, trainSubset$stress_score) # Lil wavy
scatter.smooth(trainSubset$Q9A, trainSubset$stress_score) # About the same
scatter.smooth(trainSubset$Q28A, trainSubset$stress_score) # About the same
scatter.smooth(trainSubset$Q40A, trainSubset$stress_score) # Same
scatter.smooth(trainSubset$Q10A, trainSubset$stress_score) # Same
scatter.smooth(trainSubset$Q13A, trainSubset$stress_score) # Same
scatter.smooth(trainSubset$Q21A, trainSubset$stress_score) # Same
scatter.smooth(trainSubset$TIPI4, trainSubset$stress_score) # Maybe exponential relationship?
scatter.smooth(trainSubset$TIPI4^2, trainSubset$stress_score) # Almost perfectly linear

trainSubset %>% 
  select(starts_with("Q"), TIPI4, stress_score) %>% 
  na.omit() %>% 
  cor() %>% 
  corrplot.mixed(., lower = "circle", upper = "number")
# It is, obviously, the most correlated with its own predictors
# However it is also pretty well correlated with everything
# Other highest are Q28A and Q13A at 0.66
```

```{r EDA Stress Trans}
scatter.smooth(trainSubset$Q11A, trainSubset$stress_trans) # More or less linear
scatter.smooth(trainSubset$Q27A, trainSubset$stress_trans) # Same
scatter.smooth(trainSubset$Q29A, trainSubset$stress_trans) # Lil wavy
scatter.smooth(trainSubset$Q9A, trainSubset$stress_trans) # About the same
scatter.smooth(trainSubset$Q28A, trainSubset$stress_trans) # About the same
scatter.smooth(trainSubset$Q40A, trainSubset$stress_trans) # Same
scatter.smooth(trainSubset$Q10A, trainSubset$stress_trans) # Same
scatter.smooth(trainSubset$Q13A, trainSubset$stress_trans) # Same
scatter.smooth(trainSubset$Q21A, trainSubset$stress_trans) # Same
scatter.smooth(trainSubset$TIPI4, trainSubset$stress_trans) # Maybe exponential relationship?
scatter.smooth(trainSubset$TIPI4^2, trainSubset$stress_trans) # Almost perfectly linear

trainSubset %>% 
  select(starts_with("Q"), TIPI4, stress_trans) %>% 
  na.omit() %>% 
  cor() %>% 
  corrplot.mixed(., lower = "circle", upper = "number")
# It is, obviously, the most correlated with its own predictors
# However it is also pretty well correlated with everything
# Other highest is Q13A at 0.67
```

## Anxiety

```{r EDA Anxiety}
scatter.smooth(trainSubset$Q11A, trainSubset$anxiety_score) # More or less linear
scatter.smooth(trainSubset$Q27A, trainSubset$anxiety_score) # Same
scatter.smooth(trainSubset$Q29A, trainSubset$anxiety_score) # Lil wavy
scatter.smooth(trainSubset$Q9A, trainSubset$anxiety_score) # About the same
scatter.smooth(trainSubset$Q28A, trainSubset$anxiety_score) # About the same
scatter.smooth(trainSubset$Q40A, trainSubset$anxiety_score) # Same
scatter.smooth(trainSubset$Q10A, trainSubset$anxiety_score) # Same
scatter.smooth(trainSubset$Q13A, trainSubset$anxiety_score) # Same
scatter.smooth(trainSubset$Q21A, trainSubset$anxiety_score) # Same
scatter.smooth(trainSubset$TIPI4, trainSubset$anxiety_score) # Maybe exponential relationship?
scatter.smooth(trainSubset$TIPI4^2, trainSubset$anxiety_score) # Almost perfectly linear

trainSubset %>% 
  select(starts_with("Q"), TIPI4, anxiety_score) %>% 
  na.omit() %>% 
  cor() %>% 
  corrplot.mixed(., lower = "circle", upper = "number")
# It is, obviously, the most correlated with its own predictors
# However it is also pretty well correlated with everything (less than stress was)
# Other highest is Q29A at 0.61
```

```{r EDA Anxiety Trans}
scatter.smooth(trainSubset$Q11A, trainSubset$anxiety_trans) # More or less linear
scatter.smooth(trainSubset$Q27A, trainSubset$anxiety_trans) # Same
scatter.smooth(trainSubset$Q29A, trainSubset$anxiety_trans) # Lil wavy
scatter.smooth(trainSubset$Q9A, trainSubset$anxiety_trans) # About the same
scatter.smooth(trainSubset$Q28A, trainSubset$anxiety_trans) # About the same
scatter.smooth(trainSubset$Q40A, trainSubset$anxiety_trans) # Same
scatter.smooth(trainSubset$Q10A, trainSubset$anxiety_trans) # Same
scatter.smooth(trainSubset$Q13A, trainSubset$anxiety_trans) # Same
scatter.smooth(trainSubset$Q21A, trainSubset$anxiety_trans) # Same
scatter.smooth(trainSubset$TIPI4, trainSubset$anxiety_trans) # Maybe exponential relationship?
scatter.smooth(trainSubset$TIPI4^2, trainSubset$anxiety_trans) # Almost perfectly linear

trainSubset %>% 
  select(starts_with("Q"), TIPI4, anxiety_trans) %>% 
  na.omit() %>% 
  cor() %>% 
  corrplot.mixed(., lower = "circle", upper = "number")
# It is, obviously, the most correlated with its own predictors
# However it is also pretty well correlated with everything (less than stress was)
# Other highest is Q29A at 0.62
```

## Depression

```{r EDA Depression}
scatter.smooth(trainSubset$Q11A, trainSubset$depression_score) # More or less linear
scatter.smooth(trainSubset$Q27A, trainSubset$depression_score) # Same
scatter.smooth(trainSubset$Q29A, trainSubset$depression_score) # Lil wavy
scatter.smooth(trainSubset$Q9A, trainSubset$depression_score) # About the same
scatter.smooth(trainSubset$Q28A, trainSubset$depression_score) # About the same
scatter.smooth(trainSubset$Q40A, trainSubset$depression_score) # Same
scatter.smooth(trainSubset$Q10A, trainSubset$depression_score) # Same
scatter.smooth(trainSubset$Q13A, trainSubset$depression_score) # Same
scatter.smooth(trainSubset$Q21A, trainSubset$depression_score) # Same
scatter.smooth(trainSubset$TIPI4, trainSubset$depression_score) # Maybe exponential relationship?
scatter.smooth(trainSubset$TIPI4^2, trainSubset$depression_score) # Almost perfectly linear

trainSubset %>% 
  select(starts_with("Q"), TIPI4, depression_score) %>% 
  na.omit() %>% 
  cor() %>% 
  corrplot.mixed(., lower = "circle", upper = "number")
# It is, obviously, the most correlated with its own predictors (more so than the others were)
# However it is also pretty well correlated with everything (less than stress/anx were)
# Other highest is Q11A at 0.60
```

```{r EDA Depression Trans}
scatter.smooth(trainSubset$Q11A, trainSubset$depression_trans) # More or less linear
scatter.smooth(trainSubset$Q27A, trainSubset$depression_trans) # Same
scatter.smooth(trainSubset$Q29A, trainSubset$depression_trans) # Lil wavy
scatter.smooth(trainSubset$Q9A, trainSubset$depression_trans) # About the same
scatter.smooth(trainSubset$Q28A, trainSubset$depression_trans) # About the same
scatter.smooth(trainSubset$Q40A, trainSubset$depression_trans) # Same
scatter.smooth(trainSubset$Q10A, trainSubset$depression_trans) # Notably bad
scatter.smooth(trainSubset$Q13A, trainSubset$depression_trans) # Same as others were
scatter.smooth(trainSubset$Q21A, trainSubset$depression_trans) # Same
scatter.smooth(trainSubset$TIPI4, trainSubset$depression_trans) # Maybe exponential relationship?
scatter.smooth(trainSubset$TIPI4^2, trainSubset$depression_trans) # Almost perfectly linear

trainSubset %>% 
  select(starts_with("Q"), TIPI4, depression_trans) %>% 
  na.omit() %>% 
  cor() %>% 
  corrplot.mixed(., lower = "circle", upper = "number")
# It is, obviously, the most correlated with its own predictors (more so than the others were)
# However it is also pretty well correlated with everything (less than stress/anx were)
# Other highest is Q11A at 0.61
```

# Linear Models

## Stress

```{r LM Stress}
summary(lm(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset))
# All sig, largest coefs are (obviously) own predictors
# RSE = 3.771
# Rsq = 0.871, adj rsq = 0.8709, F(10, 29473) = 19900, p < .001
plot(lm(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset))
# Aside from the trends that are still present in the scale-location, this is actually shockingly not that bad
```

```{r LM Stress Int}
summary(lm(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2),
           trainSubset[!is.na(trainSubset$TIPI4),]))
# All sig, huge coefs for polynomial (not huge t-values tho)
# RSE = 3.763
# Rsq = 0.8715 = adj rsq, F(11, 29472) = 18180, p < .001
plot(lm(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2),
           trainSubset[!is.na(trainSubset$TIPI4),]))
# Probably a little worse but not substantially so
```

```{r LM Stress Trans}
summary(lm(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset))
# All sig, largest coefs are (obviously) own predictors
# RSE = 2.023
# Rsq = 0.8733, adj rsq = 0.8732, F(10, 29473) = 20310, p < .001
plot(lm(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset))
# About the same I guess?
```

```{r LM Stress Trans Int}
summary(lm(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2),
           trainSubset[!is.na(trainSubset$TIPI4),]))
# Higher order effect is not significant
# RSE = 2.023
# Rsq = 0.8733, adj rsq = 0.8732, F(11, 29472) = 18460, p < .001
plot(lm(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2),
           trainSubset[!is.na(trainSubset$TIPI4),]))
# Probably a little worse but not substantially so
```


## Anxiety

```{r LM Anxiety}
summary(lm(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset))
# All sig except TIPI4, largest coefs are (obviously) own predictors
# RSE = 4.713
# Rsq = 0.7879, adj rsq = 0.7878, F(10, 29473) = 10950, p < .001
plot(lm(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset))
# Not as good as the stress diagnostics had been - the resid vs fitted has patterns now
```

```{r LM Anxiety Int}
summary(lm(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2),
           trainSubset[!is.na(trainSubset$TIPI4),]))
# All sig, huge coefs for polynomial (not huge t-values tho)
# RSE = 4.706
# Rsq = 0.7886, adj rsq = 0.7885, F(11, 29472) = 9994, p < .001
plot(lm(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2),
           trainSubset[!is.na(trainSubset$TIPI4),]))
# About the same
```

```{r LM Anxiety Trans}
summary(lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset))
# All sig, largest coefs are (obviously) own predictors
# RSE = 0.6509
# Rsq = 0.7925, adj rsq = 0.7924, F(10, 29473) = 11260, p < .001
plot(lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset))
# Probably a little worse here
```

```{r LM Anxiety Trans Int}
summary(lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2),
           trainSubset[!is.na(trainSubset$TIPI4),]))
# All sig
# RSE = 0.6493
# Rsq = 0.7935, adj rsq = 0.7934, F(11, 29472) = 10300, p < .001
plot(lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2),
           trainSubset[!is.na(trainSubset$TIPI4),]))
# About the same
```


## Depression

```{r LM Depression}
summary(lm(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset))
# All sig, largest coefs are (obviously) own predictors, TIPI4 actually negative predictor
# RSE = 3.803
# Rsq = 0.9046, adj rsq = 0.9045, F(10, 29473) = 27940, p < .001
# This one being the best model was not on my bingo card
plot(lm(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset))
# About the same as stress - not the dream, but we can work with it
```

```{r LM Depression Int}
summary(lm(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2),
           trainSubset[!is.na(trainSubset$TIPI4),]))
# Higher order effect is not significant
# RSE = 3.804
# Rsq = 0.9046, adj rsq = 0.9045, F(11, 29472) = 25400, p < .001
plot(lm(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2),
           trainSubset[!is.na(trainSubset$TIPI4),]))
# About the same
```

```{r LM Depression Trans}
summary(lm(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset))
# All sig, largest coefs are (obviously) own predictors
# RSE = 1.76
# Rsq = 0.8936, adj rsq = 0.8936, F(10, 29473) = 24760, p < .001
plot(lm(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset))
# Probably a little worse here
```

```{r LM Depression Trans Int}
summary(lm(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2),
           trainSubset[!is.na(trainSubset$TIPI4),]))
# All sig
# RSE = 1.756
# Rsq = 0.8941, adj rsq = 0.8941, F(11, 29472) = 22620, p < .001
plot(lm(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2),
           trainSubset[!is.na(trainSubset$TIPI4),]))
# About the same
```

# Shrinkage Models

## Stress

```{r Shrinkage Stress}
str_score <- trainSubset$stress_score[!is.na(trainSubset$TIPI4)]
str_t_score <- trainSubset$stress_trans[!is.na(trainSubset$TIPI4)]
stress_pred <- model.matrix(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
stress_pred_int <- model.matrix(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 2),
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
stress_t_pred <- model.matrix(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
stress_t_pred_int <- model.matrix(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 2),
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
```

### Ridge

```{r Shrinkage Stress Ridge}
set.seed(1)
stress_ridge_cv <- cv.glmnet(stress_pred, str_score, alpha = 0)
stress_ridge_bestlam <- stress_ridge_cv$lambda.min
stress_ridge_bestlam

stress_ridge <- glmnet(stress_pred, str_score, alpha = 0,
                       lambda = stress_ridge_bestlam)
coef(stress_ridge)
# Biggest: Q11A, Q27A, Q29A
# Smallest: Q10A, Q21A, TIPI4
```

```{r Shrinkage Stress Ridge Int}
set.seed(1)
stress_ridge_int_cv <- cv.glmnet(stress_pred_int, str_score, alpha = 0)
stress_ridge_int_bestlam <- stress_ridge_int_cv$lambda.min
stress_ridge_int_bestlam

stress_ridge_int <- glmnet(stress_pred_int, str_score, alpha = 0,
                       lambda = stress_ridge_int_bestlam)
coef(stress_ridge_int)
# Intriguingly, it doesn't shrink the massive polynomial coefficients much
```

```{r Shrinkage Stress Ridge Trans}
set.seed(1)
stress_t_ridge_cv <- cv.glmnet(stress_t_pred, str_t_score, alpha = 0)
stress_t_ridge_bestlam <- stress_t_ridge_cv$lambda.min
stress_t_ridge_bestlam

stress_t_ridge <- glmnet(stress_t_pred, str_t_score, alpha = 0,
                       lambda = stress_t_ridge_bestlam)
coef(stress_t_ridge)
# Biggest: Q11A, Q27A, Q29A
# Smallest: Q10A, Q21A, TIPI4
```

```{r Shrinkage Stress Ridge Trans Int}
set.seed(1)
stress_t_ridge_int_cv <- cv.glmnet(stress_t_pred_int, str_t_score, alpha = 0)
stress_t_ridge_int_bestlam <- stress_t_ridge_int_cv$lambda.min
stress_t_ridge_int_bestlam

stress_t_ridge_int <- glmnet(stress_t_pred_int, str_t_score, alpha = 0,
                       lambda = stress_t_ridge_int_bestlam)
coef(stress_t_ridge_int)
# Intriguingly, it doesn't shrink the massive polynomial coefficients much
```

### Lasso

```{r Shrinkage Stress Lasso}
set.seed(1)
stress_lasso_cv <- cv.glmnet(stress_pred, str_score, alpha = 1)
stress_lasso_bestlam <- stress_lasso_cv$lambda.min
stress_lasso_bestlam

stress_lasso <- glmnet(stress_pred, str_score, alpha = 1,
                       lambda = stress_lasso_bestlam)
coef(stress_lasso)
# Biggest: Q11A, Q27A, Q29A
# Smallest: Q10A, Q21A, TIPI4
# Nothing Eliminated
```

```{r Shrinkage Stress Lasso Int}
set.seed(1)
stress_lasso_int_cv <- cv.glmnet(stress_pred_int, str_score, alpha = 1)
stress_lasso_int_bestlam <- stress_lasso_int_cv$lambda.min
stress_lasso_int_bestlam

stress_lasso_int <- glmnet(stress_pred_int, str_score, alpha = 1,
                       lambda = stress_lasso_int_bestlam)
coef(stress_lasso_int)
# Intriguingly, it doesn't shrink the massive polynomial coefficients much
```

```{r Shrinkage Stress Lasso Trans}
set.seed(1)
stress_t_lasso_cv <- cv.glmnet(stress_t_pred, str_t_score, alpha = 1)
stress_t_lasso_bestlam <- stress_t_lasso_cv$lambda.min
stress_t_lasso_bestlam

stress_t_lasso <- glmnet(stress_t_pred, str_t_score, alpha = 1,
                       lambda = stress_t_lasso_bestlam)
coef(stress_t_lasso)
# Biggest: Q11A, Q27A, Q29A
# Smallest: Q10A, Q21A, TIPI4
# Again, no loss of variables
```

```{r Shrinkage Stress Lasso Trans Int}
set.seed(1)
stress_t_lasso_int_cv <- cv.glmnet(stress_t_pred_int, str_t_score, alpha = 1)
stress_t_lasso_int_bestlam <- stress_t_lasso_int_cv$lambda.min
stress_t_lasso_int_bestlam

stress_t_lasso_int <- glmnet(stress_t_pred_int, str_t_score, alpha = 1,
                       lambda = stress_t_lasso_int_bestlam)
coef(stress_t_lasso_int)
# Eliminates higher-order effect
```


## Anxiety

```{r Shrinkage Anxiety}
anx_score <- trainSubset$anxiety_score[!is.na(trainSubset$TIPI4)]
anx_t_score <- trainSubset$anxiety_trans[!is.na(trainSubset$TIPI4)]
anx_pred <- model.matrix(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
anx_pred_int <- model.matrix(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 2),
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
anx_t_pred <- model.matrix(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
anx_t_pred_int <- model.matrix(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 2),
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
```

### Ridge

```{r Shrinkage Anx Ridge}
set.seed(1)
anx_ridge_cv <- cv.glmnet(anx_pred, anx_score, alpha = 0)
anx_ridge_bestlam <- anx_ridge_cv$lambda.min
anx_ridge_bestlam

anx_ridge <- glmnet(anx_pred, anx_score, alpha = 0,
                       lambda = anx_ridge_bestlam)
coef(anx_ridge)
# Biggest: Q9A, Q28A, Q30A
# Smallest: Q11A, Q10A, TIPI4
```

```{r Shrinkage Anx Ridge Int}
set.seed(1)
anx_ridge_int_cv <- cv.glmnet(anx_pred_int, anx_score, alpha = 0)
anx_ridge_int_bestlam <- anx_ridge_int_cv$lambda.min
anx_ridge_int_bestlam

anx_ridge_int <- glmnet(anx_pred_int, anx_score, alpha = 0,
                       lambda = anx_ridge_int_bestlam)
coef(anx_ridge_int)
# Intriguingly, it doesn't shrink the massive polynomial coefficients much
```

```{r Shrinkage Anx Ridge Trans}
set.seed(1)
anx_t_ridge_cv <- cv.glmnet(anx_t_pred, anx_t_score, alpha = 0)
anx_t_ridge_bestlam <- anx_t_ridge_cv$lambda.min
anx_t_ridge_bestlam

anx_t_ridge <- glmnet(anx_t_pred, anx_t_score, alpha = 0,
                       lambda = anx_t_ridge_bestlam)
coef(anx_t_ridge)
# Biggest: Q9A, Q28A, Q40A
# Smallest: Q11A, Q10A, TIPI4
```

```{r Shrinkage Anx Ridge Trans Int}
set.seed(1)
anx_t_ridge_int_cv <- cv.glmnet(anx_t_pred_int, anx_t_score, alpha = 0)
anx_t_ridge_int_bestlam <- anx_t_ridge_int_cv$lambda.min
anx_t_ridge_int_bestlam

anx_t_ridge_int <- glmnet(anx_t_pred_int, anx_t_score, alpha = 0,
                       lambda = anx_t_ridge_int_bestlam)
coef(anx_t_ridge_int)
# Intriguingly, it doesn't shrink the massive polynomial coefficients much
```

### Lasso

```{r Shrinkage Anx Lasso}
set.seed(1)
anx_lasso_cv <- cv.glmnet(anx_pred, anx_score, alpha = 1)
anx_lasso_bestlam <- anx_lasso_cv$lambda.min
anx_lasso_bestlam

anx_lasso <- glmnet(anx_pred, anx_score, alpha = 1,
                       lambda = anx_lasso_bestlam)
coef(anx_lasso)
# Biggest: Q9A, Q28A, Q40A
# Smallest: Q10A, Q11A, TIPI4
# Nothing Eliminated, but damn if TIPI4 isn't close
```

```{r Shrinkage Anx Lasso Int}
set.seed(1)
anx_lasso_int_cv <- cv.glmnet(anx_pred_int, anx_score, alpha = 1)
anx_lasso_int_bestlam <- anx_lasso_int_cv$lambda.min
anx_lasso_int_bestlam

anx_lasso_int <- glmnet(anx_pred_int, anx_score, alpha = 1,
                       lambda = anx_lasso_int_bestlam)
coef(anx_lasso_int)
# Intriguingly, it doesn't shrink the massive polynomial coefficients much
```

```{r Shrinkage Anx Lasso Trans}
set.seed(1)
anx_t_lasso_cv <- cv.glmnet(anx_t_pred, anx_t_score, alpha = 1)
anx_t_lasso_bestlam <- anx_t_lasso_cv$lambda.min
anx_t_lasso_bestlam

anx_t_lasso <- glmnet(anx_t_pred, anx_t_score, alpha = 1,
                       lambda = anx_t_lasso_bestlam)
coef(anx_t_lasso)
# Biggest: Q9A, Q28A, Q40A
# Smallest: Q10A, Q11A, TIPI4
# Again, no loss of variables
```

```{r Shrinkage Anx Lasso Trans Int}
set.seed(1)
anx_t_lasso_int_cv <- cv.glmnet(anx_t_pred_int, anx_t_score, alpha = 1)
anx_t_lasso_int_bestlam <- anx_t_lasso_int_cv$lambda.min
anx_t_lasso_int_bestlam

anx_t_lasso_int <- glmnet(anx_t_pred_int, anx_t_score, alpha = 1,
                       lambda = anx_t_lasso_int_bestlam)
coef(anx_t_lasso_int)
# Tiny lambda, no elimination
```


## Depression

```{r Shrinkage Depression}
dep_score <- trainSubset$depression_score[!is.na(trainSubset$TIPI4)]
dep_t_score <- trainSubset$depression_trans[!is.na(trainSubset$TIPI4)]
dep_pred <- model.matrix(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
dep_pred_int <- model.matrix(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 2),
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
dep_t_pred <- model.matrix(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
dep_t_pred_int <- model.matrix(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 2),
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
```

### Ridge

```{r Shrinkage Dep Ridge}
set.seed(1)
dep_ridge_cv <- cv.glmnet(dep_pred, dep_score, alpha = 0)
dep_ridge_bestlam <- dep_ridge_cv$lambda.min
dep_ridge_bestlam

dep_ridge <- glmnet(dep_pred, dep_score, alpha = 0,
                       lambda = dep_ridge_bestlam)
coef(dep_ridge)
# Biggest: Q10A, Q13A, Q21A
# Smallest: All 3 anxiety vars, TIPI4
```

```{r Shrinkage Dep Ridge Int}
set.seed(1)
dep_ridge_int_cv <- cv.glmnet(dep_pred_int, dep_score, alpha = 0)
dep_ridge_int_bestlam <- dep_ridge_int_cv$lambda.min
dep_ridge_int_bestlam

dep_ridge_int <- glmnet(dep_pred_int, dep_score, alpha = 0,
                       lambda = dep_ridge_int_bestlam)
coef(dep_ridge_int)
# Intriguingly, it doesn't shrink the massive polynomial coefficients much
```

```{r Shrinkage Dep Ridge Trans}
set.seed(1)
dep_t_ridge_cv <- cv.glmnet(dep_t_pred, dep_t_score, alpha = 0)
dep_t_ridge_bestlam <- dep_t_ridge_cv$lambda.min
dep_t_ridge_bestlam

dep_t_ridge <- glmnet(dep_t_pred, dep_t_score, alpha = 0,
                       lambda = dep_t_ridge_bestlam)
coef(dep_t_ridge)
# Biggest: Q10A, Q13A, Q21A
# Smallest: All anxiety vars, TIPI4
```

```{r Shrinkage Dep Ridge Trans Int}
set.seed(1)
dep_t_ridge_int_cv <- cv.glmnet(dep_t_pred_int, dep_t_score, alpha = 0)
dep_t_ridge_int_bestlam <- dep_t_ridge_int_cv$lambda.min
dep_t_ridge_int_bestlam

dep_t_ridge_int <- glmnet(dep_t_pred_int, dep_t_score, alpha = 0,
                       lambda = dep_t_ridge_int_bestlam)
coef(dep_t_ridge_int)
# Intriguingly, it doesn't shrink the massive polynomial coefficients much
```

### Lasso

```{r Shrinkage Dep Lasso}
set.seed(1)
dep_lasso_cv <- cv.glmnet(dep_pred, dep_score, alpha = 1)
dep_lasso_bestlam <- dep_lasso_cv$lambda.min
dep_lasso_bestlam

dep_lasso <- glmnet(dep_pred, dep_score, alpha = 1,
                       lambda = dep_lasso_bestlam)
coef(dep_lasso)
# Biggest: Q10A, Q13A, Q21A
# Smallest: Q11A, Q28A, Q40A
# TIPI4 eliminated despite very small lambda
```

```{r Shrinkage Dep Lasso Int}
set.seed(1)
dep_lasso_int_cv <- cv.glmnet(dep_pred_int, dep_score, alpha = 1)
dep_lasso_int_bestlam <- dep_lasso_int_cv$lambda.min
dep_lasso_int_bestlam

dep_lasso_int <- glmnet(dep_pred_int, dep_score, alpha = 1,
                       lambda = dep_lasso_int_bestlam)
coef(dep_lasso_int)
# Eliminates both TIPI vars
```

```{r Shrinkage Dep Lasso Trans}
set.seed(1)
dep_t_lasso_cv <- cv.glmnet(dep_t_pred, dep_t_score, alpha = 1)
dep_t_lasso_bestlam <- dep_t_lasso_cv$lambda.min
dep_t_lasso_bestlam

dep_t_lasso <- glmnet(dep_t_pred, dep_t_score, alpha = 1,
                       lambda = dep_t_lasso_bestlam)
coef(dep_t_lasso)
# Biggest: Q10A, Q13A, Q21A
# Smallest: Q28A, TIPI4
# Again, no loss of variables
```

```{r Shrinkage Dep Lasso Trans Int}
set.seed(1)
dep_t_lasso_int_cv <- cv.glmnet(dep_t_pred_int, dep_t_score, alpha = 1)
dep_t_lasso_int_bestlam <- dep_t_lasso_int_cv$lambda.min
dep_t_lasso_int_bestlam

dep_t_lasso_int <- glmnet(dep_t_pred_int, dep_t_score, alpha = 1,
                       lambda = dep_t_lasso_int_bestlam)
coef(dep_t_lasso_int)
# Tiny lambda, no elimination
```

Main takeaway: fortunately, there seems to be very little evidence for any variable elimination in this modeling. This is good! Also expected - we don't really have enough information to predict everything. If we threw more variables at it, more would stick. Goal is simplicity, not necessarily prediction.

# Nonlinear & Interactions

## Stress

### Interactions

```{r Nonlin Stress Int}
summary(lm(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q11A*TIPI4, trainSubset)) # Sig
summary(lm(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q27A*TIPI4, trainSubset)) # Sig
summary(lm(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q29A*TIPI4, trainSubset)) # Sig
summary(lm(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q9A*TIPI4, trainSubset)) # Sig
summary(lm(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q28A*TIPI4, trainSubset)) # Sig
summary(lm(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q40A*TIPI4, trainSubset)) # Sig
summary(lm(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q10A*TIPI4, trainSubset)) # Sig, makes Q10A not sig
summary(lm(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q13A*TIPI4, trainSubset)) # Sig, but barely
summary(lm(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q21A*TIPI4, trainSubset)) # Sig, makes Q21A not sig

summary(lm(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2) + Q11A*TIPI4 + Q27A*TIPI4 +
             Q29A*TIPI4 + Q9A*TIPI4 + Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 +
             Q13A*TIPI4 + Q21A*TIPI4, trainSubset[!is.na(trainSubset$TIPI4),])) 
# R squared is not meaningfully better than model without interactions, defined above
# Not sig vars: Q21
# Not sig interactions: Q29A, Q9A, Q28A, Q40A, Q10A
```

```{r Nonlin Stress Trans Int}
summary(lm(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q11A*TIPI4, trainSubset)) # Sig
summary(lm(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q27A*TIPI4, trainSubset)) # Sig
summary(lm(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q29A*TIPI4, trainSubset)) # Sig
summary(lm(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q9A*TIPI4, trainSubset)) # Sig
summary(lm(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q28A*TIPI4, trainSubset)) # Sig
summary(lm(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q40A*TIPI4, trainSubset)) # Sig
summary(lm(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q10A*TIPI4, trainSubset)) # Sig
summary(lm(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q13A*TIPI4, trainSubset)) # Sig
summary(lm(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q21A*TIPI4, trainSubset)) # Sig

summary(lm(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2) + Q11A*TIPI4 + Q27A*TIPI4 +
             Q29A*TIPI4 + Q9A*TIPI4 + Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 +
             Q13A*TIPI4 + Q21A*TIPI4, trainSubset[!is.na(trainSubset$TIPI4),]))  
# R squared is substantially better in this case, and is now outperforming non-transformed by about .003 in rsq
# Not sig vars: Q21A
# Not sig interactions: Q29A, Q9A, Q28A, Q40A, Q10A
```

```{r Nonlin Stress Int Lasso}
str_score <- trainSubset$stress_score[!is.na(trainSubset$TIPI4)]
str_t_score <- trainSubset$stress_trans[!is.na(trainSubset$TIPI4)]
str_pred <- model.matrix(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4 +
                           Q11A*TIPI4 + Q27A*TIPI4 + Q29A*TIPI4 + Q9A*TIPI4 +
                           Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 + Q13A*TIPI4 + 
                           Q21A*TIPI4, 
                         trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
str_pred_int <- model.matrix(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 2) +
                           Q11A*TIPI4 + Q27A*TIPI4 + Q29A*TIPI4 + Q9A*TIPI4 +
                           Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 + Q13A*TIPI4 + 
                           Q21A*TIPI4, 
                         trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
str_t_pred <- model.matrix(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4 +
                             Q11A*TIPI4 + Q27A*TIPI4 + Q29A*TIPI4 + Q9A*TIPI4 +
                             Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 + Q13A*TIPI4 +
                             Q21A*TIPI4,
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
str_t_pred_int <- model.matrix(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 2) +
                             Q11A*TIPI4 + Q27A*TIPI4 + Q29A*TIPI4 + Q9A*TIPI4 +
                             Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 + Q13A*TIPI4 +
                             Q21A*TIPI4,
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
```

```{r Nonlin Stress Int Lasso Modeling}
set.seed(1)
str_int_lasso_cv <- cv.glmnet(str_pred, str_score, alpha = 1)
str_int_lasso_bestlam <- str_int_lasso_cv$lambda.min
str_int_lasso_bestlam

str_int_lasso <- glmnet(str_pred, str_score, alpha = 1,
                       lambda = str_int_lasso_bestlam)
coef(str_int_lasso)
# Zero: TIPI4, Q11A*TIPI4, Q40A*TIPI4, Q13A*TIPI4

set.seed(1)
str_int_lasso_cv_2 <- cv.glmnet(str_pred_int, str_score, alpha = 1)
str_int_lasso_bestlam_2 <- str_int_lasso_cv_2$lambda.min
str_int_lasso_bestlam_2

str_int_lasso_2 <- glmnet(str_pred_int, str_score, alpha = 1,
                       lambda = str_int_lasso_bestlam_2)
coef(str_int_lasso_2)
# Zero: Q11A*TIPI4, Q40A*TIPI4, Q10A*TIPI4, Q13A*TIPI4
```

```{r Nonlin Stress Trans Int Lasso Modeling}
set.seed(1)
str_t_int_lasso_cv <- cv.glmnet(str_t_pred, str_t_score, alpha = 1)
str_t_int_lasso_bestlam <- str_t_int_lasso_cv$lambda.min
str_t_int_lasso_bestlam

str_t_int_lasso <- glmnet(str_t_pred, str_t_score, alpha = 1,
                       lambda = str_t_int_lasso_bestlam)
coef(str_t_int_lasso)
# Zero: every single interaction

set.seed(1)
str_t_int_lasso_cv_2 <- cv.glmnet(str_t_pred_int, str_t_score, alpha = 1)
str_t_int_lasso_bestlam_2 <- str_t_int_lasso_cv_2$lambda.min
str_t_int_lasso_bestlam_2

str_t_int_lasso_2 <- glmnet(str_t_pred_int, str_t_score, alpha = 1,
                       lambda = str_t_int_lasso_bestlam_2)
coef(str_t_int_lasso_2)
# Zero: every single interaction and also the higher order term
```

### Nonlinear Effects

#### Polynomial

```{r Nonlin Stress Poly}
summary(lm(stress_score ~ poly(Q11A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(stress_score ~ poly(Q27A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Just first order
summary(lm(stress_score ~ poly(Q29A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Just first order
summary(lm(stress_score ~ poly(Q9A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant? But second barely, so third may be unnecessary
summary(lm(stress_score ~ poly(Q28A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(stress_score ~ poly(Q40A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(stress_score ~ poly(Q10A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(stress_score ~ poly(Q13A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order not significant but third order is?
summary(lm(stress_score ~ poly(Q21A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(stress_score ~ poly(TIPI4, 5),
                trainSubset[!is.na(trainSubset$TIPI4),])) # First five orders significant

str_tipi_poly <- lm(stress_score ~ poly(TIPI4, 5),
                trainSubset[!is.na(trainSubset$TIPI4),])
lims <- range(trainSubset[!is.na(trainSubset$TIPI4),]$TIPI4)
lims.grid <- seq(from = lims[1], to = lims[2])
str_tipi_preds <- predict(str_tipi_poly,
                          newdata = list(TIPI4 = lims.grid))
plot(trainSubset$TIPI4, trainSubset$stress_score,
     xlim = lims, cex = 0.5, col = "darkgrey")
title("Degree-5 Polynomial", outer = TRUE)
lines(lims.grid, str_tipi_preds, lwd = 2, col = "blue") # I guess that's a degree-5 polynomial
# Might be a good candidate for splines
```

```{r Nonlin Stress Trans Poly}
summary(lm(stress_trans ~ poly(Q11A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(stress_trans ~ poly(Q27A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(stress_trans ~ poly(Q29A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(stress_trans ~ poly(Q9A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant? But second barely, so third may be unnecessary
summary(lm(stress_trans ~ poly(Q28A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(stress_trans ~ poly(Q40A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(stress_trans ~ poly(Q10A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(stress_trans ~ poly(Q13A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order not significant but third order is?
summary(lm(stress_trans ~ poly(Q21A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(stress_trans ~ poly(TIPI4, 5),
                trainSubset[!is.na(trainSubset$TIPI4),])) # First five orders significant

str_t_tipi_poly <- lm(stress_trans ~ poly(TIPI4, 5),
                trainSubset[!is.na(trainSubset$TIPI4),])
lims <- range(trainSubset[!is.na(trainSubset$TIPI4),]$TIPI4)
lims.grid <- seq(from = lims[1], to = lims[2])
str_t_tipi_preds <- predict(str_t_tipi_poly,
                          newdata = list(TIPI4 = lims.grid))
plot(trainSubset$TIPI4, trainSubset$stress_score,
     xlim = lims, cex = 0.5, col = "darkgrey")
title("Degree-5 Polynomial", outer = TRUE)
lines(lims.grid, str_t_tipi_preds, lwd = 2, col = "blue") # I guess that's a degree-5 polynomial
# Might be a good candidate for splines
```

#### Splines

```{r Nonlin Stress Splines}
summary(lm(stress_score ~ bs(TIPI4, df = 5), trainSubset)) # All sig, slightly lower Rsq than for degree-5 polynomial (but probably better prediction? we will see)
summary(lm(stress_score ~ ns(TIPI4, df = 5), trainSubset)) # About the same idk
set.seed(1)
smooth.spline(trainSubset[!is.na(trainSubset$TIPI4),]$TIPI4,
              trainSubset[!is.na(trainSubset$TIPI4),]$stress_score, cv = T) # Df almost 7 exactly
smooth.spline(trainSubset[!is.na(trainSubset$TIPI4),]$TIPI4,
              trainSubset[!is.na(trainSubset$TIPI4),]$stress_score, df = 7)
```

```{r Nonlin Stress Trans Splines}
summary(lm(stress_trans ~ bs(TIPI4, df = 5), trainSubset)) # Approx equivalent to quintic polynomial
summary(lm(stress_score ~ ns(TIPI4, df = 5), trainSubset)) # Bit worse
set.seed(1)
smooth.spline(trainSubset[!is.na(trainSubset$TIPI4),]$TIPI4,
              trainSubset[!is.na(trainSubset$TIPI4),]$stress_trans, cv = T)
smooth.spline(trainSubset[!is.na(trainSubset$TIPI4),]$TIPI4,
              trainSubset[!is.na(trainSubset$TIPI4),]$stress_trans, df = 7)
```

#### GAMs

```{r Nonlin Stress GAMs}
summary(lm(stress_score ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), trainSubset))
# Rsq = .8721, adj rsq = .872, this is slightly better than the linear model
plot(lm(stress_score ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), trainSubset)) # Actually could be worse

summary(lm(stress_score ~ Q11A + Q27A + poly(Q29A, 2) + poly(Q9A, 3) + # Only the significant from above
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             bs(TIPI4, df = 5), trainSubset)) # Same r squared & adj r squared
plot(lm(stress_score ~ Q11A + Q27A + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             bs(TIPI4, df = 5), trainSubset)) # the same-ish?

summary(lm(stress_score ~ Q11A + Q27A + poly(Q29A, 2) + poly(Q9A, 3) + # Diff TIPI4
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             ns(TIPI4, df = 5), trainSubset)) # Same rsq and adj rsq
plot(lm(stress_score ~ Q11A + Q27A + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             ns(TIPI4, df = 5), trainSubset)) # Same to maybe slightly worse?

summary(lm(stress_score ~ Q11A + Q27A + poly(Q29A, 2) + poly(Q9A, 3) + # Polynomial yippee
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             poly(TIPI4, 5), trainSubset[!is.na(trainSubset$TIPI4),])) # Same adj rsq and rsq, quartic & quintic effects not sig

summary(lm(stress_score ~ Q11A + Q27A + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             poly(TIPI4, 3), trainSubset[!is.na(trainSubset$TIPI4),])) # The same but now just about everything is significant
plot(lm(stress_score ~ Q11A + Q27A + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             poly(TIPI4, 3), trainSubset[!is.na(trainSubset$TIPI4),])) # About the same
```

```{r Nonlin Stress Trans GAMs}
summary(lm(stress_trans ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), trainSubset))
# Rsq = .8744, adj rsq = .8743, this is not meaningfully better than the linear model
plot(lm(stress_trans ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), trainSubset)) # worse

summary(lm(stress_trans ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 2) + # Only the significant from above
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + poly(Q13A, 3) + Q21A +
             bs(TIPI4, df = 5), trainSubset)) # Same r squared & adj r squared
plot(lm(stress_trans ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 2) + # Only the significant from above
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + poly(Q13A, 3) + Q21A +
             bs(TIPI4, df = 5), trainSubset)) # the same-ish?

summary(lm(stress_trans ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 2) + # Different TIPI4
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + poly(Q13A, 3) + Q21A +
             ns(TIPI4, df = 5), trainSubset)) # Same rsq and adj rsq
plot(lm(stress_trans ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 2) + 
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + poly(Q13A, 3) + Q21A +
             ns(TIPI4, df = 5), trainSubset)) # Same

summary(lm(stress_trans ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 2) + # Polynomial
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + poly(Q13A, 3) + Q21A +
             poly(TIPI4, 5), trainSubset[!is.na(trainSubset$TIPI4),])) # Same adj rsq and rsq, quartic & quintic effects not sig

summary(lm(stress_trans ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 2) + # Polynomial
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + poly(Q13A, 3) + Q21A +
             poly(TIPI4, 3), trainSubset[!is.na(trainSubset$TIPI4),])) # The same but now everything is significant
plot(lm(stress_trans ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 2) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + poly(Q13A, 3) + Q21A +
             poly(TIPI4, 3), trainSubset[!is.na(trainSubset$TIPI4),])) # About the same
```

Main takeaway: this is probably a futile exercise. The improvements are so little that I'm almost positive this will not hold up under CV.

## Anxiety

### Interactions

```{r Nonlin Anx Int}
summary(lm(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q11A*TIPI4, trainSubset)) # Sig
summary(lm(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q27A*TIPI4, trainSubset)) # Sig
summary(lm(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q29A*TIPI4, trainSubset)) # Sig
summary(lm(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q9A*TIPI4, trainSubset)) # Sig
summary(lm(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q28A*TIPI4, trainSubset)) # Sig
summary(lm(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q40A*TIPI4, trainSubset)) # Sig
summary(lm(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q10A*TIPI4, trainSubset)) # Sig
summary(lm(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q13A*TIPI4, trainSubset)) # Sig
summary(lm(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q21A*TIPI4, trainSubset)) # Sig, makes Q21A not sig

summary(lm(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2) + Q11A*TIPI4 + Q27A*TIPI4 +
             Q29A*TIPI4 + Q9A*TIPI4 + Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 +
             Q13A*TIPI4 + Q21A*TIPI4, trainSubset[!is.na(trainSubset$TIPI4),])) 
# R squared is a little better than model without interactions, defined above
# Not sig vars: Q10A
# Not sig interactions: Q27A, Q9A, Q40A, Q13A
```

```{r Nonlin Anx Trans Int}
summary(lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q11A*TIPI4, trainSubset)) # Sig
summary(lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q27A*TIPI4, trainSubset)) # Sig
summary(lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q29A*TIPI4, trainSubset)) # Sig
summary(lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q9A*TIPI4, trainSubset)) # Sig
summary(lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q28A*TIPI4, trainSubset)) # Sig
summary(lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q40A*TIPI4, trainSubset)) # Sig
summary(lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q10A*TIPI4, trainSubset)) # Sig
summary(lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q13A*TIPI4, trainSubset)) # Sig
summary(lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q21A*TIPI4, trainSubset)) # Sig

summary(lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2) + Q11A*TIPI4 + Q27A*TIPI4 +
             Q29A*TIPI4 + Q9A*TIPI4 + Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 +
             Q13A*TIPI4 + Q21A*TIPI4, trainSubset[!is.na(trainSubset$TIPI4),]))  
# R squared is substantially better in this case, and is now outperforming non-transformed by almost .01 in rsq
# Not sig vars: none
# Not sig interactions: Q11A, Q29A, Q10A, Q13A
```

```{r Nonlin Anx Int Lasso}
anx_score <- trainSubset$anxiety_score[!is.na(trainSubset$TIPI4)]
anx_t_score <- trainSubset$anxiety_trans[!is.na(trainSubset$TIPI4)]
anx_pred <- model.matrix(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4 +
                           Q11A*TIPI4 + Q27A*TIPI4 + Q29A*TIPI4 + Q9A*TIPI4 +
                           Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 + Q13A*TIPI4 + 
                           Q21A*TIPI4, 
                         trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
anx_pred_int <- model.matrix(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 2) +
                           Q11A*TIPI4 + Q27A*TIPI4 + Q29A*TIPI4 + Q9A*TIPI4 +
                           Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 + Q13A*TIPI4 + 
                           Q21A*TIPI4, 
                         trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
anx_t_pred <- model.matrix(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4 +
                             Q11A*TIPI4 + Q27A*TIPI4 + Q29A*TIPI4 + Q9A*TIPI4 +
                             Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 + Q13A*TIPI4 +
                             Q21A*TIPI4,
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
anx_t_pred_int <- model.matrix(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 2) +
                             Q11A*TIPI4 + Q27A*TIPI4 + Q29A*TIPI4 + Q9A*TIPI4 +
                             Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 + Q13A*TIPI4 +
                             Q21A*TIPI4,
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
```

```{r Nonlin Anx Int Lasso Modeling}
set.seed(1)
anx_int_lasso_cv <- cv.glmnet(anx_pred, anx_score, alpha = 1)
anx_int_lasso_bestlam <- anx_int_lasso_cv$lambda.min
anx_int_lasso_bestlam

anx_int_lasso <- glmnet(anx_pred, anx_score, alpha = 1,
                       lambda = anx_int_lasso_bestlam)
coef(anx_int_lasso)
# Zero: Q11A*TIPI4, Q27A*TIPI4, Q9A*TIPI4, Q13A*TIPI4

set.seed(1)
anx_int_lasso_cv_2 <- cv.glmnet(anx_pred_int, anx_score, alpha = 1)
anx_int_lasso_bestlam_2 <- anx_int_lasso_cv_2$lambda.min
anx_int_lasso_bestlam_2

anx_int_lasso_2 <- glmnet(anx_pred_int, anx_score, alpha = 1,
                       lambda = anx_int_lasso_bestlam_2)
coef(anx_int_lasso_2)
# Zero: Q11A*TIPI4, Q27A*TIPI4, Q9A*TIPI4, Q13A*TIPI4
```

```{r Nonlin Anx Trans Int Lasso Modeling}
set.seed(1)
anx_t_int_lasso_cv <- cv.glmnet(anx_t_pred, anx_t_score, alpha = 1)
anx_t_int_lasso_bestlam <- anx_t_int_lasso_cv$lambda.min
anx_t_int_lasso_bestlam

anx_t_int_lasso <- glmnet(anx_t_pred, anx_t_score, alpha = 1,
                       lambda = anx_t_int_lasso_bestlam)
coef(anx_t_int_lasso)
# Zero: Q11A*TIPI4, Q29A*TIPI4, Q10A*TIPI4, Q13A*TIPI4

set.seed(1)
anx_t_int_lasso_cv_2 <- cv.glmnet(anx_t_pred_int, anx_t_score, alpha = 1)
anx_t_int_lasso_bestlam_2 <- anx_t_int_lasso_cv_2$lambda.min
anx_t_int_lasso_bestlam_2

anx_t_int_lasso_2 <- glmnet(anx_t_pred_int, anx_t_score, alpha = 1,
                       lambda = anx_t_int_lasso_bestlam_2)
coef(anx_t_int_lasso_2)
# Zero: Q11A*TIPI4, Q29A*TIPI4, Q10A*TIPI4, Q13A*TIPI4
```

### Nonlinear Effects

#### Polynomial

```{r Nonlin Anx Poly}
summary(lm(anxiety_score ~ poly(Q11A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(anxiety_score ~ poly(Q27A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(anxiety_score ~ poly(Q29A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(anxiety_score ~ poly(Q9A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(anxiety_score ~ poly(Q28A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Third order significant, second not
summary(lm(anxiety_score ~ poly(Q40A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(anxiety_score ~ poly(Q10A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Third order significant, second not
summary(lm(anxiety_score ~ poly(Q13A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(anxiety_score ~ poly(Q21A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(anxiety_score ~ poly(TIPI4, 6),
                trainSubset[!is.na(trainSubset$TIPI4),])) # First three orders significant, also fifth

anx_tipi_poly <- lm(anxiety_score ~ poly(TIPI4, 5),
                trainSubset[!is.na(trainSubset$TIPI4),])
lims <- range(trainSubset[!is.na(trainSubset$TIPI4),]$TIPI4)
lims.grid <- seq(from = lims[1], to = lims[2])
anx_tipi_preds <- predict(anx_tipi_poly,
                          newdata = list(TIPI4 = lims.grid))
plot(trainSubset$TIPI4, trainSubset$anxiety_score,
     xlim = lims, cex = 0.5, col = "darkgrey")
title("Degree-5 Polynomial", outer = TRUE)
lines(lims.grid, anx_tipi_preds, lwd = 2, col = "blue") # I guess that's a degree-5 polynomial
# Might be a good candidate for splines
```

```{r Nonlin Anx Trans Poly}
summary(lm(anxiety_trans ~ poly(Q11A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(anxiety_trans ~ poly(Q27A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(anxiety_trans ~ poly(Q29A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(anxiety_trans ~ poly(Q9A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(anxiety_trans ~ poly(Q28A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(anxiety_trans ~ poly(Q40A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(anxiety_trans ~ poly(Q10A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(anxiety_trans ~ poly(Q13A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(anxiety_trans ~ poly(Q21A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(anxiety_trans ~ poly(TIPI4, 6),
                trainSubset[!is.na(trainSubset$TIPI4),])) # First five orders significant

anx_t_tipi_poly <- lm(anxiety_trans ~ poly(TIPI4, 5),
                trainSubset[!is.na(trainSubset$TIPI4),])
lims <- range(trainSubset[!is.na(trainSubset$TIPI4),]$TIPI4)
lims.grid <- seq(from = lims[1], to = lims[2])
anx_t_tipi_preds <- predict(anx_t_tipi_poly,
                          newdata = list(TIPI4 = lims.grid))
plot(trainSubset$TIPI4, trainSubset$anxiety_trans,
     xlim = lims, cex = 0.5, col = "darkgrey")
title("Degree-5 Polynomial", outer = TRUE)
lines(lims.grid, anx_t_tipi_preds, lwd = 2, col = "blue") # A little curvy
```

#### Splines

```{r Nonlin Anx Splines}
summary(lm(anxiety_score ~ bs(TIPI4, df = 5), trainSubset)) # All sig except for 1st order, identical rsq
summary(lm(anxiety_score ~ ns(TIPI4, df = 5), trainSubset)) # About the same idk
```

```{r Nonlin Anx Trans Splines}
summary(lm(anxiety_trans ~ bs(TIPI4, df = 5), trainSubset)) # Approx equivalent to quintic polynomial
summary(lm(anxiety_trans ~ ns(TIPI4, df = 5), trainSubset)) # About the same
```

#### GAMs

```{r Nonlin Anx GAMs}
summary(lm(anxiety_score ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), trainSubset))
# Rsq = .7903, adj rsq = .7901, this is slightly better than the linear model
plot(lm(anxiety_score ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), trainSubset)) # Far from ideal

summary(lm(anxiety_score ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) + poly(Q9A, 2) + # Only the significant from above
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             bs(TIPI4, df = 5), trainSubset)) # Same r squared & adj r squared
plot(lm(anxiety_score ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) + poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             bs(TIPI4, df = 5), trainSubset)) # the same-ish?

summary(lm(anxiety_score ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) + poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             ns(TIPI4, df = 5), trainSubset)) # Same rsq and adj rsq
plot(lm(anxiety_score ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) + poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             ns(TIPI4, df = 5), trainSubset)) # Same to maybe slightly worse?

summary(lm(anxiety_score ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) + poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             poly(TIPI4, 5), trainSubset[!is.na(trainSubset$TIPI4),])) # Same adj rsq and rsq, quartic & quintic effects not sig

summary(lm(anxiety_score ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) + poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             poly(TIPI4, 3), trainSubset[!is.na(trainSubset$TIPI4),])) # The same but now everything is significant
plot(lm(anxiety_score ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) + poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             poly(TIPI4, 3), trainSubset[!is.na(trainSubset$TIPI4),])) # About the same
```

```{r Nonlin Anx Trans GAMs}
summary(lm(anxiety_trans ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), trainSubset))
# Rsq = .8038, adj rsq = .8037, this is substantially better than the linear model
plot(lm(anxiety_trans ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), trainSubset)) # Far from ideal

summary(lm(anxiety_trans ~ Q11A + poly(Q27A, 2) + poly(Q29A, 2) + poly(Q9A, 3) + # Only the significant from above
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + poly(Q13A, 2) + Q21A +
             bs(TIPI4, df = 5), trainSubset)) # Same r squared & adj r squared
plot(lm(anxiety_trans ~ Q11A + poly(Q27A, 2) + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + poly(Q13A, 2) + Q21A +
             bs(TIPI4, df = 5), trainSubset)) # the same-ish?

summary(lm(anxiety_trans ~ Q11A + poly(Q27A, 2) + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + poly(Q13A, 2) + Q21A +
             ns(TIPI4, df = 5), trainSubset)) # Same rsq and adj rsq
plot(lm(anxiety_trans ~ Q11A + poly(Q27A, 2) + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + poly(Q13A, 2) + Q21A +
             ns(TIPI4, df = 5), trainSubset)) # Same

summary(lm(anxiety_trans ~ Q11A + poly(Q27A, 2) + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + poly(Q13A, 2) + Q21A +
             poly(TIPI4, 5),trainSubset[!is.na(trainSubset$TIPI4),])) # Same adj rsq and rsq, quartic & quintic effects not sig

summary(lm(anxiety_trans ~ Q11A + poly(Q27A, 2) + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + poly(Q13A, 2) + Q21A +
             poly(TIPI4, 3),trainSubset[!is.na(trainSubset$TIPI4),])) # The same but now everything is significant
plot(lm(anxiety_trans ~ Q11A + poly(Q27A, 2) + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + poly(Q13A, 2) + Q21A +
             poly(TIPI4, 3),trainSubset[!is.na(trainSubset$TIPI4),])) # About the same
```


## Depression

### Interactions

```{r Nonlin Dep Int}
summary(lm(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q11A*TIPI4, trainSubset)) # Not sig
summary(lm(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q27A*TIPI4, trainSubset)) # Sig, TIPI4 not sig
summary(lm(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q29A*TIPI4, trainSubset)) # Not sig
summary(lm(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q9A*TIPI4, trainSubset)) # Not sig
summary(lm(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q28A*TIPI4, trainSubset)) # Not sig, and also makes Q28A n.s.
summary(lm(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q40A*TIPI4, trainSubset)) # Not sig, nor is TIPI4
summary(lm(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q10A*TIPI4, trainSubset)) # Sig
summary(lm(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q13A*TIPI4, trainSubset)) # Sig
summary(lm(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q21A*TIPI4, trainSubset)) # Sig

summary(lm(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2) + Q11A*TIPI4 + Q27A*TIPI4 +
             Q29A*TIPI4 + Q9A*TIPI4 + Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 +
             Q13A*TIPI4 + Q21A*TIPI4, trainSubset[!is.na(trainSubset$TIPI4),])) 
# R squared is about the same as model without interactions, defined above
# Not sig vars: Q11A, Q29A, Q28A, TIPI (both terms)
# Not sig interactions: Q11A, Q27A, Q9A, Q40A, Q21A
```

```{r Nonlin Dep Trans Int}
summary(lm(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q11A*TIPI4, trainSubset)) # Sig
summary(lm(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q27A*TIPI4, trainSubset)) # Sig
summary(lm(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q29A*TIPI4, trainSubset)) # Sig
summary(lm(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q9A*TIPI4, trainSubset)) # Sig
summary(lm(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q28A*TIPI4, trainSubset)) # Sig
summary(lm(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q40A*TIPI4, trainSubset)) # Sig
summary(lm(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q10A*TIPI4, trainSubset)) # Sig
summary(lm(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q13A*TIPI4, trainSubset)) # Sig
summary(lm(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4 + Q21A*TIPI4, trainSubset)) # Sig

summary(lm(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2) + Q11A*TIPI4 + Q27A*TIPI4 +
             Q29A*TIPI4 + Q9A*TIPI4 + Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 +
             Q13A*TIPI4 + Q21A*TIPI4, trainSubset[!is.na(trainSubset$TIPI4),]))  
# Slightly better than straight-up linear, but untransformed is behaving better
# Not sig vars: Q11A, Q29A, Q28A, quadratic term
# Not sig interactions: Q11A, Q9A, Q40A
```

```{r Nonlin Dep Int Lasso}
dep_score <- trainSubset$depression_score[!is.na(trainSubset$TIPI4)]
dep_t_score <- trainSubset$depression_trans[!is.na(trainSubset$TIPI4)]
dep_pred <- model.matrix(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                           Q40A + Q10A + Q13A + Q21A + TIPI4 +
                           Q11A*TIPI4 + Q27A*TIPI4 + Q29A*TIPI4 + Q9A*TIPI4 +
                           Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 + Q13A*TIPI4 + 
                           Q21A*TIPI4, 
                         trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
dep_pred_int <- model.matrix(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 2) +
                           Q11A*TIPI4 + Q27A*TIPI4 + Q29A*TIPI4 + Q9A*TIPI4 +
                           Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 + Q13A*TIPI4 + 
                           Q21A*TIPI4, 
                         trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
dep_t_pred <- model.matrix(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4 +
                             Q11A*TIPI4 + Q27A*TIPI4 + Q29A*TIPI4 + Q9A*TIPI4 +
                             Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 + Q13A*TIPI4 +
                             Q21A*TIPI4,
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
dep_t_pred_int <- model.matrix(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 2) +
                             Q11A*TIPI4 + Q27A*TIPI4 + Q29A*TIPI4 + Q9A*TIPI4 +
                             Q28A*TIPI4 + Q40A*TIPI4 + Q10A*TIPI4 + Q13A*TIPI4 +
                             Q21A*TIPI4,
                            trainSubset[!is.na(trainSubset$TIPI4),])[,-1]
```

```{r Nonlin Dep Int Lasso Modeling}
set.seed(1)
dep_int_lasso_cv <- cv.glmnet(dep_pred, dep_score, alpha = 1)
dep_int_lasso_bestlam <- dep_int_lasso_cv$lambda.min
dep_int_lasso_bestlam

dep_int_lasso <- glmnet(dep_pred, dep_score, alpha = 1,
                       lambda = dep_int_lasso_bestlam)
coef(dep_int_lasso)
# Zero: TIPI4 and every single interaction

set.seed(1)
dep_int_lasso_cv_2 <- cv.glmnet(dep_pred_int, dep_score, alpha = 1)
dep_int_lasso_bestlam_2 <- dep_int_lasso_cv_2$lambda.min
dep_int_lasso_bestlam_2

dep_int_lasso_2 <- glmnet(dep_pred_int, dep_score, alpha = 1,
                       lambda = dep_int_lasso_bestlam_2)
coef(dep_int_lasso_2)
# Zero: TIPI4 and every single interaction
```

```{r Nonlin Dep Trans Int Lasso Modeling}
set.seed(1)
dep_t_int_lasso_cv <- cv.glmnet(dep_t_pred, dep_t_score, alpha = 1)
dep_t_int_lasso_bestlam <- dep_t_int_lasso_cv$lambda.min
dep_t_int_lasso_bestlam

dep_t_int_lasso <- glmnet(dep_t_pred, dep_t_score, alpha = 1,
                       lambda = dep_t_int_lasso_bestlam)
coef(dep_t_int_lasso)
# Zero: Q11A*TIPI4, Q9A*TIPI4, Q40A*TIPI4

set.seed(1)
dep_t_int_lasso_cv_2 <- cv.glmnet(dep_t_pred_int, dep_t_score, alpha = 1)
dep_t_int_lasso_bestlam_2 <- dep_t_int_lasso_cv_2$lambda.min
dep_t_int_lasso_bestlam_2

dep_t_int_lasso_2 <- glmnet(dep_t_pred_int, dep_t_score, alpha = 1,
                       lambda = dep_t_int_lasso_bestlam_2)
coef(dep_t_int_lasso_2)
# Zero: Q28A, Q11A*TIPI4, Q40A*TIPI4
```

### Nonlinear Effects

#### Polynomial

```{r Nonlin Dep Poly}
summary(lm(depression_score ~ poly(Q11A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(depression_score ~ poly(Q27A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # First order significant
summary(lm(depression_score ~ poly(Q29A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(depression_score ~ poly(Q9A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(depression_score ~ poly(Q28A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(depression_score ~ poly(Q40A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(depression_score ~ poly(Q10A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(depression_score ~ poly(Q13A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(depression_score ~ poly(Q21A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(depression_score ~ poly(TIPI4, 6),
                trainSubset[!is.na(trainSubset$TIPI4),])) # First five orders significant

dep_tipi_poly <- lm(depression_score ~ poly(TIPI4, 5),
                trainSubset[!is.na(trainSubset$TIPI4),])
lims <- range(trainSubset[!is.na(trainSubset$TIPI4),]$TIPI4)
lims.grid <- seq(from = lims[1], to = lims[2])
dep_tipi_preds <- predict(dep_tipi_poly,
                          newdata = list(TIPI4 = lims.grid))
plot(trainSubset$TIPI4, trainSubset$depression_score,
     xlim = lims, cex = 0.5, col = "darkgrey")
title("Degree-5 Polynomial", outer = TRUE)
lines(lims.grid, dep_tipi_preds, lwd = 2, col = "blue") # I guess that's a degree-5 polynomial
# Demonstrates more nonlinearity than anxiety & stress did
```

```{r Nonlin Dep Trans Poly}
summary(lm(depression_trans ~ poly(Q11A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Third order significant, second not
summary(lm(depression_trans ~ poly(Q27A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(depression_trans ~ poly(Q29A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(depression_trans ~ poly(Q9A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Third order significant, second not
summary(lm(depression_trans ~ poly(Q28A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # Second order significant
summary(lm(depression_trans ~ poly(Q40A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(depression_trans ~ poly(Q10A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(depression_trans ~ poly(Q13A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(depression_trans ~ poly(Q21A, 3),
                trainSubset[!is.na(trainSubset$TIPI4),])) # All three significant
summary(lm(depression_trans ~ poly(TIPI4, 6),
                trainSubset[!is.na(trainSubset$TIPI4),])) # First five orders significant

dep_t_tipi_poly <- lm(depression_trans ~ poly(TIPI4, 5),
                trainSubset[!is.na(trainSubset$TIPI4),])
lims <- range(trainSubset[!is.na(trainSubset$TIPI4),]$TIPI4)
lims.grid <- seq(from = lims[1], to = lims[2])
dep_t_tipi_preds <- predict(dep_t_tipi_poly,
                          newdata = list(TIPI4 = lims.grid))
plot(trainSubset$TIPI4, trainSubset$depression_trans,
     xlim = lims, cex = 0.5, col = "darkgrey")
title("Degree-5 Polynomial", outer = TRUE)
lines(lims.grid, anx_t_tipi_preds, lwd = 2, col = "blue") # That's almost a straight line
```

#### Splines

```{r Nonlin Dep Splines}
summary(lm(depression_score ~ bs(TIPI4, df = 5), trainSubset)) # All sig, same rsq
summary(lm(depression_score ~ ns(TIPI4, df = 5), trainSubset)) # About the same idk
```

```{r Nonlin Dep Trans Splines}
summary(lm(depression_trans ~ bs(TIPI4, df = 5), trainSubset)) # Approx equivalent to quintic polynomial
summary(lm(depression_trans ~ ns(TIPI4, df = 5), trainSubset)) # About the same
```

#### GAMs

```{r Nonlin Dep GAMs}
summary(lm(depression_score ~ poly(Q11A, 3) + Q27A + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 2) + poly(Q40A, 3) + poly(Q10A, 3) + 
             poly(Q13A, 2) + poly(Q21A, 3) + bs(TIPI4, df = 5), trainSubset))
# Rsq = .9048, adj rsq = .9047, this is equivalent to the linear model
plot(lm(depression_score ~ poly(Q11A, 3) + Q27A + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 2) + poly(Q40A, 3) + poly(Q10A, 3) + 
             poly(Q13A, 2) + poly(Q21A, 3) + bs(TIPI4, df = 5), trainSubset)) # Seen worse but it's not good

summary(lm(depression_score ~ Q11A + Q27A + Q29A + poly(Q9A, 3) + # Only the significant from above
             Q28A + poly(Q40A, 3) + Q10A + Q13A + poly(Q21A, 3) +
             bs(TIPI4, df = 5), trainSubset)) # Same r squared & adj r squared
plot(lm(depression_score ~ Q11A + Q27A + Q29A + poly(Q9A, 3) +
             Q28A + poly(Q40A, 3) + Q10A + Q13A + poly(Q21A, 3) +
             bs(TIPI4, df = 5), trainSubset)) # the same-ish?

summary(lm(depression_score ~ Q11A + Q27A + Q29A + poly(Q9A, 3) +
             Q28A + poly(Q40A, 3) + Q10A + Q13A + poly(Q21A, 3) +
             ns(TIPI4, df = 5), trainSubset)) # Same rsq and adj rsq
plot(lm(depression_score ~ Q11A + Q27A + Q29A + poly(Q9A, 3) +
             Q28A + poly(Q40A, 3) + Q10A + Q13A + poly(Q21A, 3) +
             ns(TIPI4, df = 5), trainSubset)) # Same to maybe slightly worse?

summary(lm(depression_score ~ Q11A + Q27A + Q29A + poly(Q9A, 3) +
             Q28A + poly(Q40A, 3) + Q10A + Q13A + poly(Q21A, 3) +
             poly(TIPI4, 5), trainSubset[!is.na(trainSubset$TIPI4),])) # Same adj rsq and rsq, not a single TIPI effect significant
```

```{r Nonlin Dep Trans GAMs}
summary(lm(depression_trans ~ poly(Q11A, 3) + Q27A + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 2) + poly(Q40A, 3) + poly(Q10A, 3) + 
             poly(Q13A, 2) + poly(Q21A, 3) + bs(TIPI4, df = 5), trainSubset))
# Rsq = .9002, adj rsq = .9001, this is a bit better than the linear model
plot(lm(depression_trans ~ poly(Q11A, 3) + Q27A + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 2) + poly(Q40A, 3) + poly(Q10A, 3) + 
             poly(Q13A, 2) + poly(Q21A, 3) + bs(TIPI4, df = 5), trainSubset)) # Not the dream

summary(lm(depression_trans ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) + Q9A + # Only the significant from above
             Q28A + Q40A + poly(Q10A, 3) + poly(Q13A, 2) + poly(Q21A, 3) +
             bs(TIPI4, df = 5), trainSubset)) # Same r squared & adj r squared
plot(lm(depression_trans ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) + Q9A +
             Q28A + Q40A + poly(Q10A, 3) + poly(Q13A, 2) + poly(Q21A, 3) +
             bs(TIPI4, df = 5), trainSubset)) # the same-ish?

summary(lm(depression_trans ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) + Q9A +
             Q28A + Q40A + poly(Q10A, 3) + poly(Q13A, 2) + poly(Q21A, 3) +
             ns(TIPI4, df = 5), trainSubset)) # Same rsq and adj rsq
plot(lm(depression_trans ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) + Q9A +
             Q28A + Q40A + poly(Q10A, 3) + poly(Q13A, 2) + poly(Q21A, 3) +
             bs(TIPI4, df = 5), trainSubset)) # Same

summary(lm(depression_trans ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) + Q9A +
             Q28A + Q40A + poly(Q10A, 3) + poly(Q13A, 2) + poly(Q21A, 3) +
             poly(TIPI4, 5), trainSubset[!is.na(trainSubset$TIPI4),])) # Same adj rsq and rsq, TIPI sig in 2nd and 4th degrees

summary(lm(depression_trans ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) + Q9A +
             Q28A + Q40A + poly(Q10A, 3) + poly(Q13A, 2) + poly(Q21A, 3) +
             poly(TIPI4, 4), trainSubset[!is.na(trainSubset$TIPI4),])) # The same but now everything is significant except 1st and 3rd order effects
plot(lm(depression_trans ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) + Q9A +
             Q28A + Q40A + poly(Q10A, 3) + poly(Q13A, 2) + poly(Q21A, 3) +
             poly(TIPI4, 4), trainSubset[!is.na(trainSubset$TIPI4),])) # About the same, maybe a bit worse
```


# Trees

## Stress

### Basic Regression Trees

```{r Tree Stress}
str_tree <- tree(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset)

summary(str_tree) # Terminal nodes = 8, Residual mean deviance = 29.04

plot(str_tree)
text(str_tree, pretty = 0)

set.seed(1)
str_tree_cv <- cv.tree(str_tree)
plot(str_tree_cv$size, str_tree_cv$dev, type = "b") # Best is 8
```

```{r Tree Stress Trans}
str_t_tree <- tree(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset)

summary(str_t_tree) # Terminal nodes = 10, Residual Mean Deviance = 7.822

plot(str_t_tree)
text(str_t_tree, pretty = 0)

set.seed(1)
str_t_tree_cv <- cv.tree(str_t_tree)
plot(str_t_tree_cv$size, str_t_tree_cv$dev, type = "b") # Best is 10
```

```{r Tree Stress Bin}
str_b_tree <- tree(stress_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset)

summary(str_b_tree) # Terminal nodes = 8, RMD = 0.6164, Misclass error rate = .1274

plot(str_b_tree)
text(str_b_tree, pretty = 0)

set.seed(1)
str_b_tree_cv <- cv.tree(str_b_tree, FUN = prune.misclass)

par(mfrow = c(1,2))
plot(str_b_tree_cv$size, str_b_tree_cv$dev, type = "b") # Best is 6 I think?
plot(str_b_tree_cv$k, str_b_tree_cv$dev, type = "b") # Wants to be small

dev.off()

str_b_tree_prune <- prune.misclass(str_b_tree, best = 6)

summary(str_b_tree_prune) # Terminal nodes = 6, RMD = 0.688, MCE = 0.1274

plot(str_b_tree_prune)
text(str_b_tree_prune, pretty = 0)
```

### Bagging

BEFORE running (although this should be ~1hr each): make sure to understand exactly how to calculate OOB error rate. We are NOT running CV on ensemble trees.

```{r Tree Stress Bag}
set.seed(1)
str_bag <- randomForest(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A +
                         Q10A + Q13A + Q21A + TIPI4, trainSubset, mtry = 10,
                       importance = T, ntree = 500)
save(str_bag, file = "Stress Final Bag.RData") # MSE = 16.097, rsq = .854

set.seed(1)
str_bag_red <- randomForest(stress_score_reduced ~ Q11A + Q27A + Q29A + Q9A + 
                              Q28A + Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset, mtry = 10, importance = T, ntree = 500)
save(str_bag_red, file = "Stress Reduced Final Bag.RData") # MSE = 15.907, rsq = 76.16

set.seed(1)
str_bag_bin <- randomForest(stress_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A +
                              Q10A + Q13A + Q21A + TIPI4, trainSubset, mtry = 10,
                            importance = T, ntree = 500)
save(str_bag_bin, file = "Stress Binary Final Bag.RData") # MCE = 11.41; FN = 6%
```


### Random Forest

Same as above. Will run with $m = 4$.

```{r Tree Stress RF}
set.seed(1)
str_rf <- randomForest(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A +
                         Q10A + Q13A + Q21A + TIPI4, trainSubset, mtry = 4,
                       importance = T, ntree = 500)
save(str_rf, file = "Stress Final RF.RData")

str_rf # MSE = 15.01075, rsq = 86.38, it's worse but remember we expected this to be
# This is also worse when we do 3-fold CV instead of 10

set.seed(1)
str_rf_red <- randomForest(stress_score_reduced ~ Q11A + Q27A + Q29A + Q9A + 
                             Q28A + Q40A + Q10A + Q13A + Q21A + TIPI4,
                           trainSubset, mtry = 4, importance = T, ntree = 500)
save(str_rf_red, file = "Stress Reduced Final RF.RData") # MSE = 14.905, rsq = .7766

set.seed(1)
str_rf_bin <- randomForest(stress_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A +
                             Q10A + Q13A + Q21A + TIPI4, trainSubset, mtry = 4,
                           importance = T, ntree = 500)
save(str_rf_bin, file = "Stress Binary Final RF.RData") # MCE = 10.62, RN = 5.6% worse than almost everything in LR/KNN
```



### Boosting

Need to figure out how to do it with cross-validation properly. There's good documentation in gbm; maybe can't tune amazingly but can do CV estimation within!


## Anxiety

### Basic Regression Trees

```{r Tree Anxiety}
anx_tree <- tree(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset)

summary(anx_tree) # Terminal nodes = 9, Residual mean deviance = 32.46

plot(anx_tree)
text(anx_tree, pretty = 0)

set.seed(1)
anx_tree_cv <- cv.tree(anx_tree)
plot(anx_tree_cv$size, anx_tree_cv$dev, type = "b") # Best is 9
```

```{r Tree Anx Trans}
anx_t_tree <- tree(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A +
                     Q10A + Q13A + Q21A + TIPI4, trainSubset)

summary(anx_t_tree) # Terminal nodes = 9, Residual Mean Deviance = 0.6082

plot(anx_t_tree)
text(anx_t_tree, pretty = 0)

set.seed(1)
anx_t_tree_cv <- cv.tree(anx_t_tree)
plot(anx_t_tree_cv$size, anx_t_tree_cv$dev, type = "b") # Best is 9
```

```{r Tree Anx Bin}
anx_b_tree <- tree(anxiety_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset)

summary(anx_b_tree) # Terminal nodes = 7, RMD = 0.6342, Misclass error rate = 0.15

plot(anx_b_tree)
text(anx_b_tree, pretty = 0)

set.seed(1)
anx_b_tree_cv <- cv.tree(anx_b_tree, FUN = prune.misclass)

par(mfrow = c(1,2))
plot(anx_b_tree_cv$size, anx_b_tree_cv$dev, type = "b") # Best is 3
plot(anx_b_tree_cv$k, anx_b_tree_cv$dev, type = "b") # Wants to be small

dev.off()

anx_b_tree_prune <- prune.misclass(anx_b_tree, best = 3)

summary(anx_b_tree_prune) # Terminal nodes = 3, RMD = 0.825, MCE = 0.15

plot(anx_b_tree_prune)
text(anx_b_tree_prune, pretty = 0)
# Surely this isnt meaningful - we probably aren't getting a simple model that we want
```

### Bagging

```{r Tree Anx Bag}
set.seed(1)
anx_bag <- randomForest(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + 
                          Q10A + Q13A + Q21A + TIPI4, trainSubset, mtry = 10,
                        importance = T, ntree = 500)
save(anx_bag, file = "Anxiety Final Bag.RData") # MSE = 24.513, rsq = .7659

set.seed(1)
anx_bag_red <- randomForest(anxiety_score_reduced ~ Q11A + Q27A + Q29A + Q9A +
                              Q28A + Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset, mtry = 10, importance = T, ntree = 500)
save(anx_bag_red, file = "Anxiety Reduced Final Bag.RData") # MSE = 24.217, rsq = .6217

set.seed(1)
anx_bag_bin <- randomForest(anxiety_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4, trainSubset,
                            mtry = 10, importance = T, ntree = 500)
save(anx_bag_bin, file = "Anxiety Binary Final Bag.RData") # MCE = 13.1%, FN = 7.1%
```


### Random Forests

```{r Tree Anx RF}
set.seed(1)
anx_rf <- randomForest(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + 
                          Q10A + Q13A + Q21A + TIPI4, trainSubset, mtry = 4,
                        importance = T, ntree = 500)
save(anx_rf, file = "Anxiety Final RF.RData") # MSE = 23.03, rsq = .78

set.seed(1)
anx_rf_red <- randomForest(anxiety_score_reduced ~ Q11A + Q27A + Q29A + Q9A +
                              Q28A + Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset, mtry = 4, importance = T, ntree = 500)
save(anx_rf_red, file = "Anxiety Reduced Final RF.RData") # MSE = 22.96, rsq = .6414

set.seed(1)
anx_rf_bin <- randomForest(anxiety_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4, trainSubset,
                            mtry = 4, importance = T, ntree = 500)
save(anx_rf_bin, file = "Anxiety Binary Final RF.RData") # MCE = 12.12%, FN = 6.6%


set.seed(325)
anx_rf_test <- randomForest(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + 
                          Q10A + Q13A + Q21A + TIPI4, trainSubset, mtry = 4,
                        importance = T, ntree = 600, init.forest = anx_rf$forest)
# An extra 100 trees does not make the model meaningfully better
```


### Boosting


## Depression

### Basic Regression Trees

```{r Tree Dep}
dep_tree <- tree(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A +
                   Q10A + Q13A + Q21A + TIPI4, trainSubset)

summary(dep_tree) # Terminal nodes = 8, Residual mean deviance = 26.32

plot(dep_tree)
text(dep_tree, pretty = 0)

set.seed(1)
dep_tree_cv <- cv.tree(dep_tree)
plot(dep_tree_cv$size, dep_tree_cv$dev, type = "b") # Best is technically still 8
```

```{r Tree Dep Trans}
dep_t_tree <- tree(depression_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A +
                     Q10A + Q13A + Q21A + TIPI4, trainSubset)

summary(dep_t_tree) # Terminal nodes = 9, Residual Mean Deviance = 0.6082

plot(dep_t_tree)
text(dep_t_tree, pretty = 0)

set.seed(1)
dep_t_tree_cv <- cv.tree(dep_t_tree)
plot(dep_t_tree_cv$size, dep_t_tree_cv$dev, type = "b") # Best is 8
```

```{r Tree Dep Bin}
dep_b_tree <- tree(depression_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A +
                     Q10A + Q13A + Q21A + TIPI4, trainSubset)

summary(dep_b_tree) # Terminal nodes = 9, RMD = 0.4473, Misclass error rate = 0.09

plot(dep_b_tree)
text(dep_b_tree, pretty = 0)

set.seed(1)
dep_b_tree_cv <- cv.tree(dep_b_tree, FUN = prune.misclass)

par(mfrow = c(1,2))
plot(dep_b_tree_cv$size, dep_b_tree_cv$dev, type = "b") # Best is 7
plot(dep_b_tree_cv$k, dep_b_tree_cv$dev, type = "b") # Wants to be small

dev.off()

dep_b_tree_prune <- prune.misclass(dep_b_tree, best = 7)

summary(dep_b_tree_prune) # Terminal nodes = 7, RMD = 0.5062, MCE = 0.09

plot(dep_b_tree_prune)
text(dep_b_tree_prune, pretty = 0)
# Probably not super meaningful idk
```

### Bagging

```{r Tree Dep Bag}
set.seed(1)
dep_bag <- randomForest(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + 
                          Q10A + Q13A + Q21A + TIPI4, trainSubset, mtry = 10,
                        importance = T, ntree = 500)
save(dep_bag, file = "Depression Final Bag.RData") # MSE = 16.582, rsq = .8906

set.seed(1)
dep_bag_red <- randomForest(depression_score_reduced ~ Q11A + Q27A + Q29A + Q9A +
                              Q28A + Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset, mtry = 10, importance = T, ntree = 500)
save(dep_bag_red, file = "Depression Reduced Final Bag.RData") # MSE = 16.476, rsq = .8193

set.seed(1)
dep_bag_bin <- randomForest(depression_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4, trainSubset,
                            mtry = 10, importance = T, ntree = 500)
save(dep_bag_bin, file = "Depression Binary Final Bag.RData") # MCE = 9.5%, FN = 5%
```


### Random Forests

```{r Tree Dep RF}
set.seed(1)
dep_rf <- randomForest(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + 
                          Q10A + Q13A + Q21A + TIPI4, trainSubset, mtry = 4,
                        importance = T, ntree = 500)
save(dep_rf, file = "Depression Final RF.RData") # MSE = 15.456, rsq = .898

set.seed(1)
dep_rf_red <- randomForest(depression_score_reduced ~ Q11A + Q27A + Q29A + Q9A +
                              Q28A + Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset, mtry = 4, importance = T, ntree = 500)
save(dep_rf_red, file = "Depression Reduced Final RF.RData") # MSE = 15.432, rsq = .8308

set.seed(1)
dep_rf_bin <- randomForest(depression_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4, trainSubset,
                            mtry = 4, importance = T, ntree = 500)
save(dep_rf_bin, file = "Depression Binary Final RF.RData") # MCE = 8.64%, FN = 4.7%
```

### Boosting


# Logistic Regression

## Stress

```{r Logistic Stress}
str_lr <- glm(stress_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset, family = "binomial")
summary(str_lr) # All sig, AIC = 13535
contrasts(trainSubset$stress_bin)

str_lr_prob <- predict(str_lr, type = "response")
str_lr_preds <- rep("Low", length(str_lr_prob))
str_lr_preds[str_lr_prob < .5] <- "High"
mean(str_lr_preds == trainSubset$stress_bin) # .904

table(str_lr_preds, trainSubset$stress_bin)

test_threshold_vals <- function(probs, truth, test_thresholds = test_thr){
  for(i in 1:length(test_thresholds)){
    preds <- rep("Low", length(probs))
    preds[probs < test_thresholds[i]] <- "High"
    accuracy <- sum(diag(table(preds, truth))) / sum(table(preds, truth))
    print(paste("Test:", i))
    print(paste("Threshold:", test_thresholds[i]))
    print(table(preds, truth))
    print(paste("Accuracy:", round(accuracy, 3)))
  }
}

test_threshold_vals(str_lr_prob, trainSubset$stress_bin,
                    test_thresholds = seq(.5, .8, by = .01)) # At 90% accuracy cutoff, best value is threshold = 0.62; only 934 false negatives (3.1%)
# At 88% accuracy, this threshold is 0.77, false negatives = 494 (1.7%)


str_lr_int <- glm(stress_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2) + Q11A*TIPI4 + Q27A*TIPI4 + 
               Q13A*TIPI4 + Q21A*TIPI4,
             trainSubset, family = "binomial")
summary(str_lr_int) # Interaction of Q13A sig, Q21A not sig, AIC = 13517
str_lr_int_prob <- predict(str_lr_int, type = "response")
str_lr_int_preds <- rep("Low", length(str_lr_int_prob))
str_lr_int_preds[str_lr_int_prob < .5] <- "High"
mean(str_lr_int_preds == trainSubset$stress_bin) # Also .904

table(str_lr_int_preds, trainSubset$stress_bin) # Fewer false negatives, more false positives

test_threshold_vals(str_lr_int_prob, trainSubset$stress_bin,
                    test_thresholds = seq(.5, .8, by = .01)) # At 90% accuracy cutoff, best value is threshold = 0.63; only 898 false negatives (3.0%)
# At 88% accuracy, threshold = 0.77, only 484 false negatives (1.7%)

str_lr_nonlin <- glm(stress_bin ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), trainSubset,
             family = "binomial")
summary(str_lr_nonlin) # Fairly sparse, AIC = 13492
str_lr_nonlin_prob <- predict(str_lr_nonlin, type = "response")
str_lr_nonlin_preds <- rep("Low", length(str_lr_nonlin_prob))
str_lr_nonlin_preds[str_lr_nonlin_prob < .5] <- "High"
mean(str_lr_nonlin_preds == trainSubset$stress_bin) # .906

test_threshold_vals(str_lr_nonlin_prob, trainSubset$stress_bin,
                    test_thresholds = seq(.5, .8, by = .01)) # At 90% accuracy cutoff, best value is threshold = 0.62; only 946 false negatives (3.2%)
# At 88%, best threshold is 0.77, only 491 false negatives (1.7%)
```

## Anxiety

```{r Logistic Anxiety}
anx_lr <- glm(anxiety_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset, family = "binomial")
summary(anx_lr) # All sig, AIC = 14973
contrasts(trainSubset$anxiety_bin)

anx_lr_prob <- predict(anx_lr, type = "response")
anx_lr_preds <- rep("Low", length(anx_lr_prob))
anx_lr_preds[anx_lr_prob < .5] <- "High"
mean(anx_lr_preds == trainSubset$anxiety_bin) # .900

table(anx_lr_preds, trainSubset$anxiety_bin)

test_threshold_vals(anx_lr_prob, trainSubset$anxiety_bin) # At 88% accuracy cutoff, best value is threshold = 0.68; only 772 false negatives (2.6%)

anx_lr_int <- glm(anxiety_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2) + Q11A*TIPI4 + Q29A*TIPI4 + 
               Q28A*TIPI4 + Q10A*TIPI4 + Q21A*TIPI4,
             trainSubset, family = "binomial")
summary(anx_lr_int) # Interaction of Q11A sig, Q21A not sig, AIC = 14938
anx_lr_int_prob <- predict(anx_lr_int, type = "response")
anx_lr_int_preds <- rep("Low", length(anx_lr_int_prob))
anx_lr_int_preds[anx_lr_int_prob < .5] <- "High"
mean(anx_lr_int_preds == trainSubset$anxiety_bin) # .890

table(anx_lr_int_preds, trainSubset$anxiety_bin) # Fewer false negatives, more false positives

test_threshold_vals(anx_lr_int_prob, trainSubset$anxiety_bin) # At 88% accuracy cutoff, best value is threshold = 0.68; only 778 false negatives (2.6%)

anx_lr_nonlin <- glm(anxiety_bin ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), trainSubset,
             family = "binomial")
summary(anx_lr_nonlin) # Fairly non-sparse (dense?), AIC = 14932
anx_lr_nonlin_prob <- predict(anx_lr_nonlin, type = "response")
anx_lr_nonlin_preds <- rep("Low", length(anx_lr_nonlin_prob))
anx_lr_nonlin_preds[anx_lr_nonlin_prob < .5] <- "High"
mean(anx_lr_nonlin_preds == trainSubset$anxiety_bin) # .892

test_threshold_vals(anx_lr_nonlin_prob, trainSubset$anxiety_bin) # At 88% accuracy cutoff, best value is threshold = 0.68; only 782 false negatives (2.6%)
```

## Depression

```{r Logistic Depression}
dep_lr <- glm(depression_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainSubset, family = "binomial")
summary(dep_lr) # All sig except TIPI4, AIC = 10758
contrasts(trainSubset$depression_bin)

dep_lr_prob <- predict(dep_lr, type = "response")
dep_lr_preds <- rep("Low", length(dep_lr_prob))
dep_lr_preds[dep_lr_prob < .5] <- "High"
mean(dep_lr_preds == trainSubset$depression_bin) # .921

table(dep_lr_preds, trainSubset$depression_bin)

test_threshold_vals(dep_lr_prob, trainSubset$depression_bin,
                    test_thresholds = seq(.5, .85, by = .01)) # At 90% accuracy cutoff, best value is threshold = 0.8; only 342 false negatives (1.1%)

dep_lr_int <- glm(depression_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2) + Q29A*TIPI4 + Q28A*TIPI4 + 
               Q10A*TIPI4 + Q13A*TIPI4,
             trainSubset, family = "binomial")
summary(dep_lr_int) # Interaction of Q13A sig, Q29A and Q28A not sig, AIC = 10760
dep_lr_int_prob <- predict(dep_lr_int, type = "response")
dep_lr_int_preds <- rep("Low", length(dep_lr_int_prob))
dep_lr_int_preds[dep_lr_int_prob < .5] <- "High"
mean(dep_lr_int_preds == trainSubset$depression_bin) # .921

table(dep_lr_int_preds, trainSubset$depression_bin) # Fewer false negatives and false positives

test_threshold_vals(dep_lr_int_prob, trainSubset$depression_bin,
                    test_thresholds = seq(.5, .85, by = .01)) # At 90% accuracy cutoff, best value is threshold = 0.8; only 345 false negatives (1.2%)

dep_lr_nonlin <- glm(depression_bin ~ poly(Q11A, 3) + Q27A + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 2) + poly(Q40A, 3) + poly(Q10A, 3) + 
             poly(Q13A, 2) + poly(Q21A, 3) + bs(TIPI4, df = 5), trainSubset,
             family = "binomial")
summary(dep_lr_nonlin) # Quite sparse, AIC = 10738
dep_lr_nonlin_prob <- predict(dep_lr_nonlin, type = "response")
dep_lr_nonlin_preds <- rep("Low", length(dep_lr_nonlin_prob))
dep_lr_nonlin_preds[dep_lr_nonlin_prob < .5] <- "High"
mean(dep_lr_nonlin_preds == trainSubset$depression_bin) # .921

test_threshold_vals(dep_lr_nonlin_prob, trainSubset$depression_bin,
                    test_thresholds = seq(.5, .85, by = .01)) # At 90% accuracy cutoff, best value is threshold = 0.81; only 336 false negatives (1.1%)
```

# KNN

## Stress

### Classification

```{r KNN Stress Classification}
trainSubset_tr <- trainSubset[1:(dim(trainSubset)[1] / 2),]
trainSubset_te <- trainSubset[((dim(trainSubset)[1] + 1) / 2):dim(trainSubset)[1],]

trainSubset_tr_X <- trainSubset_tr %>% 
  select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
trainSubset_te_X <- trainSubset_te %>% 
  select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)

set.seed(1)
str_knn <- knn(trainSubset_tr_X, trainSubset_te_X,
               trainSubset_tr$stress_bin, k = 1)
table(str_knn, trainSubset_te$stress_bin)
sum(diag(table(str_knn, trainSubset_te$stress_bin))) / sum(table(str_knn, trainSubset_te$stress_bin)) # Accuracy = .869

test_knn <- function(train, test, truth, test_truth, k_test = seq(1, 25, by = 1)){
  for(i in 1:length(k_test)){
    set.seed(1)
    knn_out <- knn(train, test, truth, k = k_test[i])
    accuracy <- sum(diag(table(knn_out, test_truth))) /
      sum(table(knn_out, test_truth))
    print(paste("Test:", i))
    print(paste("K =", k_test[i]))
    print(paste("Accuracy:", round(accuracy, 3)))
    print(table(knn_out, test_truth))
  }
}

test_knn(trainSubset_tr_X, trainSubset_te_X, trainSubset_tr$stress_bin,
         trainSubset_te$stress_bin) # Doesn't increase much past K = 10, accuracy = .895 (slightly worse but comparable to logistic)
test_knn(trainSubset_tr_X, trainSubset_te_X, trainSubset_tr$stress_bin,
         trainSubset_te$stress_bin, k_test = seq(1, 200, by = 5)) # Never gets better

test_k_thresholds <- function(train, test, truth, test_truth, k,
                              thresh = seq(.5, .7, by = .01)){
  set.seed(1)
  knn_model <- class::knn(train, test, truth, k, prob = T)
  preds <- rep("Low", length(knn_model))
  preds[knn_model == "High"] <- "High"
  for(i in 1:length(thresh)){
    preds2 <- if_else(attr(knn_model, "prob") <= thresh[i], "High", preds)
    confusion <- table(preds2, test_truth)
    accuracy <- mean(preds2 == test_truth)
    fn <- mean(preds2 == "Low" & test_truth == "High")
    print(paste("Test:", i))
    print(paste("Threshold:", thresh[i]))
    print(paste("Accuracy:", accuracy))
    print(paste("False Negative Rate:", fn))
    print(confusion)
  }
}

test_k_thresholds(trainSubset_tr_X, trainSubset_te_X, trainSubset_tr$stress_bin,
                  trainSubset_te$stress_bin, 10) # At 88% accuracy: set threshold = 0.7, get false negative rate down to .026
```

### Regression

```{r KNN Stress Regression}
trainSubset_X <- trainSubset %>% 
  select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)

set.seed(1)
str_knn_reg <- knn.reg(trainSubset_X, y = trainSubset$stress_score, k = 3)
str_knn_reg # Rsq (predictive) = .840

knn_reg <- function(train, truth, k_test = seq(1, 49, by = 2)){
  for(i in 1:length(k_test)){
      set.seed(1)
    reg <- knn.reg(train, y = truth, k = k_test[i])
    print(paste("K:", k_test[i]))
    print(reg)
  }
}

knn_reg(trainSubset_X, trainSubset$stress_score) # Peaks about K = 25, rsq = .865, almost as good as linear
```


## Anxiety

### Classification

```{r KNN Anx Classification}
set.seed(1)
anx_knn <- knn(trainSubset_tr_X, trainSubset_te_X,
               trainSubset_tr$anxiety_bin, k = 1)
table(anx_knn, trainSubset_te$anxiety_bin)
sum(diag(table(anx_knn, trainSubset_te$anxiety_bin))) / sum(table(anx_knn, trainSubset_te$anxiety_bin)) # Accuracy = .854

test_knn(trainSubset_tr_X, trainSubset_te_X, trainSubset_tr$anxiety_bin,
         trainSubset_te$anxiety_bin) # Doesn't increase much past K = 12, accuracy = .881 (slightly worse but comparable to logistic)
test_knn(trainSubset_tr_X, trainSubset_te_X, trainSubset_tr$anxiety_bin,
         trainSubset_te$anxiety_bin, k_test = seq(1, 200, by = 5)) # Never gets better

test_k_thresholds(trainSubset_tr_X, trainSubset_te_X, trainSubset_tr$anxiety_bin,
                  trainSubset_te$anxiety_bin, 12) # At 87% acuracy, best threshold is .64
```

### Regression

```{r KNN Anx Regression}
set.seed(1)
anx_knn_reg <- knn.reg(trainSubset_X, y = trainSubset$anxiety_score, k = 3)
anx_knn_reg # Rsq (predictive) = .741

knn_reg(trainSubset_X, trainSubset$anxiety_score)

knn_reg(trainSubset_X, trainSubset$anxiety_score,
        k_test = seq(45, 65, by = 2)) # Peaks about K = 45, rsq = .786, almost identical to linear
```


## Depression

### Classification

```{r KNN Dep Classification}
set.seed(1)
dep_knn <- knn(trainSubset_tr_X, trainSubset_te_X,
               trainSubset_tr$depression_bin, k = 1)
table(anx_knn, trainSubset_te$depression_bin)
sum(diag(table(dep_knn, trainSubset_te$depression_bin))) / sum(table(dep_knn, trainSubset_te$depression_bin)) # Accuracy = .895

test_knn(trainSubset_tr_X, trainSubset_te_X, trainSubset_tr$depression_bin,
         trainSubset_te$depression_bin) # Doesn't increase much past K = 9, accuracy = .915 (slightly worse but comparable to logistic)
test_knn(trainSubset_tr_X, trainSubset_te_X, trainSubset_tr$depression_bin,
         trainSubset_te$depression_bin, k_test = seq(1, 200, by = 5)) # Never gets better

test_k_thresholds(trainSubset_tr_X, trainSubset_te_X, trainSubset_tr$depression_bin,
                  trainSubset_te$depression_bin, 9, thresh = seq(.5, .8, by = .01)) # At 90% accuracy, threshold is .73
```

### Regression

```{r KNN Dep Regression}
set.seed(1)
dep_knn_reg <- knn.reg(trainSubset_X, y = trainSubset$depression_score, k = 3)
dep_knn_reg # Rsq (predictive) = .869

knn_reg(trainSubset_X, trainSubset$depression_score)

knn_reg(trainSubset_X, trainSubset$depression_score,
        k_test = seq(45, 65, by = 2)) # Peaks about K = 47, rsq = .896, almost as good as linear
```


# Cross-Validation & Selection


```{r CV-Setup}
set.seed(325)
folds <- vfold_cv(trainSubset, v = 10)
folds2 <- vfold_cv(trainSubset, v = 3)
grid <- 10^seq(10, -2, length = 100)
```

## Stress

Here's what we plan to do with this: first, if we have to find a better way to make the cv happen, so be it. We are going to include ALL models that seem possibly reasonable - linear, ridge & lasso, nonlinear GAMs, logistic regression, and KNN regression AND classification. Trees will be run on their own, using OOB samples (will look into boosting) to estimate testing error. We will take that testing error with a grain of salt, since it will be higher than the CV testing error here (but we cannot realistically train those models ten times). We will compare across BOTH the full prediction (i.e., true stress score) and the reduced prediction (i.e., stress score not including these variables). BOTH will be used for selection. We will also calculate MSE on the transformed data, and see if they are better predictively (need to rescale the MSE to be comparable). Will eventually decide how to differentiate MSE and MCE, but generally speaking a continuous predictor here can almost certainly only be better than a categorical one. 

### Untransformed

```{r CV Stress Untransformed}
#v <- 3
#folds_test <- folds2
v <- 10
folds_test <- folds

str_resp <- trainSubset$stress_score
str_pred_1 <- model.matrix(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset)[,-1]
str_pred_2 <- model.matrix(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 5),
                            trainSubset)[,-1]

str_lin_1_mse <- rep(0, v)
str_lin_2_mse <- rep(0, v)

str_r1_mse <- rep(0, v)
str_r2_mse <- rep(0, v)
str_l1_mse <- rep(0, v)
str_l2_mse <- rep(0, v)

str_nonlin_1_mse <- rep(0, v)
str_nonlin_2_mse <- rep(0, v)
str_nonlin_3_mse <- rep(0, v)
str_nonlin_4_mse <- rep(0, v)

str_lr_1A_mce <- rep(0, v)
str_lr_1B_mce <- rep(0, v)
str_lr_2A_mce <- rep(0, v)
str_lr_2B_mce <- rep(0, v)
str_lr_3A_mce <- rep(0, v)
str_lr_3B_mce <- rep(0, v)

str_lr_1A_fn <- rep(0, v)
str_lr_1B_fn <- rep(0, v)
str_lr_2A_fn <- rep(0, v)
str_lr_2B_fn <- rep(0, v)
str_lr_3A_fn <- rep(0, v)
str_lr_3B_fn <- rep(0, v)

str_knn_1_mce <- rep(0, v)
str_knn_1_fn <- rep(0, v)
str_knn_2_mce <- rep(0, v)
str_knn_2_fn <- rep(0, v)
str_knn_mse <- rep(0, v)

for(i in 1:v){
  indices <- folds_test$splits[[i]][[2]]
  correct_indices <- vector("logical", dim(trainSubset)[1])
  for(j in 1:dim(trainSubset)[1]){
    if(j %in% indices){
      correct_indices[j] = T
    } else{
      correct_indices[j] = F
    }
  }
  
  data_tr <- trainSubset[correct_indices,]
  data_test <- trainSubset[!correct_indices,]
  
  data_tr_X <- data_tr %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  data_te_X <- data_test %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  
  
  # Linear
  str_lin_1 <- lm(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, data_tr)
  str_lin_1_mse[i] <- mean((predict(str_lin_1, data_test) -
                              data_test$stress_score)^2)
  str_lin_2 <- lm(stress_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 5), data_tr)
  str_lin_2_mse[i] <- mean((predict(str_lin_2, data_test) - 
                             data_test$stress_score)^2)
  
  # Shrinkage
  set.seed(1)
  str_r1 <- glmnet(str_pred_1[correct_indices,], str_resp[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  str_r1_cv <- cv.glmnet(str_pred_1[correct_indices,],
                              str_resp[correct_indices], alpha = 0)
  bestlam_str_r1 <- str_r1_cv$lambda.min
  pred_str_r1 <- predict(str_r1, s = bestlam_str_r1,
                         newx = str_pred_1[!correct_indices,])
  str_r1_mse[i] <- mean((pred_str_r1 - data_test$stress_score)^2)
  
  set.seed(1)
  str_r2 <- glmnet(str_pred_2[correct_indices,], str_resp[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  str_r2_cv <- cv.glmnet(str_pred_2[correct_indices,],
                              str_resp[correct_indices], alpha = 0)
  bestlam_str_r2 <- str_r2_cv$lambda.min
  pred_str_r2 <- predict(str_r2, s = bestlam_str_r2,
                         newx = str_pred_2[!correct_indices,])
  str_r2_mse[i] <- mean((pred_str_r2 - data_test$stress_score)^2)
  
  
  set.seed(1)
  str_l1 <- glmnet(str_pred_1[correct_indices,], str_resp[correct_indices],
                   alpha = 1, lambda = grid)
  str_l1_cv <- cv.glmnet(str_pred_1[correct_indices,],
                              str_resp[correct_indices], alpha = 1)
  bestlam_str_l1 <- str_l1_cv$lambda.min
  pred_str_l1 <- predict(str_l1, s = bestlam_str_l1,
                         newx = str_pred_1[!correct_indices,])
  str_l1_mse[i] <- mean((pred_str_l1 - data_test$stress_score)^2)
  
  set.seed(1)
  str_l2 <- glmnet(str_pred_2[correct_indices,], str_resp[correct_indices],
                   alpha = 1, lambda = grid)
  str_l2_cv <- cv.glmnet(str_pred_2[correct_indices,],
                              str_resp[correct_indices], alpha = 1)
  bestlam_str_l2 <- str_l2_cv$lambda.min
  pred_str_l2 <- predict(str_l2, s = bestlam_str_l2,
                         newx = str_pred_2[!correct_indices,])
  str_l2_mse[i] <- mean((pred_str_l2 - data_test$stress_score)^2)
  
  
  # Nonlinear
  str_nonlin_1 <- lm(stress_score ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), data_tr)
  str_nonlin_1_mse[i] <- mean((predict(str_nonlin_1, data_test) -
                                 data_test$stress_score)^2)
  
  str_nonlin_2 <- lm(stress_score ~ Q11A + Q27A + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             bs(TIPI4, df = 5), data_tr)
  str_nonlin_2_mse[i] <- mean((predict(str_nonlin_2, data_test) - 
                                 data_test$stress_score)^2)
  
  str_nonlin_3 <- lm(stress_score ~ Q11A + Q27A + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             ns(TIPI4, df = 5), data_tr)
  str_nonlin_3_mse[i] <- mean((predict(str_nonlin_3, data_test) - 
                                 data_test$stress_score)^2)
  
  str_nonlin_4 <- lm(stress_score ~ Q11A + Q27A + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             poly(TIPI4, 3), data_tr)
  str_nonlin_4_mse[i] <- mean((predict(str_nonlin_4, data_test) - 
                                 data_test$stress_score)^2)
  
  # Logistic
  str_lr_1 <- glm(stress_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, data_tr, family = "binomial")
  str_lr_1_prob <- predict(str_lr_1, data_test, type = "response")
  preds_str_lr_1A <- rep("Low", length(str_lr_1_prob))
  preds_str_lr_1A[str_lr_1_prob < .5] <- "High"
  str_lr_1A_mce[i] <- mean(preds_str_lr_1A != data_test$stress_bin)
  str_lr_1A_fn[i] <- mean(preds_str_lr_1A == "Low" &
                            data_test$stress_bin == "High")
  preds_str_lr_1B <- rep("Low", length(str_lr_1_prob))
  preds_str_lr_1B[str_lr_1_prob < .62] <- "High"
  str_lr_1B_mce[i] <- mean(preds_str_lr_1B != data_test$stress_bin)
  str_lr_1B_fn[i] <- mean(preds_str_lr_1B == "Low" &
                            data_test$stress_bin == "High")
  
  str_lr_2 <- glm(stress_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2) + Q11A*TIPI4 + Q27A*TIPI4 + 
               Q13A*TIPI4 + Q21A*TIPI4, data_tr, family = "binomial")
  str_lr_2_prob <- predict(str_lr_2, data_test, type = "response")
  preds_str_lr_2A <- rep("Low", length(str_lr_2_prob))
  preds_str_lr_2A[str_lr_2_prob < .5] <- "High"
  str_lr_2A_mce[i] <- mean(preds_str_lr_2A != data_test$stress_bin)
  str_lr_2A_fn[i] <- mean(preds_str_lr_2A == "Low" &
                            data_test$stress_bin == "High")
  preds_str_lr_2B <- rep("Low", length(str_lr_2_prob))
  preds_str_lr_2B[str_lr_2_prob < .63] <- "High"
  str_lr_2B_mce[i] <- mean(preds_str_lr_2B != data_test$stress_bin)
  str_lr_2B_fn[i] <- mean(preds_str_lr_2B == "Low" &
                            data_test$stress_bin == "High")
  
  str_lr_3 <- glm(stress_bin ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), data_tr,
             family = "binomial")
  str_lr_3_prob <- predict(str_lr_3, data_test, type = "response")
  preds_str_lr_3A <- rep("Low", length(str_lr_3_prob))
  preds_str_lr_3A[str_lr_3_prob < .5] <- "High"
  str_lr_3A_mce[i] <- mean(preds_str_lr_3A != data_test$stress_bin)
  str_lr_3A_fn[i] <- mean(preds_str_lr_3A == "Low" &
                            data_test$stress_bin == "High")
  preds_str_lr_3B <- rep("Low", length(str_lr_3_prob))
  preds_str_lr_3B[str_lr_3_prob < .62] <- "High"
  str_lr_3B_mce[i] <- mean(preds_str_lr_3B != data_test$stress_bin)
  str_lr_3B_fn[i] <- mean(preds_str_lr_3B == "Low" &
                            data_test$stress_bin == "High")
  
  # KNN
  set.seed(1)
  str_knn_class <- class::knn(data_tr_X, data_te_X,
                       data_tr$stress_bin, k = 10, prob = T)
  str_knn_1_mce[i] <- 1 - (sum(diag(table(str_knn_class, data_test$stress_bin))) /
                           sum(table(str_knn_class, data_test$stress_bin)))
  str_knn_1_fn[i] <- mean(str_knn_class == "Low" & data_test$stress_bin == "High")
  str_knn_preds <- rep("Low", length(str_knn_class))
  str_knn_preds[str_knn_class == "High"] <- "High"
  str_knn_preds_2 <- if_else(attr(str_knn_class, "prob") <= .7,
                             "High", str_knn_preds)
  str_knn_2_mce[i] <- 1 - (sum(diag(table(str_knn_preds_2, data_test$stress_bin))) /
                             sum(table(str_knn_preds_2, data_test$stress_bin)))
  str_knn_2_fn[i] <- mean(str_knn_preds_2 == "Low" & data_test$stress_bin == "High")
  
  set.seed(1)
  str_knn_reg <- knn.reg(data_tr_X, test = data_te_X,
                         y = data_tr$stress_score, k = 25)
  str_knn_mse[i] <- mean((str_knn_reg$pred - data_test$stress_score)^2)
}
```

### Reduced

```{r CV Stress Reduced}
str_resp_red <- trainSubset$stress_score_reduced
str_pred_1_red <- model.matrix(stress_score_reduced ~ Q11A + Q27A + Q29A + Q9A +
                                 Q28A + Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset)[,-1]
str_pred_2_red <- model.matrix(stress_score_reduced ~ Q11A + Q27A + Q29A + Q9A +
                                 Q28A + Q40A + Q10A + Q13A + Q21A +
                                 poly(TIPI4, 5), trainSubset)[,-1]

str_lin_1_mse_red <- rep(0, v)
str_lin_2_mse_red <- rep(0, v)

str_r1_mse_red <- rep(0, v)
str_r2_mse_red <- rep(0, v)
str_l1_mse_red <- rep(0, v)
str_l2_mse_red <- rep(0, v)

str_nonlin_1_mse_red <- rep(0, v)
str_nonlin_2_mse_red <- rep(0, v)
str_nonlin_3_mse_red <- rep(0, v)
str_nonlin_4_mse_red <- rep(0, v)

str_knn_mse_red <- rep(0, v)

for(i in 1:v){
  indices <- folds_test$splits[[i]][[2]]
  correct_indices <- vector("logical", dim(trainSubset)[1])
  for(j in 1:dim(trainSubset)[1]){
    if(j %in% indices){
      correct_indices[j] = T
    } else{
      correct_indices[j] = F
    }
  }
  
  data_tr <- trainSubset[correct_indices,]
  data_test <- trainSubset[!correct_indices,]
  
  data_tr_X <- data_tr %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  data_te_X <- data_test %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  
  
  # Linear
  str_lin_1_red <- lm(stress_score_reduced ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                        Q40A + Q10A + Q13A + Q21A + TIPI4, data_tr)
  str_lin_1_mse_red[i] <- mean((predict(str_lin_1_red, data_test) -
                              data_test$stress_score_reduced)^2)
  str_lin_2_red <- lm(stress_score_reduced ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                        Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 5), data_tr)
  str_lin_2_mse_red[i] <- mean((predict(str_lin_2_red, data_test) - 
                             data_test$stress_score_reduced)^2)
  
  # Shrinkage
  set.seed(1)
  str_r1_red <- glmnet(str_pred_1_red[correct_indices,],
                       str_resp_red[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  str_r1_cv_red <- cv.glmnet(str_pred_1_red[correct_indices,],
                              str_resp_red[correct_indices], alpha = 0)
  bestlam_str_r1_red <- str_r1_cv_red$lambda.min
  pred_str_r1_red <- predict(str_r1_red, s = bestlam_str_r1_red,
                         newx = str_pred_1_red[!correct_indices,])
  str_r1_mse_red[i] <- mean((pred_str_r1_red - data_test$stress_score_reduced)^2)
  
  set.seed(1)
  str_r2_red <- glmnet(str_pred_2_red[correct_indices,],
                       str_resp_red[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  str_r2_cv_red <- cv.glmnet(str_pred_2_red[correct_indices,],
                              str_resp_red[correct_indices], alpha = 0)
  bestlam_str_r2_red <- str_r2_cv_red$lambda.min
  pred_str_r2_red <- predict(str_r2_red, s = bestlam_str_r2_red,
                         newx = str_pred_2_red[!correct_indices,])
  str_r2_mse_red[i] <- mean((pred_str_r2_red - data_test$stress_score_reduced)^2)
  
  
  set.seed(1)
  str_l1_red <- glmnet(str_pred_1_red[correct_indices,],
                       str_resp_red[correct_indices],
                   alpha = 1, lambda = grid)
  str_l1_cv_red <- cv.glmnet(str_pred_1_red[correct_indices,],
                              str_resp_red[correct_indices], alpha = 1)
  bestlam_str_l1_red <- str_l1_cv_red$lambda.min
  pred_str_l1_red <- predict(str_l1_red, s = bestlam_str_l1_red,
                         newx = str_pred_1_red[!correct_indices,])
  str_l1_mse_red[i] <- mean((pred_str_l1_red - data_test$stress_score_reduced)^2)
  
  set.seed(1)
  str_l2_red <- glmnet(str_pred_2_red[correct_indices,],
                       str_resp_red[correct_indices],
                   alpha = 1, lambda = grid)
  str_l2_cv_red <- cv.glmnet(str_pred_2_red[correct_indices,],
                              str_resp_red[correct_indices], alpha = 1)
  bestlam_str_l2_red <- str_l2_cv_red$lambda.min
  pred_str_l2_red <- predict(str_l2_red, s = bestlam_str_l2_red,
                         newx = str_pred_2_red[!correct_indices,])
  str_l2_mse_red[i] <- mean((pred_str_l2_red - data_test$stress_score_reduced)^2)
  
  
  # Nonlinear
  str_nonlin_1_red <- lm(stress_score_reduced ~ poly(Q11A, 2) + poly(Q27A, 2) +
                           poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), data_tr)
  str_nonlin_1_mse_red[i] <- mean((predict(str_nonlin_1_red, data_test) -
                                 data_test$stress_score_reduced)^2)
  
  str_nonlin_2_red <- lm(stress_score_reduced ~ Q11A + Q27A + poly(Q29A, 2) +
                           poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             bs(TIPI4, df = 5), data_tr)
  str_nonlin_2_mse_red[i] <- mean((predict(str_nonlin_2_red, data_test) - 
                                 data_test$stress_score_reduced)^2)
  
  str_nonlin_3_red <- lm(stress_score_reduced ~ Q11A + Q27A + poly(Q29A, 2) +
                           poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             ns(TIPI4, df = 5), data_tr)
  str_nonlin_3_mse_red[i] <- mean((predict(str_nonlin_3_red, data_test) - 
                                 data_test$stress_score_reduced)^2)
  
  str_nonlin_4_red <- lm(stress_score_reduced ~ Q11A + Q27A + poly(Q29A, 2) +
                           poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             poly(TIPI4, 3), data_tr)
  str_nonlin_4_mse_red[i] <- mean((predict(str_nonlin_4_red, data_test) - 
                                 data_test$stress_score_reduced)^2)
  
  
  # KNN
  set.seed(1)
  str_knn_reg_red <- knn.reg(data_tr_X, test = data_te_X,
                         y = data_tr$stress_score_reduced, k = 25)
  str_knn_mse_red[i] <- mean((str_knn_reg_red$pred -
                                data_test$stress_score_reduced)^2)
}
```

### Transformed

I may not actually run this, even after all this work - minimal evidence it'll perform. However, if I have time I will!

```{r CV Stress Trans}
str_resp_t <- trainSubset$stress_trans
str_pred_1_t <- model.matrix(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset)[,-1]
str_pred_2_t <- model.matrix(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 2),
                            trainSubset)[,-1]

str_lin_1_mse_t <- rep(0, 10)
str_lin_2_mse_t <- rep(0, 10)

str_r1_mse_t <- rep(0, 10)
str_r2_mse_t <- rep(0, 10)
str_l1_mse_t <- rep(0, 10)
str_l2_mse_t <- rep(0, 10)

str_nonlin_1_mse_t <- rep(0, 10)
str_nonlin_2_mse_t <- rep(0, 10)
str_nonlin_3_mse_t <- rep(0, 10)
str_nonlin_4_mse_t <- rep(0, 10)

str_knn_mse_t <- rep(0, 10)

for(i in 1:10){
  indices <- folds$splits[[i]][[2]]
  correct_indices <- vector("logical", dim(trainSubset)[1])
  for(j in 1:dim(trainSubset)[1]){
    if(j %in% indices){
      correct_indices[j] = T
    } else{
      correct_indices[j] = F
    }
  }
  
  data_tr <- trainSubset[correct_indices,]
  data_test <- trainSubset[!correct_indices,]
  
  data_tr_X <- data_tr %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  data_te_X <- data_test %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  
  
  # Linear
  str_lin_1_t <- lm(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, data_tr)
  str_lin_1_mse_t[i] <- mean((predict(str_lin_1_t, data_test) -
                              data_test$stress_trans)^2)
  str_lin_2_t <- lm(stress_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2), data_tr)
  str_lin_2_mse_t[i] <- mean((predict(str_lin_2_t, data_test) - 
                             data_test$stress_trans)^2)
  
  # Shrinkage
  set.seed(1)
  str_r1_t <- glmnet(str_pred_1_t[correct_indices,], str_resp_t[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  str_r1_cv_t <- cv.glmnet(str_pred_1_t[correct_indices,],
                              str_resp_t[correct_indices], alpha = 0)
  bestlam_str_r1_t <- str_r1_cv_t$lambda.min
  pred_str_r1_t <- predict(str_r1_t, s = bestlam_str_r1_t,
                         newx = str_pred_1_t[!correct_indices,])
  str_r1_mse_t[i] <- mean((pred_str_r1_t - data_test$stress_trans)^2)
  
  set.seed(1)
  str_r2_t <- glmnet(str_pred_2_t[correct_indices,], str_resp_t[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  str_r2_cv_t <- cv.glmnet(str_pred_2_t[correct_indices,],
                              str_resp_t[correct_indices], alpha = 0)
  bestlam_str_r2_t <- str_r2_cv_t$lambda.min
  pred_str_r2_t <- predict(str_r2_t, s = bestlam_str_r2_t,
                         newx = str_pred_2_t[!correct_indices,])
  str_r2_mse_t[i] <- mean((pred_str_r2_t - data_test$stress_trans)^2)
  
  
  set.seed(1)
  str_l1_t <- glmnet(str_pred_1_t[correct_indices,], str_resp_t[correct_indices],
                   alpha = 1, lambda = grid)
  str_l1_cv_t <- cv.glmnet(str_pred_1_t[correct_indices,],
                              str_resp_t[correct_indices], alpha = 1)
  bestlam_str_l1_t <- str_l1_cv_t$lambda.min
  pred_str_l1_t <- predict(str_l1_t, s = bestlam_str_l1_t,
                         newx = str_pred_1_t[!correct_indices,])
  str_l1_mse_t[i] <- mean((pred_str_l1_t - data_test$stress_trans)^2)
  
  set.seed(1)
  str_l2_t <- glmnet(str_pred_2_t[correct_indices,], str_resp_t[correct_indices],
                   alpha = 1, lambda = grid)
  str_l2_cv_t <- cv.glmnet(str_pred_2_t[correct_indices,],
                              str_resp_t[correct_indices], alpha = 1)
  bestlam_str_l2_t <- str_l2_cv_t$lambda.min
  pred_str_l2_t <- predict(str_l2_t, s = bestlam_str_l2_t,
                         newx = str_pred_2_t[!correct_indices,])
  str_l2_mse_t[i] <- mean((pred_str_l2_t - data_test$stress_trans)^2)
  
  
  # Nonlinear
  str_nonlin_1_t <- lm(stress_trans ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), data_tr)
  str_nonlin_1_mse_t[i] <- mean((predict(str_nonlin_1_t, data_test) -
                                 data_test$stress_trans)^2)
  
  str_nonlin_2_t <- lm(stress_trans ~ Q11A + Q27A + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             bs(TIPI4, df = 5), data_tr)
  str_nonlin_2_mse_t[i] <- mean((predict(str_nonlin_2_t, data_test) - 
                                 data_test$stress_trans)^2)
  
  str_nonlin_3_t <- lm(stress_trans ~ Q11A + Q27A + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             ns(TIPI4, df = 5), data_tr)
  str_nonlin_3_mse_t[i] <- mean((predict(str_nonlin_3_t, data_test) - 
                                 data_test$stress_trans)^2)
  
  str_nonlin_4_t <- lm(stress_trans ~ Q11A + Q27A + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             poly(TIPI4, 3), data_tr)
  str_nonlin_4_mse_t[i] <- mean((predict(str_nonlin_4_t, data_test) - 
                                 data_test$stress_trans)^2)
  
  # KNN
  set.seed(1)
  str_knn_reg_t <- knn.reg(data_tr_X, test = data_te_X,
                         y = data_tr$stress_trans, k = 25)
  str_knn_mse_t[i] <- mean((str_knn_reg_t$pred - data_test$stress_trans)^2)
}
```

### Output

It's impossible to properly recover MSE of transformed variable to compare - we will leave it out for stress & depression (showed minimal evidence of a useful transformation), but will still have to consider through other methods for anxiety (showed real evidence).

```{r CV Stress Output}
get_se <- function(mse){
  ret = 0
  for(i in 1:v){
    ret = ret + (mse[i] - mean(mse))^2
  }
  return(sqrt(ret / 9))
}

str_mse <- data.frame(t(c(Model = "str_lin_1", MSE = mean(str_lin_1_mse),
                          SD = get_se(str_lin_1_mse),
                          MSE_red = mean(str_lin_1_mse_red),
                          SD_red = get_se(str_lin_1_mse_red))))
str_mse <- rbind(str_mse,
                 c("str_lin_2", mean(str_lin_2_mse), get_se(str_lin_2_mse),
                   mean(str_lin_2_mse_red), get_se(str_lin_2_mse_red)),
                 c("str_r1", mean(str_r1_mse), get_se(str_r1_mse),
                   mean(str_r1_mse_red), get_se(str_r1_mse_red)),
                 c("str_r2", mean(str_r2_mse), get_se(str_r2_mse),
                   mean(str_r2_mse_red), get_se(str_r2_mse_red)),
                 c("str_l1", mean(str_l1_mse), get_se(str_l1_mse),
                   mean(str_l1_mse_red), get_se(str_l1_mse_red)),
                 c("str_l2", mean(str_l2_mse), get_se(str_l2_mse),
                   mean(str_l2_mse_red), get_se(str_l2_mse_red)),
                 c("str_nonlin_1", mean(str_nonlin_1_mse),
                   get_se(str_nonlin_1_mse), mean(str_nonlin_1_mse_red),
                   get_se(str_nonlin_1_mse_red)),
                 c("str_nonlin_2", mean(str_nonlin_2_mse),
                   get_se(str_nonlin_2_mse), mean(str_nonlin_2_mse_red),
                   get_se(str_nonlin_2_mse_red)),
                 c("str_nonlin_3", mean(str_nonlin_3_mse),
                   get_se(str_nonlin_3_mse), mean(str_nonlin_3_mse_red),
                   get_se(str_nonlin_3_mse_red)),
                 c("str_nonlin_4", mean(str_nonlin_4_mse),
                   get_se(str_nonlin_4_mse), mean(str_nonlin_4_mse_red),
                   get_se(str_nonlin_4_mse_red)),
                 c("str_knn_reg", mean(str_knn_mse), get_se(str_knn_mse),
                   mean(str_knn_mse_red), get_se(str_knn_mse_red)))

str_mse %>% 
  arrange(MSE, MSE_red) %>% 
  knitr::kable(digits = 3)

str_mse %>% 
  mutate(sd_1 = as.numeric(MSE) + as.numeric(SD)) %>% 
  arrange(sd_1) # No changes


str_mce <- data.frame(t(c(Model = "str_lr_1A", MCE = mean(str_lr_1A_mce),
                          SD = get_se(str_lr_1A_mce), FN = mean(str_lr_1A_fn),
                          FN_SD = get_se(str_lr_1A_fn))))
str_mce <- rbind(str_mce,
                 c("str_lr_1B", mean(str_lr_1B_mce), get_se(str_lr_1B_mce),
                   mean(str_lr_1B_fn), get_se(str_lr_1B_fn)),
                 c("str_lr_2A", mean(str_lr_2A_mce), get_se(str_lr_2A_mce),
                   mean(str_lr_2A_fn), get_se(str_lr_2A_fn)),
                 c("str_lr_2B", mean(str_lr_2B_mce), get_se(str_lr_2B_mce),
                   mean(str_lr_2B_fn), get_se(str_lr_2B_fn)),
                 c("str_lr_3A", mean(str_lr_3A_mce), get_se(str_lr_3A_mce),
                   mean(str_lr_3A_fn), get_se(str_lr_3A_fn)),
                 c("str_lr_3B", mean(str_lr_3B_mce), get_se(str_lr_3B_mce),
                   mean(str_lr_3B_fn), get_se(str_lr_3B_fn)),
                 c("str_knn_1", mean(str_knn_1_mce), get_se(str_knn_1_mce),
                   mean(str_knn_1_fn), get_se(str_knn_1_fn)),
                 c("str_knn_2", mean(str_knn_2_mce), get_se(str_knn_2_mce),
                   mean(str_knn_2_fn), get_se(str_knn_2_fn)))

str_mce %>% 
  arrange(FN, MCE) %>% 
  knitr::kable(digits = 3) # Admittedly has the highest MCE (~12%), but we can really drive down the false negative rate here
```

### Comparison on New Data

The best way I can think of to compare the tree and non-tree error rates (even though I wanted to avoid this) is to test their predictions on new data. I will only compare the best 1-2 non-tree models to the tree models.

```{r New Data Stress}
str_nonlin_4_test <- lm(stress_score ~ Q11A + Q27A + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             poly(TIPI4, 3), trainSubset)
str_nonlin_4_test_mse <- mean((predict(str_nonlin_4_test, testData) -
                                 testData$stress_score)^2) # 14.339

str_nonlin_4_red_test <- lm(stress_score_reduced ~ Q11A + Q27A + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             poly(TIPI4, 3), trainSubset)
str_nonlin_4_red_test_mse <- mean((predict(str_nonlin_4_red_test, testData) - 
                                     testData$stress_score_reduced)^2) # 14.339

str_bag_test_mse <- mean((predict(str_bag, testData) - testData$stress_score)^2) # 16.301

str_bag_red_test_mse <- mean((predict(str_bag_red, testData) -
                                testData$stress_score_reduced)^2) # 16.187

str_rf_test_mse <- mean((predict(str_rf, testData) - testData$stress_score)^2) # 15.186

str_rf_red_test_mse <- mean((predict(str_rf_red, testData) -
                               testData$stress_score_reduced)^2) # 15.119

# Literally zero evidence that these are outperforming ANY non-tree models, except maybe KNN reg

trainData_X <- trainSubset %>% 
  select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)

testData_X <- testData %>% 
  select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  
set.seed(1)
str_knn_class_test <- class::knn(trainData_X, testData_X,
                       trainSubset$stress_bin, k = 10, prob = T)
str_knn_preds_test <- rep("Low", length(str_knn_class_test))
str_knn_preds_test[str_knn_class_test == "High"] <- "High"
str_knn_preds_2_test <- if_else(attr(str_knn_class_test, "prob") <= .7,
                             "High", str_knn_preds_test)
str_knn_2_mce_test <- 1 - (sum(diag(table(str_knn_preds_2_test, testData$stress_bin))) /
                             sum(table(str_knn_preds_2_test, testData$stress_bin))) # 12.1%
str_knn_2_fn_test <- mean(str_knn_preds_2_test == "Low" & testData$stress_bin == "High") # 2.4%
  
  

str_lr_2_test <- glm(stress_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2) + Q11A*TIPI4 + Q27A*TIPI4 + 
               Q13A*TIPI4 + Q21A*TIPI4, trainSubset, family = "binomial")
str_lr_2_prob_test <- predict(str_lr_2_test, testData, type = "response")
preds_str_lr_2B_test <- rep("Low", length(str_lr_2_prob_test))
preds_str_lr_2B_test[str_lr_2_prob_test < .63] <- "High"
str_lr_2B_mce_test <- mean(preds_str_lr_2B_test != testData$stress_bin) # 10.5%
str_lr_2B_fn_test <- mean(preds_str_lr_2B_test == "Low" &
                            testData$stress_bin == "High") # 3.2%

str_bag_bin_pred_test <- predict(str_bag_bin, testData)
str_bag_bin_mce_test <- mean(str_bag_bin_pred_test != testData$stress_bin) # 11.3%
str_bag_bin_fn_test <- mean(str_bag_bin_pred_test == "Low" & 
                              testData$stress_bin == "High") # 5.3%

str_rf_bin_pred_test <- predict(str_rf_bin, testData)
str_rf_bin_mce_test <- mean(str_rf_bin_pred_test != testData$stress_bin) # 10.8%
str_rf_bin_fn_test <- mean(str_rf_bin_pred_test == "Low" &
                             testData$stress_bin == "High") # 4.9%
```

There is zero evidence the trees perform better here. OOB estimates were more accurate than I expected.


### Selection

For regression values: `str_nonlin_4`. For classification values: `str_lr_2B` (it's more fair; if I let logistic go to that 88% threshold, it outperforms KNN again).

```{r Stress Selection}
stress_final_reg <- lm(stress_score ~ Q11A + Q27A + poly(Q29A, 2) + poly(Q9A, 3) +
             poly(Q28A, 3) + poly(Q40A, 2) + Q10A + Q13A + poly(Q21A, 2) +
             poly(TIPI4, 3), trainData)
summary(stress_final_reg)# Only two insignificant variables: 2nd order effect of Q9A, and 2nd order effect of Q28A (in both cases, 3rd order is significant)
# RSE = 3.768
# Rsq = adj rsq = 0.8719
# F(19, 29811) = 10680, p < .001
plot(stress_final_reg) # Resid vs Fitted is acceptable
# Q-Q is pretty solid, little fluctuation around the ends (but that always happens)
# Scale-locatoin still has those weird trends, but at least the residuals don't vary based on fitted values
# Nothing really has all that much leverage - only 3 noteworthy data points
save(stress_final_reg, file = "Stress Regression Model.RData")

stress_final_reg_red <- lm(stress_score_reduced ~ Q11A + Q27A + poly(Q29A, 2) +
                             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) +
                             Q10A + Q13A + poly(Q21A, 2) + poly(TIPI4, 3),
                           trainData)
summary(stress_final_reg_red) # Same insignificant vars
# RSE = 3.758
# Rsq = 0.7884, adj rsq = 0.7883
# F(19, 29811) = 5847, p < .001
plot(stress_final_reg_red) # A little more pattern in residuals vs fitted, but not terrible
# Q-Q is about the same, same with scale-location
# No points with concerning leverage
save(stress_final_reg_red, file = "Stress Reduced Regression Model.RData")


stress_final_class <- glm(stress_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A +
                            Q10A + Q13A + Q21A + poly(TIPI4, 2) + Q11A*TIPI4 +
                            Q27A*TIPI4 + Q13A*TIPI4 + Q21A*TIPI4, trainData,
                          family = "binomial")
# Note: threshold = 0.63, NOT 0.5
summary(stress_final_class)
# AIC = 13517
save(stress_final_class, file = "Stress Classification Model.RData")


#stress_final_prob <- predict(str_lr_2, data_test, type = "response")
#preds_str_lr_2B <- rep("Low", length(str_lr_2_prob))
#preds_str_lr_2B[str_lr_2_prob < .63] <- "High"
```

Note: the predictors with orthogonal polynomials cannot really be written in equation form. We will save these models as RData objects, and those will be the final "model" that could be used.


## Anxiety

### Untransformed

```{r CV Anx Untransformed}
anx_resp <- trainSubset$anxiety_score
anx_pred_1 <- model.matrix(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset)[,-1]
anx_pred_2 <- model.matrix(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 5),
                            trainSubset)[,-1]

anx_lin_1_mse <- rep(0, 10)
anx_lin_2_mse <- rep(0, 10)

anx_r1_mse <- rep(0, 10)
anx_r2_mse <- rep(0, 10)
anx_l1_mse <- rep(0, 10)
anx_l2_mse <- rep(0, 10)

anx_nonlin_1_mse <- rep(0, 10)
anx_nonlin_2_mse <- rep(0, 10)
anx_nonlin_3_mse <- rep(0, 10)
anx_nonlin_4_mse <- rep(0, 10)

anx_lr_1A_mce <- rep(0, 10)
anx_lr_1B_mce <- rep(0, 10)
anx_lr_2A_mce <- rep(0, 10)
anx_lr_2B_mce <- rep(0, 10)
anx_lr_3A_mce <- rep(0, 10)
anx_lr_3B_mce <- rep(0, 10)

anx_lr_1A_fn <- rep(0, 10)
anx_lr_1B_fn <- rep(0, 10)
anx_lr_2A_fn <- rep(0, 10)
anx_lr_2B_fn <- rep(0, 10)
anx_lr_3A_fn <- rep(0, 10)
anx_lr_3B_fn <- rep(0, 10)

anx_knn_1_mce <- rep(0, 10)
anx_knn_1_fn <- rep(0, 10)
anx_knn_2_mce <- rep(0, 10)
anx_knn_2_fn <- rep(0, 10)
anx_knn_mse <- rep(0, 10)

for(i in 1:10){
  indices <- folds$splits[[i]][[2]]
  correct_indices <- vector("logical", dim(trainSubset)[1])
  for(j in 1:dim(trainSubset)[1]){
    if(j %in% indices){
      correct_indices[j] = T
    } else{
      correct_indices[j] = F
    }
  }
  
  data_tr <- trainSubset[correct_indices,]
  data_test <- trainSubset[!correct_indices,]
  
  data_tr_X <- data_tr %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  data_te_X <- data_test %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  
  
  # Linear
  anx_lin_1 <- lm(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, data_tr)
  anx_lin_1_mse[i] <- mean((predict(anx_lin_1, data_test) -
                              data_test$anxiety_score)^2)
  anx_lin_2 <- lm(anxiety_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 5), data_tr)
  anx_lin_2_mse[i] <- mean((predict(anx_lin_2, data_test) - 
                             data_test$anxiety_score)^2)
  
  # Shrinkage
  set.seed(1)
  anx_r1 <- glmnet(anx_pred_1[correct_indices,], anx_resp[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  anx_r1_cv <- cv.glmnet(anx_pred_1[correct_indices,],
                              anx_resp[correct_indices], alpha = 0)
  bestlam_anx_r1 <- anx_r1_cv$lambda.min
  pred_anx_r1 <- predict(anx_r1, s = bestlam_anx_r1,
                         newx = anx_pred_1[!correct_indices,])
  anx_r1_mse[i] <- mean((pred_anx_r1 - data_test$anxiety_score)^2)
  
  set.seed(1)
  anx_r2 <- glmnet(anx_pred_2[correct_indices,], anx_resp[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  anx_r2_cv <- cv.glmnet(anx_pred_2[correct_indices,],
                              anx_resp[correct_indices], alpha = 0)
  bestlam_anx_r2 <- anx_r2_cv$lambda.min
  pred_anx_r2 <- predict(anx_r2, s = bestlam_anx_r2,
                         newx = anx_pred_2[!correct_indices,])
  anx_r2_mse[i] <- mean((pred_anx_r2 - data_test$anxiety_score)^2)
  
  
  set.seed(1)
  anx_l1 <- glmnet(anx_pred_1[correct_indices,], anx_resp[correct_indices],
                   alpha = 1, lambda = grid)
  anx_l1_cv <- cv.glmnet(anx_pred_1[correct_indices,],
                              anx_resp[correct_indices], alpha = 1)
  bestlam_anx_l1 <- anx_l1_cv$lambda.min
  pred_anx_l1 <- predict(anx_l1, s = bestlam_anx_l1,
                         newx = anx_pred_1[!correct_indices,])
  anx_l1_mse[i] <- mean((pred_anx_l1 - data_test$anxiety_score)^2)
  
  set.seed(1)
  anx_l2 <- glmnet(anx_pred_2[correct_indices,], anx_resp[correct_indices],
                   alpha = 1, lambda = grid)
  anx_l2_cv <- cv.glmnet(anx_pred_2[correct_indices,],
                              anx_resp[correct_indices], alpha = 1)
  bestlam_anx_l2 <- anx_l2_cv$lambda.min
  pred_anx_l2 <- predict(anx_l2, s = bestlam_anx_l2,
                         newx = anx_pred_2[!correct_indices,])
  anx_l2_mse[i] <- mean((pred_anx_l2 - data_test$anxiety_score)^2)
  
  
  # Nonlinear
  anx_nonlin_1 <-lm(anxiety_score ~ poly(Q11A, 2) + poly(Q27A, 2) +
                               poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), data_tr)
  anx_nonlin_1_mse[i] <- mean((predict(anx_nonlin_1, data_test) -
                                 data_test$anxiety_score)^2)
  
  anx_nonlin_2 <- lm(anxiety_score ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) +
                       poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             bs(TIPI4, df = 5), data_tr)
  anx_nonlin_2_mse[i] <- mean((predict(anx_nonlin_2, data_test) - 
                                 data_test$anxiety_score)^2)
  
  anx_nonlin_3 <- lm(anxiety_score ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) +
                       poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             ns(TIPI4, df = 5), data_tr)
  anx_nonlin_3_mse[i] <- mean((predict(anx_nonlin_3, data_test) - 
                                 data_test$anxiety_score)^2)
  
  anx_nonlin_4 <- lm(anxiety_score ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) + poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             poly(TIPI4, 3), data_tr)
  anx_nonlin_4_mse[i] <- mean((predict(anx_nonlin_4, data_test) - 
                                 data_test$anxiety_score)^2)
  
  # Logistic
  anx_lr_1 <- glm(anxiety_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, data_tr, family = "binomial")
  anx_lr_1_prob <- predict(anx_lr_1, data_test, type = "response")
  preds_anx_lr_1A <- rep("Low", length(anx_lr_1_prob))
  preds_anx_lr_1A[anx_lr_1_prob < .5] <- "High"
  anx_lr_1A_mce[i] <- mean(preds_anx_lr_1A != data_test$anxiety_bin)
  anx_lr_1A_fn[i] <- mean(preds_anx_lr_1A == "Low" &
                            data_test$anxiety_bin == "High")
  preds_anx_lr_1B <- rep("Low", length(anx_lr_1_prob))
  preds_anx_lr_1B[anx_lr_1_prob < .68] <- "High"
  anx_lr_1B_mce[i] <- mean(preds_anx_lr_1B != data_test$anxiety_bin)
  anx_lr_1B_fn[i] <- mean(preds_anx_lr_1B == "Low" &
                            data_test$anxiety_bin == "High")
  
  anx_lr_2 <- glm(anxiety_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2) + Q11A*TIPI4 + Q29A*TIPI4 + 
               Q28A*TIPI4 + Q10A*TIPI4 + Q21A*TIPI4, 
             data_tr, family = "binomial")
  anx_lr_2_prob <- predict(anx_lr_2, data_test, type = "response")
  preds_anx_lr_2A <- rep("Low", length(anx_lr_2_prob))
  preds_anx_lr_2A[anx_lr_2_prob < .5] <- "High"
  anx_lr_2A_mce[i] <- mean(preds_anx_lr_2A != data_test$anxiety_bin)
  anx_lr_2A_fn[i] <- mean(preds_anx_lr_2A == "Low" &
                            data_test$anxiety_bin == "High")
  preds_anx_lr_2B <- rep("Low", length(anx_lr_2_prob))
  preds_anx_lr_2B[anx_lr_2_prob < .68] <- "High"
  anx_lr_2B_mce[i] <- mean(preds_anx_lr_2B != data_test$anxiety_bin)
  anx_lr_2B_fn[i] <- mean(preds_anx_lr_2B == "Low" &
                            data_test$anxiety_bin == "High")
  
  anx_lr_3 <- glm(anxiety_bin ~ poly(Q11A, 2) + poly(Q27A, 2) + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), data_tr,
             family = "binomial")
  anx_lr_3_prob <- predict(anx_lr_3, data_test, type = "response")
  preds_anx_lr_3A <- rep("Low", length(anx_lr_3_prob))
  preds_anx_lr_3A[anx_lr_3_prob < .5] <- "High"
  anx_lr_3A_mce[i] <- mean(preds_anx_lr_3A != data_test$anxiety_bin)
  anx_lr_3A_fn[i] <- mean(preds_anx_lr_3A == "Low" &
                            data_test$anxiety_bin == "High")
  preds_anx_lr_3B <- rep("Low", length(anx_lr_3_prob))
  preds_anx_lr_3B[anx_lr_3_prob < .68] <- "High"
  anx_lr_3B_mce[i] <- mean(preds_anx_lr_3B != data_test$anxiety_bin)
  anx_lr_3B_fn[i] <- mean(preds_anx_lr_3B == "Low" &
                            data_test$anxiety_bin == "High")
  
  # KNN
  set.seed(1)
  anx_knn_class <- class::knn(data_tr_X, data_te_X,
                       data_tr$anxiety_bin, k = 12, prob = T)
  anx_knn_1_mce[i] <- 1 - (sum(diag(table(anx_knn_class, data_test$anxiety_bin))) /
                           sum(table(anx_knn_class, data_test$anxiety_bin)))
  anx_knn_1_fn[i] <- mean(anx_knn_class == "Low" & data_test$anxiety_bin == "High")
  anx_knn_preds <- rep("Low", length(anx_knn_class))
  anx_knn_preds[anx_knn_class == "High"] <- "High"
  anx_knn_preds_2 <- if_else(attr(anx_knn_class, "prob") <= .64,
                             "High", anx_knn_preds)
  anx_knn_2_mce[i] <- 1 - (sum(diag(table(anx_knn_preds_2, data_test$anxiety_bin))) /
                             sum(table(anx_knn_preds_2, data_test$anxiety_bin)))
  anx_knn_2_fn[i] <- mean(anx_knn_preds_2 == "Low" & data_test$anxiety_bin == "High")
  
  set.seed(1)
  anx_knn_reg <- knn.reg(data_tr_X, test = data_te_X,
                         y = data_tr$anxiety_score, k = 45)
  anx_knn_mse[i] <- mean((anx_knn_reg$pred - data_test$anxiety_score)^2)
}
```

### Reduced

```{r CV Anx Reduced}
anx_resp_red <- trainSubset$anxiety_score_reduced
anx_pred_1_red <- model.matrix(anxiety_score_reduced ~ Q11A + Q27A + Q29A +
                                 Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset)[,-1]
anx_pred_2_red <- model.matrix(anxiety_score_reduced ~ Q11A + Q27A + Q29A +
                                 Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 5),
                            trainSubset)[,-1]

anx_lin_1_mse_red <- rep(0, 10)
anx_lin_2_mse_red <- rep(0, 10)

anx_r1_mse_red <- rep(0, 10)
anx_r2_mse_red <- rep(0, 10)
anx_l1_mse_red <- rep(0, 10)
anx_l2_mse_red <- rep(0, 10)

anx_nonlin_1_mse_red <- rep(0, 10)
anx_nonlin_2_mse_red <- rep(0, 10)
anx_nonlin_3_mse_red <- rep(0, 10)
anx_nonlin_4_mse_red <- rep(0, 10)

anx_knn_mse_red <- rep(0, 10)

for(i in 1:10){
  indices <- folds$splits[[i]][[2]]
  correct_indices <- vector("logical", dim(trainSubset)[1])
  for(j in 1:dim(trainSubset)[1]){
    if(j %in% indices){
      correct_indices[j] = T
    } else{
      correct_indices[j] = F
    }
  }
  
  data_tr <- trainSubset[correct_indices,]
  data_test <- trainSubset[!correct_indices,]
  
  data_tr_X <- data_tr %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  data_te_X <- data_test %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  
  
  # Linear
  anx_lin_1_red <- lm(anxiety_score_reduced ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, data_tr)
  anx_lin_1_mse_red[i] <- mean((predict(anx_lin_1_red, data_test) -
                              data_test$anxiety_score_reduced)^2)
  anx_lin_2_red <- lm(anxiety_score_reduced ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 5), data_tr)
  anx_lin_2_mse_red[i] <- mean((predict(anx_lin_2_red, data_test) - 
                             data_test$anxiety_score_reduced)^2)
  
  # Shrinkage
  set.seed(1)
  anx_r1_red <- glmnet(anx_pred_1_red[correct_indices,], anx_resp_red[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  anx_r1_cv_red <- cv.glmnet(anx_pred_1_red[correct_indices,],
                              anx_resp_red[correct_indices], alpha = 0)
  bestlam_anx_r1_red <- anx_r1_cv_red$lambda.min
  pred_anx_r1_red <- predict(anx_r1_red, s = bestlam_anx_r1_red,
                         newx = anx_pred_1_red[!correct_indices,])
  anx_r1_mse_red[i] <- mean((pred_anx_r1_red - data_test$anxiety_score_reduced)^2)
  
  set.seed(1)
  anx_r2_red <- glmnet(anx_pred_2_red[correct_indices,], anx_resp_red[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  anx_r2_cv_red <- cv.glmnet(anx_pred_2_red[correct_indices,],
                              anx_resp_red[correct_indices], alpha = 0)
  bestlam_anx_r2_red <- anx_r2_cv_red$lambda.min
  pred_anx_r2_red <- predict(anx_r2_red, s = bestlam_anx_r2_red,
                         newx = anx_pred_2_red[!correct_indices,])
  anx_r2_mse_red[i] <- mean((pred_anx_r2_red - data_test$anxiety_score_reduced)^2)
  
  
  set.seed(1)
  anx_l1_red <- glmnet(anx_pred_1_red[correct_indices,], anx_resp_red[correct_indices],
                   alpha = 1, lambda = grid)
  anx_l1_cv_red <- cv.glmnet(anx_pred_1_red[correct_indices,],
                              anx_resp_red[correct_indices], alpha = 1)
  bestlam_anx_l1_red <- anx_l1_cv_red$lambda.min
  pred_anx_l1_red <- predict(anx_l1_red, s = bestlam_anx_l1_red,
                         newx = anx_pred_1_red[!correct_indices,])
  anx_l1_mse_red[i] <- mean((pred_anx_l1_red - data_test$anxiety_score_reduced)^2)
  
  set.seed(1)
  anx_l2_red <- glmnet(anx_pred_2_red[correct_indices,], anx_resp_red[correct_indices],
                   alpha = 1, lambda = grid)
  anx_l2_cv_red <- cv.glmnet(anx_pred_2_red[correct_indices,],
                              anx_resp_red[correct_indices], alpha = 1)
  bestlam_anx_l2_red <- anx_l2_cv_red$lambda.min
  pred_anx_l2_red <- predict(anx_l2_red, s = bestlam_anx_l2_red,
                         newx = anx_pred_2_red[!correct_indices,])
  anx_l2_mse_red[i] <- mean((pred_anx_l2_red - data_test$anxiety_score_reduced)^2)
  
  
  # Nonlinear
  anx_nonlin_1_red <- lm(anxiety_score_reduced ~ poly(Q11A, 2) + poly(Q27A, 2) +
                               poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), data_tr)
  anx_nonlin_1_mse_red[i] <- mean((predict(anx_nonlin_1_red, data_test) -
                                 data_test$anxiety_score_reduced)^2)
  
  anx_nonlin_2_red <- lm(anxiety_score_reduced ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) +
                       poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             bs(TIPI4, df = 5), data_tr)
  anx_nonlin_2_mse_red[i] <- mean((predict(anx_nonlin_2_red, data_test) - 
                                 data_test$anxiety_score_reduced)^2)
  
  anx_nonlin_3_red <- lm(anxiety_score_reduced ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) +
                       poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             ns(TIPI4, df = 5), data_tr)
  anx_nonlin_3_mse_red[i] <- mean((predict(anx_nonlin_3_red, data_test) - 
                                 data_test$anxiety_score_reduced)^2)
  
  anx_nonlin_4_red <- lm(anxiety_score_reduced ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) +
                           poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             poly(TIPI4, 3), data_tr)
  anx_nonlin_4_mse_red[i] <- mean((predict(anx_nonlin_4_red, data_test) - 
                                 data_test$anxiety_score_reduced)^2)
  
  # KNN
  set.seed(1)
  anx_knn_reg_red <- knn.reg(data_tr_X, test = data_te_X,
                         y = data_tr$anxiety_score_reduced, k = 45)
  anx_knn_mse_red[i] <- mean((anx_knn_reg_red$pred - data_test$anxiety_score_reduced)^2)
}
```

### Transformed


```{r CV Amx Trans}
anx_resp_t <- trainSubset$anxiety_trans
anx_pred_1_t <- model.matrix(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset)[,-1]
anx_pred_2_t <- model.matrix(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 5),
                            trainSubset)[,-1]

anx_lin_1_mse_t <- rep(0, 10)
anx_lin_2_mse_t <- rep(0, 10)

anx_r1_mse_t <- rep(0, 10)
anx_r2_mse_t <- rep(0, 10)
anx_l1_mse_t <- rep(0, 10)
anx_l2_mse_t <- rep(0, 10)

anx_nonlin_1_mse_t <- rep(0, 10)
anx_nonlin_2_mse_t <- rep(0, 10)
anx_nonlin_3_mse_t <- rep(0, 10)
anx_nonlin_4_mse_t <- rep(0, 10)

anx_knn_mse_t <- rep(0, 10)

for(i in 1:10){
  indices <- folds$splits[[i]][[2]]
  correct_indices <- vector("logical", dim(trainSubset)[1])
  for(j in 1:dim(trainSubset)[1]){
    if(j %in% indices){
      correct_indices[j] = T
    } else{
      correct_indices[j] = F
    }
  }
  
  data_tr <- trainSubset[correct_indices,]
  data_test <- trainSubset[!correct_indices,]
  
  data_tr_X <- data_tr %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  data_te_X <- data_test %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  
  
  # Linear
  anx_lin_1_t <- lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, data_tr)
  anx_lin_1_mse_t[i] <- mean((predict(anx_lin_1_t, data_test) -
                              data_test$anxiety_trans)^2)
  anx_lin_2_t <- lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 5), data_tr)
  anx_lin_2_mse_t[i] <- mean((predict(anx_lin_2_t, data_test) - 
                             data_test$anxiety_trans)^2)
  
  # Shrinkage
  set.seed(1)
  anx_r1_t <- glmnet(anx_pred_1_t[correct_indices,], anx_resp_t[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  anx_r1_cv_t <- cv.glmnet(anx_pred_1_t[correct_indices,],
                              anx_resp_t[correct_indices], alpha = 0)
  bestlam_anx_r1_t <- anx_r1_cv_t$lambda.min
  pred_anx_r1_t <- predict(anx_r1_t, s = bestlam_anx_r1_t,
                         newx = anx_pred_1_t[!correct_indices,])
  anx_r1_mse_t[i] <- mean((pred_anx_r1_t - data_test$anxiety_trans)^2)
  
  set.seed(1)
  anx_r2_t <- glmnet(anx_pred_2_t[correct_indices,], anx_resp_t[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  anx_r2_cv_t <- cv.glmnet(anx_pred_2_t[correct_indices,],
                              anx_resp_t[correct_indices], alpha = 0)
  bestlam_anx_r2_t <- anx_r2_cv_t$lambda.min
  pred_anx_r2_t <- predict(anx_r2_t, s = bestlam_anx_r2_t,
                         newx = anx_pred_2_t[!correct_indices,])
  anx_r2_mse_t[i] <- mean((pred_anx_r2_t - data_test$anxiety_trans)^2)
  
  
  set.seed(1)
  anx_l1_t <- glmnet(anx_pred_1_t[correct_indices,], anx_resp_t[correct_indices],
                   alpha = 1, lambda = grid)
  anx_l1_cv_t <- cv.glmnet(anx_pred_1_t[correct_indices,],
                              anx_resp_t[correct_indices], alpha = 1)
  bestlam_anx_l1_t <- anx_l1_cv_t$lambda.min
  pred_anx_l1_t <- predict(anx_l1_t, s = bestlam_anx_l1_t,
                         newx = anx_pred_1_t[!correct_indices,])
  anx_l1_mse_t[i] <- mean((pred_anx_l1_t - data_test$anxiety_trans)^2)
  
  set.seed(1)
  anx_l2_t <- glmnet(anx_pred_2_t[correct_indices,], anx_resp_t[correct_indices],
                   alpha = 1, lambda = grid)
  anx_l2_cv_t <- cv.glmnet(anx_pred_2_t[correct_indices,],
                              anx_resp_t[correct_indices], alpha = 1)
  bestlam_anx_l2_t <- anx_l2_cv_t$lambda.min
  pred_anx_l2_t <- predict(anx_l2_t, s = bestlam_anx_l2,
                         newx = anx_pred_2_t[!correct_indices,])
  anx_l2_mse_t[i] <- mean((pred_anx_l2_t - data_test$anxiety_trans)^2)
  
  
  # Nonlinear
  anx_nonlin_1_t <- lm(anxiety_trans ~ poly(Q11A, 2) + poly(Q27A, 2) +
                               poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), data_tr)
  anx_nonlin_1_mse_t[i] <- mean((predict(anx_nonlin_1_t, data_test) -
                                 data_test$anxiety_trans)^2)
  
  anx_nonlin_2_t <- lm(anxiety_trans ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) +
                       poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             bs(TIPI4, df = 5), data_tr)
  anx_nonlin_2_mse_t[i] <- mean((predict(anx_nonlin_2_t, data_test) - 
                                 data_test$anxiety_trans)^2)
  
  anx_nonlin_3_t <- lm(anxiety_trans ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) +
                       poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             ns(TIPI4, df = 5), data_tr)
  anx_nonlin_3_mse_t[i] <- mean((predict(anx_nonlin_3_t, data_test) - 
                                 data_test$anxiety_trans)^2)
  
  anx_nonlin_4_t <- lm(anxiety_trans ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) + poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             poly(TIPI4, 3), data_tr)
  anx_nonlin_4_mse_t[i] <- mean((predict(anx_nonlin_4_t, data_test) - 
                                 data_test$anxiety_trans)^2)
  
  # KNN
  set.seed(1)
  anx_knn_reg_t <- knn.reg(data_tr_X, test = data_te_X,
                         y = data_tr$anxiety_trans, k = 45)
  anx_knn_mse_t[i] <- mean((anx_knn_reg_t$pred - data_test$anxiety_trans)^2)
}
```

### Output

It's impossible to properly recover MSE of transformed variable to compare - we will leave it out for stress & depression (showed minimal evidence of a useful transformation), but will still have to consider through other methods for anxiety (showed real evidence).

```{r CV Anx Output}
get_se <- function(mse){
  ret = 0
  for(i in 1:10){
    ret = ret + (mse[i] - mean(mse))^2
  }
  return(sqrt(ret / 9))
}

anx_mse <- data.frame(t(c(Model = "anx_lin_1", MSE = mean(anx_lin_1_mse),
                          SD = get_se(anx_lin_1_mse),
                          MSE_red = mean(anx_lin_1_mse_red),
                          SD_red = get_se(anx_lin_1_mse_red))))
anx_mse <- rbind(anx_mse,
                 c("anx_lin_2", mean(anx_lin_2_mse), get_se(anx_lin_2_mse),
                   mean(anx_lin_2_mse_red), get_se(anx_lin_2_mse_red)),
                 c("anx_r1", mean(anx_r1_mse), get_se(anx_r1_mse),
                   mean(anx_r1_mse_red), get_se(anx_r1_mse_red)),
                 c("anx_r2", mean(anx_r2_mse), get_se(anx_r2_mse),
                   mean(anx_r2_mse_red), get_se(anx_r2_mse_red)),
                 c("anx_l1", mean(anx_l1_mse), get_se(anx_l1_mse),
                   mean(anx_l1_mse_red), get_se(anx_l1_mse_red)),
                 c("anx_l2", mean(anx_l2_mse), get_se(anx_l2_mse),
                   mean(anx_l2_mse_red), get_se(anx_l2_mse_red)),
                 c("anx_nonlin_1", mean(anx_nonlin_1_mse),
                   get_se(anx_nonlin_1_mse), mean(anx_nonlin_1_mse_red),
                   get_se(anx_nonlin_1_mse_red)),
                 c("anx_nonlin_2", mean(anx_nonlin_2_mse),
                   get_se(anx_nonlin_2_mse), mean(anx_nonlin_2_mse_red),
                   get_se(anx_nonlin_2_mse_red)),
                 c("anx_nonlin_3", mean(anx_nonlin_3_mse),
                   get_se(anx_nonlin_3_mse), mean(anx_nonlin_3_mse_red),
                   get_se(anx_nonlin_3_mse_red)),
                 c("anx_nonlin_4", mean(anx_nonlin_4_mse),
                   get_se(anx_nonlin_4_mse), mean(anx_nonlin_4_mse_red),
                   get_se(anx_nonlin_4_mse_red)),
                 c("anx_knn_reg", mean(anx_knn_mse), get_se(anx_knn_mse),
                   mean(anx_knn_mse_red), get_se(anx_knn_mse_red)))

anx_mse %>% 
  arrange(MSE, MSE_red) %>% 
  knitr::kable(digits = 3)

anx_mse %>% 
  mutate(sd_1 = as.numeric(MSE) + as.numeric(SD)) %>% 
  arrange(sd_1) # No changes


# Also cannot easily be recovered, may have to select on other criteria

anx_mse_t <- data.frame(t(c(Model = "anx_lin_1_t", MSE = mean(anx_lin_1_mse_t),
                          SD = get_se(anx_lin_1_mse_t))))
anx_mse_t <- rbind(anx_mse_t,
                 c("anx_lin_2_t", mean(anx_lin_2_mse_t), get_se(anx_lin_2_mse_t)),
                 c("anx_r1_t", mean(anx_r1_mse_t), get_se(anx_r1_mse_t)),
                 c("anx_r2_t", mean(anx_r2_mse_t), get_se(anx_r2_mse_t)),
                 c("anx_l1_t", mean(anx_l1_mse_t), get_se(anx_l1_mse_t)),
                 c("anx_l2_t", mean(anx_l2_mse_t), get_se(anx_l2_mse_t)),
                 c("anx_nonlin_1_t", mean(anx_nonlin_1_mse_t),
                   get_se(anx_nonlin_1_mse_t)),
                 c("anx_nonlin_2_t", mean(anx_nonlin_2_mse_t),
                   get_se(anx_nonlin_2_mse_t)),
                 c("anx_nonlin_3_t", mean(anx_nonlin_3_mse_t),
                   get_se(anx_nonlin_3_mse_t)),
                 c("anx_nonlin_4_t", mean(anx_nonlin_4_mse_t),
                   get_se(anx_nonlin_4_mse_t)),
                 c("anx_knn_reg_t", mean(anx_knn_mse_t), get_se(anx_knn_mse_t)))

anx_mse_t %>% 
  arrange(MSE) %>% 
  knitr::kable(digits = 3) # nonlin_3 is 2nd best model here

anx_mse_t %>% 
  mutate(sd_1 = as.numeric(MSE) + as.numeric(SD)) %>% 
  arrange(sd_1)



anx_mce <- data.frame(t(c(Model = "anx_lr_1A", MCE = mean(anx_lr_1A_mce),
                          SD = get_se(anx_lr_1A_mce), FN = mean(anx_lr_1A_fn),
                          FN_SD = get_se(anx_lr_1A_fn))))
anx_mce <- rbind(anx_mce,
                 c("anx_lr_1B", mean(anx_lr_1B_mce), get_se(anx_lr_1B_mce),
                   mean(anx_lr_1B_fn), get_se(anx_lr_1B_fn)),
                 c("anx_lr_2A", mean(anx_lr_2A_mce), get_se(anx_lr_2A_mce),
                   mean(anx_lr_2A_fn), get_se(anx_lr_2A_fn)),
                 c("anx_lr_2B", mean(anx_lr_2B_mce), get_se(anx_lr_2B_mce),
                   mean(anx_lr_2B_fn), get_se(anx_lr_2B_fn)),
                 c("anx_lr_3A", mean(anx_lr_3A_mce), get_se(anx_lr_3A_mce),
                   mean(anx_lr_3A_fn), get_se(anx_lr_3A_fn)),
                 c("anx_lr_3B", mean(anx_lr_3B_mce), get_se(anx_lr_3B_mce),
                   mean(anx_lr_3B_fn), get_se(anx_lr_3B_fn)),
                 c("anx_knn_1", mean(anx_knn_1_mce), get_se(anx_knn_1_mce),
                   mean(anx_knn_1_fn), get_se(anx_knn_1_fn)),
                 c("anx_knn_2", mean(anx_knn_2_mce), get_se(anx_knn_2_mce),
                   mean(anx_knn_2_fn), get_se(anx_knn_2_fn)))

anx_mce %>% 
  arrange(FN, MCE) %>% 
  knitr::kable(digits = 3) # KNN doesn't keep up here
```

### Comparison on New Data

The best way I can think of to compare the tree and non-tree error rates (even though I wanted to avoid this) is to test their predictions on new data. I will only compare the best 1-2 non-tree models to the tree models.

```{r New Data Anx}
anx_nonlin_3_test <- lm(anxiety_score ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) +
                       poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             ns(TIPI4, df = 5), trainData)
anx_nonlin_3_mse_test <- mean((predict(anx_nonlin_3_test, testData) - 
                                 testData$anxiety_score)^2) # 21.799

anx_t_nonlin_3_test <- lm(anxiety_trans ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) +
                       poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             ns(TIPI4, df = 5), trainData)
anx_t_nonlin_3_mse_test <- mean((predict(anx_t_nonlin_3_test, testData) - 
                                   testData$anxiety_trans)^2)

anx_nonlin_3_red_test <- lm(anxiety_score_reduced ~ poly(Q11A, 2) + Q27A +
                              poly(Q29A, 2) + poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             ns(TIPI4, df = 5), trainData)
anx_nonlin_3_mse_red_test <- mean((predict(anx_nonlin_3_red_test, testData) - 
                                 testData$anxiety_score_reduced)^2) # 21.799

anx_bag_test_mse <- mean((predict(anx_bag, testData) - testData$anxiety_score)^2) # 24.076

anx_bag_red_test_mse <- mean((predict(anx_bag_red, testData) -
                                testData$anxiety_score_reduced)^2) # 23.787

anx_rf_test_mse <- mean((predict(anx_rf, testData) - testData$anxiety_score)^2) # 22.592

anx_rf_red_test_mse <- mean((predict(anx_rf_red, testData) -
                               testData$anxiety_score_reduced)^2) # 22.436

# Literally zero evidence that these are outperforming


trainData_X <- trainSubset %>% 
  select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)

testData_X <- testData %>% 
  select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  
anx_lr_1_test <- glm(anxiety_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, trainData, family = "binomial")
anx_lr_1_prob_test <- predict(anx_lr_1_test, testData, type = "response")
preds_anx_lr_1B_test <- rep("Low", length(anx_lr_1_prob_test))
preds_anx_lr_1B_test[anx_lr_1_prob_test < .68] <- "High"
anx_lr_1B_mce_test <- mean(preds_anx_lr_1B_test != testData$anxiety_bin) # 11.987%
anx_lr_1B_fn_test <- mean(preds_anx_lr_1B_test == "Low" &
                            testData$anxiety_bin == "High") # 2.645%
  
anx_lr_2_test <- glm(anxiety_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2) + Q11A*TIPI4 + Q29A*TIPI4 + 
               Q28A*TIPI4 + Q10A*TIPI4 + Q21A*TIPI4, 
             trainData, family = "binomial")
anx_lr_2_prob_test <- predict(anx_lr_2_test, testData, type = "response")
preds_anx_lr_2B_test <- rep("Low", length(anx_lr_2_prob_test))
preds_anx_lr_2B_test[anx_lr_2_prob_test < .68] <- "High"
anx_lr_2B_mce_test <- mean(preds_anx_lr_2B_test != testData$anxiety_bin) # 11.977%
anx_lr_2B_fn_test <- mean(preds_anx_lr_2B_test == "Low" &
                            testData$anxiety_bin == "High") # 2.655%

anx_bag_bin_pred_test <- predict(anx_bag_bin, testData)
anx_bag_bin_mce_test <- mean(anx_bag_bin_pred_test != testData$anxiety_bin) # 13.113%
anx_bag_bin_fn_test <- mean(anx_bag_bin_pred_test == "Low" & 
                              testData$anxiety_bin == "High") # 6.144

anx_rf_bin_pred_test <- predict(anx_rf_bin, testData)
anx_rf_bin_mce_test <- mean(anx_rf_bin_pred_test != testData$anxiety_bin) # 11.866%
anx_rf_bin_fn_test <- mean(anx_rf_bin_pred_test == "Low" &
                             testData$anxiety_bin == "High") # 5.541%
```

There is zero evidence the trees perform better here. OOB estimates were more accurate than I expected. Additionally, adding more trees does not a better model make.


### Selection

We are actually going to use the transformed model here, because A) it does seem to perform a little better (mostly in terms of $R^2$) and B) it is a simple transformation that is easy to turn back to a prediction on the same scale as the others (by squaring the prediction). Thus, we select for regression, `anx_nonlin_1_t`, and for classification, `anx_lr_2B`.

```{r}
anx_final_reg <- lm(anxiety_trans ~ poly(Q11A, 2) + poly(Q27A, 2) + 
                      poly(Q29A, 2) + poly(Q9A, 3) + poly(Q28A, 3) +
                      poly(Q40A, 2) + poly(Q10A, 3) + poly(Q13A, 3) +
                      poly(Q21A, 3) + bs(TIPI4, df = 5), trainData)
summary(anx_final_reg) # Many higher-order terms are actually not significant
# Residual SE = 0.6333
# Rsq = 0.8037, adj rsq = 0.8035
# F(28, 29802) = 4356, p < .001
plot(anx_final_reg) # Resid vs fitted looks weird but there's not actually much trend
# Q-Q is actually very solid
# Scale-location is a little wonky, but if it didn't have these trends we wouldn't notice any problem
# Some residuals have slightly high leverage pulling the model towards higher values
save(anx_final_reg, file = "Anxiety Regression Model.RData")

anx_final_reg_red <- lm(anxiety_trans_reduced ~ poly(Q11A, 2) + poly(Q27A, 2) + 
                      poly(Q29A, 2) + poly(Q9A, 3) + poly(Q28A, 3) +
                      poly(Q40A, 2) + poly(Q10A, 3) + poly(Q13A, 3) +
                      poly(Q21A, 3) + bs(TIPI4, df = 5), trainData)
summary(anx_final_reg_red) # A little sparse but could be worse
# Residual SE = 0.7623
# Rsq = 0.6654, adj rsq = 0.6651
# F(28, 29802) = 2117, p < .001
plot(anx_final_reg_red) # Resid vs Fitted is a little concerning
# Q-Q looks fine
# Scale location a tiny bit concerning
# Nothing with too much leverage
save(anx_final_reg_red, file = "Anxiety Reduced Regression Model.RData")


anx_final_class <- glm(anxiety_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A +
                         Q10A + Q13A + Q21A + poly(TIPI4, 2) + Q11A*TIPI4 +
                         Q29A*TIPI4 + Q28A*TIPI4 + Q10A*TIPI4 + Q21A*TIPI4, 
                       trainData, family = "binomial")
# Threshold = .68
summary(anx_final_class) # Most interactions non-significant, as is Q21A and 2nd order TIPI4 term; AIC = 14972
save(anx_final_class, file = "Anxiety Classification Model.RData")

anx_lr_2_prob <- predict(anx_lr_2, data_test, type = "response")
preds_anx_lr_2B <- rep("Low", length(anx_lr_2_prob))
preds_anx_lr_2B[anx_lr_2_prob < .68] <- "High"
```


## Depression

### Untransformed

```{r CV Dep Untransformed}
dep_resp <- trainSubset$depression_score
dep_pred_1 <- model.matrix(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset)[,-1]
dep_pred_2 <- model.matrix(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 5),
                            trainSubset)[,-1]

dep_lin_1_mse <- rep(0, 10)
dep_lin_2_mse <- rep(0, 10)

dep_r1_mse <- rep(0, 10)
dep_r2_mse <- rep(0, 10)
dep_l1_mse <- rep(0, 10)
dep_l2_mse <- rep(0, 10)

dep_nonlin_1_mse <- rep(0, 10)
dep_nonlin_2_mse <- rep(0, 10)
dep_nonlin_3_mse <- rep(0, 10)
dep_nonlin_4_mse <- rep(0, 10)

dep_lr_1A_mce <- rep(0, 10)
dep_lr_1B_mce <- rep(0, 10)
dep_lr_2A_mce <- rep(0, 10)
dep_lr_2B_mce <- rep(0, 10)
dep_lr_3A_mce <- rep(0, 10)
dep_lr_3B_mce <- rep(0, 10)

dep_lr_1A_fn <- rep(0, 10)
dep_lr_1B_fn <- rep(0, 10)
dep_lr_2A_fn <- rep(0, 10)
dep_lr_2B_fn <- rep(0, 10)
dep_lr_3A_fn <- rep(0, 10)
dep_lr_3B_fn <- rep(0, 10)

dep_knn_1_mce <- rep(0, 10)
dep_knn_1_fn <- rep(0, 10)
dep_knn_2_mce <- rep(0, 10)
dep_knn_2_fn <- rep(0, 10)
dep_knn_mse <- rep(0, 10)

for(i in 1:10){
  indices <- folds$splits[[i]][[2]]
  correct_indices <- vector("logical", dim(trainSubset)[1])
  for(j in 1:dim(trainSubset)[1]){
    if(j %in% indices){
      correct_indices[j] = T
    } else{
      correct_indices[j] = F
    }
  }
  
  data_tr <- trainSubset[correct_indices,]
  data_test <- trainSubset[!correct_indices,]
  
  data_tr_X <- data_tr %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  data_te_X <- data_test %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  
  
  # Linear
  dep_lin_1 <- lm(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, data_tr)
  dep_lin_1_mse[i] <- mean((predict(dep_lin_1, data_test) -
                              data_test$depression_score)^2)
  dep_lin_2 <- lm(depression_score ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 5), data_tr)
  dep_lin_2_mse[i] <- mean((predict(dep_lin_2, data_test) - 
                             data_test$depression_score)^2)
  
  # Shrinkage
  set.seed(1)
  dep_r1 <- glmnet(dep_pred_1[correct_indices,], dep_resp[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  dep_r1_cv <- cv.glmnet(dep_pred_1[correct_indices,],
                              dep_resp[correct_indices], alpha = 0)
  bestlam_dep_r1 <- dep_r1_cv$lambda.min
  pred_dep_r1 <- predict(dep_r1, s = bestlam_dep_r1,
                         newx = dep_pred_1[!correct_indices,])
  dep_r1_mse[i] <- mean((pred_dep_r1 - data_test$depression_score)^2)
  
  set.seed(1)
  dep_r2 <- glmnet(dep_pred_2[correct_indices,], dep_resp[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  dep_r2_cv <- cv.glmnet(dep_pred_2[correct_indices,],
                              dep_resp[correct_indices], alpha = 0)
  bestlam_dep_r2 <- dep_r2_cv$lambda.min
  pred_dep_r2 <- predict(dep_r2, s = bestlam_dep_r2,
                         newx = dep_pred_2[!correct_indices,])
  dep_r2_mse[i] <- mean((pred_dep_r2 - data_test$depression_score)^2)
  
  
  set.seed(1)
  dep_l1 <- glmnet(dep_pred_1[correct_indices,], dep_resp[correct_indices],
                   alpha = 1, lambda = grid)
  dep_l1_cv <- cv.glmnet(dep_pred_1[correct_indices,],
                              dep_resp[correct_indices], alpha = 1)
  bestlam_dep_l1 <- dep_l1_cv$lambda.min
  pred_dep_l1 <- predict(dep_l1, s = bestlam_dep_l1,
                         newx = dep_pred_1[!correct_indices,])
  dep_l1_mse[i] <- mean((pred_dep_l1 - data_test$depression_score)^2)
  
  set.seed(1)
  dep_l2 <- glmnet(dep_pred_2[correct_indices,], dep_resp[correct_indices],
                   alpha = 1, lambda = grid)
  dep_l2_cv <- cv.glmnet(dep_pred_2[correct_indices,],
                              dep_resp[correct_indices], alpha = 1)
  bestlam_dep_l2 <- dep_l2_cv$lambda.min
  pred_dep_l2 <- predict(dep_l2, s = bestlam_dep_l2,
                         newx = dep_pred_2[!correct_indices,])
  dep_l2_mse[i] <- mean((pred_dep_l2 - data_test$depression_score)^2)
  
  
  # Nonlinear
  dep_nonlin_1 <-lm(depression_score ~ poly(Q11A, 3) + Q27A + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 2) + poly(Q40A, 3) + poly(Q10A, 3) + 
             poly(Q13A, 2) + poly(Q21A, 3) + bs(TIPI4, df = 5), data_tr)
  dep_nonlin_1_mse[i] <- mean((predict(dep_nonlin_1, data_test) -
                                 data_test$depression_score)^2)
  
  dep_nonlin_2 <- lm(depression_score ~ Q11A + Q27A + Q29A + poly(Q9A, 3) +
             Q28A + poly(Q40A, 3) + Q10A + Q13A + poly(Q21A, 3) +
             bs(TIPI4, df = 5), data_tr)
  dep_nonlin_2_mse[i] <- mean((predict(dep_nonlin_2, data_test) - 
                                 data_test$depression_score)^2)
  
  dep_nonlin_3 <- lm(depression_score ~ Q11A + Q27A + Q29A + poly(Q9A, 3) +
             Q28A + poly(Q40A, 3) + Q10A + Q13A + poly(Q21A, 3) +
             ns(TIPI4, df = 5), data_tr)
  dep_nonlin_3_mse[i] <- mean((predict(dep_nonlin_3, data_test) - 
                                 data_test$depression_score)^2)
  
  dep_nonlin_4 <- lm(depression_score ~ Q11A + Q27A + Q29A + poly(Q9A, 3) +
             Q28A + poly(Q40A, 3) + Q10A + Q13A + poly(Q21A, 3) +
             poly(TIPI4, 5), data_tr)
  dep_nonlin_4_mse[i] <- mean((predict(dep_nonlin_4, data_test) - 
                                 data_test$depression_score)^2)
  
  # Logistic
  dep_lr_1 <- glm(depression_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, data_tr, family = "binomial")
  dep_lr_1_prob <- predict(dep_lr_1, data_test, type = "response")
  preds_dep_lr_1A <- rep("Low", length(dep_lr_1_prob))
  preds_dep_lr_1A[dep_lr_1_prob < .5] <- "High"
  dep_lr_1A_mce[i] <- mean(preds_dep_lr_1A != data_test$depression_bin)
  dep_lr_1A_fn[i] <- mean(preds_dep_lr_1A == "Low" &
                            data_test$depression_bin == "High")
  preds_dep_lr_1B <- rep("Low", length(dep_lr_1_prob))
  preds_dep_lr_1B[dep_lr_1_prob < .8] <- "High"
  dep_lr_1B_mce[i] <- mean(preds_dep_lr_1B != data_test$depression_bin)
  dep_lr_1B_fn[i] <- mean(preds_dep_lr_1B == "Low" &
                            data_test$depression_bin == "High")
  
  dep_lr_2 <- glm(depression_bin ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 2) + Q29A*TIPI4 + Q28A*TIPI4 + 
               Q10A*TIPI4 + Q13A*TIPI4, data_tr, family = "binomial")
  dep_lr_2_prob <- predict(dep_lr_2, data_test, type = "response")
  preds_dep_lr_2A <- rep("Low", length(dep_lr_2_prob))
  preds_dep_lr_2A[dep_lr_2_prob < .5] <- "High"
  dep_lr_2A_mce[i] <- mean(preds_dep_lr_2A != data_test$depression_bin)
  dep_lr_2A_fn[i] <- mean(preds_dep_lr_2A == "Low" &
                            data_test$depression_bin == "High")
  preds_dep_lr_2B <- rep("Low", length(dep_lr_2_prob))
  preds_dep_lr_2B[dep_lr_2_prob < .8] <- "High"
  dep_lr_2B_mce[i] <- mean(preds_dep_lr_2B != data_test$depression_bin)
  dep_lr_2B_fn[i] <- mean(preds_dep_lr_2B == "Low" &
                            data_test$depression_bin == "High")
  
  dep_lr_3 <- glm(depression_bin ~ poly(Q11A, 3) + Q27A + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 2) + poly(Q40A, 3) + poly(Q10A, 3) + 
             poly(Q13A, 2) + poly(Q21A, 3) + bs(TIPI4, df = 5), data_tr,
             family = "binomial")
  dep_lr_3_prob <- predict(dep_lr_3, data_test, type = "response")
  preds_dep_lr_3A <- rep("Low", length(dep_lr_3_prob))
  preds_dep_lr_3A[dep_lr_3_prob < .5] <- "High"
  dep_lr_3A_mce[i] <- mean(preds_dep_lr_3A != data_test$depression_bin)
  dep_lr_3A_fn[i] <- mean(preds_dep_lr_3A == "Low" &
                            data_test$depression_bin == "High")
  preds_dep_lr_3B <- rep("Low", length(dep_lr_3_prob))
  preds_dep_lr_3B[dep_lr_3_prob < .81] <- "High"
  dep_lr_3B_mce[i] <- mean(preds_dep_lr_3B != data_test$depression_bin)
  dep_lr_3B_fn[i] <- mean(preds_dep_lr_3B == "Low" &
                            data_test$depression_bin == "High")
  
  # KNN
  set.seed(1)
  dep_knn_class <- class::knn(data_tr_X, data_te_X,
                       data_tr$depression_bin, k = 9, prob = T)
  dep_knn_1_mce[i] <- 1 - (sum(diag(table(dep_knn_class, data_test$depression_bin))) /
                           sum(table(dep_knn_class, data_test$depression_bin)))
  dep_knn_1_fn[i] <- mean(dep_knn_class == "Low" & data_test$depression_bin == "High")
  dep_knn_preds <- rep("Low", length(dep_knn_class))
  dep_knn_preds[dep_knn_class == "High"] <- "High"
  dep_knn_preds_2 <- if_else(attr(dep_knn_class, "prob") <= .73,
                             "High", dep_knn_preds)
  dep_knn_2_mce[i] <- 1 - (sum(diag(table(dep_knn_preds_2, data_test$depression_bin))) /
                             sum(table(dep_knn_preds_2, data_test$depression_bin)))
  dep_knn_2_fn[i] <- mean(dep_knn_preds_2 == "Low" & data_test$depression_bin == "High")
  
  set.seed(1)
  dep_knn_reg <- knn.reg(data_tr_X, test = data_te_X,
                         y = data_tr$depression_score, k = 47)
  dep_knn_mse[i] <- mean((dep_knn_reg$pred - data_test$depression_score)^2)
}
```

### Reduced

```{r CV Dep Reduced}
dep_resp_red <- trainSubset$depression_score_reduced
dep_pred_1_red <- model.matrix(depression_score_reduced ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset)[,-1]
dep_pred_2_red <- model.matrix(depression_score_reduced ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 5),
                            trainSubset)[,-1]

dep_lin_1_mse_red <- rep(0, 10)
dep_lin_2_mse_red <- rep(0, 10)

dep_r1_mse_red <- rep(0, 10)
dep_r2_mse_red <- rep(0, 10)
dep_l1_mse_red <- rep(0, 10)
dep_l2_mse_red <- rep(0, 10)

dep_nonlin_1_mse_red <- rep(0, 10)
dep_nonlin_2_mse_red <- rep(0, 10)
dep_nonlin_3_mse_red <- rep(0, 10)
dep_nonlin_4_mse_red <- rep(0, 10)

dep_knn_mse_red <- rep(0, 10)

for(i in 1:10){
  indices <- folds$splits[[i]][[2]]
  correct_indices <- vector("logical", dim(trainSubset)[1])
  for(j in 1:dim(trainSubset)[1]){
    if(j %in% indices){
      correct_indices[j] = T
    } else{
      correct_indices[j] = F
    }
  }
  
  data_tr <- trainSubset[correct_indices,]
  data_test <- trainSubset[!correct_indices,]
  
  data_tr_X <- data_tr %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  data_te_X <- data_test %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  
  
  # Linear
  dep_lin_1_red <- lm(depression_score_reduced ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, data_tr)
  dep_lin_1_mse_red[i] <- mean((predict(dep_lin_1_red, data_test) -
                              data_test$depression_score_reduced)^2)
  dep_lin_2_red <- lm(depression_score_reduced ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 5), data_tr)
  dep_lin_2_mse_red[i] <- mean((predict(dep_lin_2_red, data_test) - 
                             data_test$depression_score_reduced)^2)
  
  # Shrinkage
  set.seed(1)
  dep_r1_red <- glmnet(dep_pred_1_red[correct_indices,], dep_resp_red[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  dep_r1_cv_red <- cv.glmnet(dep_pred_1_red[correct_indices,],
                              dep_resp_red[correct_indices], alpha = 0)
  bestlam_dep_r1_red <- dep_r1_cv_red$lambda.min
  pred_dep_r1_red <- predict(dep_r1_red, s = bestlam_dep_r1_red,
                         newx = dep_pred_1_red[!correct_indices,])
  dep_r1_mse_red[i] <- mean((pred_dep_r1_red - data_test$depression_score_reduced)^2)
  
  set.seed(1)
  dep_r2_red <- glmnet(dep_pred_2_red[correct_indices,], dep_resp_red[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  dep_r2_cv_red <- cv.glmnet(dep_pred_2_red[correct_indices,],
                              dep_resp_red[correct_indices], alpha = 0)
  bestlam_dep_r2_red <- dep_r2_cv_red$lambda.min
  pred_dep_r2_red <- predict(dep_r2_red, s = bestlam_dep_r2_red,
                         newx = dep_pred_2_red[!correct_indices,])
  dep_r2_mse_red[i] <- mean((pred_dep_r2_red - data_test$depression_score_reduced)^2)
  
  
  set.seed(1)
  dep_l1_red <- glmnet(dep_pred_1_red[correct_indices,], dep_resp_red[correct_indices],
                   alpha = 1, lambda = grid)
  dep_l1_cv_red <- cv.glmnet(dep_pred_1_red[correct_indices,],
                              dep_resp_red[correct_indices], alpha = 1)
  bestlam_dep_l1_red <- dep_l1_cv_red$lambda.min
  pred_dep_l1_red <- predict(dep_l1_red, s = bestlam_dep_l1_red,
                         newx = dep_pred_1_red[!correct_indices,])
  dep_l1_mse_red[i] <- mean((pred_dep_l1_red - data_test$depression_score_reduced)^2)
  
  set.seed(1)
  dep_l2_red <- glmnet(dep_pred_2_red[correct_indices,], dep_resp_red[correct_indices],
                   alpha = 1, lambda = grid)
  dep_l2_cv_red <- cv.glmnet(dep_pred_2_red[correct_indices,],
                              dep_resp_red[correct_indices], alpha = 1)
  bestlam_dep_l2_red <- dep_l2_cv_red$lambda.min
  pred_dep_l2_red <- predict(dep_l2_red, s = bestlam_dep_l2_red,
                         newx = dep_pred_2_red[!correct_indices,])
  dep_l2_mse_red[i] <- mean((pred_dep_l2_red - data_test$depression_score_reduced)^2)
  
  
  # Nonlinear
  dep_nonlin_1_red <-lm(depression_score_reduced ~ poly(Q11A, 3) + Q27A + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 2) + poly(Q40A, 3) + poly(Q10A, 3) + 
             poly(Q13A, 2) + poly(Q21A, 3) + bs(TIPI4, df = 5), data_tr)
  dep_nonlin_1_mse_red[i] <- mean((predict(dep_nonlin_1_red, data_test) -
                                 data_test$depression_score_reduced)^2)
  
  dep_nonlin_2_red <- lm(depression_score_reduced ~ Q11A + Q27A + Q29A + poly(Q9A, 3) +
             Q28A + poly(Q40A, 3) + Q10A + Q13A + poly(Q21A, 3) +
             bs(TIPI4, df = 5), data_tr)
  dep_nonlin_2_mse_red[i] <- mean((predict(dep_nonlin_2_red, data_test) - 
                                 data_test$depression_score_reduced)^2)
  
  dep_nonlin_3_red <- lm(depression_score_reduced ~ Q11A + Q27A + Q29A + poly(Q9A, 3) +
             Q28A + poly(Q40A, 3) + Q10A + Q13A + poly(Q21A, 3) +
             ns(TIPI4, df = 5), data_tr)
  dep_nonlin_3_mse_red[i] <- mean((predict(dep_nonlin_3_red, data_test) - 
                                 data_test$depression_score_reduced)^2)
  
  dep_nonlin_4_red <- lm(depression_score_reduced ~ Q11A + Q27A + Q29A + poly(Q9A, 3) +
             Q28A + poly(Q40A, 3) + Q10A + Q13A + poly(Q21A, 3) +
             poly(TIPI4, 5), data_tr)
  dep_nonlin_4_mse_red[i] <- mean((predict(dep_nonlin_4_red, data_test) - 
                                 data_test$depression_score_reduced)^2)
  
  # KNN
  set.seed(1)
  dep_knn_reg_red <- knn.reg(data_tr_X, test = data_te_X,
                         y = data_tr$depression_score_reduced, k = 47)
  dep_knn_mse_red[i] <- mean((dep_knn_reg_red$pred - data_test$depression_score_reduced)^2)
}
```

### Transformed

I have no real reason to run this, so I don't think I will.


```{r CV Amx Trans}
anx_resp_t <- trainSubset$anxiety_trans
anx_pred_1_t <- model.matrix(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + TIPI4,
                            trainSubset)[,-1]
anx_pred_2_t <- model.matrix(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A +
                              Q40A + Q10A + Q13A + Q21A + poly(TIPI4, 5),
                            trainSubset)[,-1]

anx_lin_1_mse_t <- rep(0, 10)
anx_lin_2_mse_t <- rep(0, 10)

anx_r1_mse_t <- rep(0, 10)
anx_r2_mse_t <- rep(0, 10)
anx_l1_mse_t <- rep(0, 10)
anx_l2_mse_t <- rep(0, 10)

anx_nonlin_1_mse_t <- rep(0, 10)
anx_nonlin_2_mse_t <- rep(0, 10)
anx_nonlin_3_mse_t <- rep(0, 10)
anx_nonlin_4_mse_t <- rep(0, 10)

anx_knn_mse_t <- rep(0, 10)

for(i in 1:10){
  indices <- folds$splits[[i]][[2]]
  correct_indices <- vector("logical", dim(trainSubset)[1])
  for(j in 1:dim(trainSubset)[1]){
    if(j %in% indices){
      correct_indices[j] = T
    } else{
      correct_indices[j] = F
    }
  }
  
  data_tr <- trainSubset[correct_indices,]
  data_test <- trainSubset[!correct_indices,]
  
  data_tr_X <- data_tr %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  data_te_X <- data_test %>% 
    select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  
  
  # Linear
  anx_lin_1_t <- lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + TIPI4, data_tr)
  anx_lin_1_mse_t[i] <- mean((predict(anx_lin_1_t, data_test) -
                              data_test$anxiety_trans)^2)
  anx_lin_2_t <- lm(anxiety_trans ~ Q11A + Q27A + Q29A + Q9A + Q28A + Q40A + Q10A +
             Q13A + Q21A + poly(TIPI4, 5), data_tr)
  anx_lin_2_mse_t[i] <- mean((predict(anx_lin_2_t, data_test) - 
                             data_test$anxiety_trans)^2)
  
  # Shrinkage
  set.seed(1)
  anx_r1_t <- glmnet(anx_pred_1_t[correct_indices,], anx_resp_t[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  anx_r1_cv_t <- cv.glmnet(anx_pred_1_t[correct_indices,],
                              anx_resp_t[correct_indices], alpha = 0)
  bestlam_anx_r1_t <- anx_r1_cv_t$lambda.min
  pred_anx_r1_t <- predict(anx_r1_t, s = bestlam_anx_r1_t,
                         newx = anx_pred_1_t[!correct_indices,])
  anx_r1_mse_t[i] <- mean((pred_anx_r1_t - data_test$anxiety_trans)^2)
  
  set.seed(1)
  anx_r2_t <- glmnet(anx_pred_2_t[correct_indices,], anx_resp_t[correct_indices],
                   alpha = 0, lambda = grid, thresh = 1e-12)
  anx_r2_cv_t <- cv.glmnet(anx_pred_2_t[correct_indices,],
                              anx_resp_t[correct_indices], alpha = 0)
  bestlam_anx_r2_t <- anx_r2_cv_t$lambda.min
  pred_anx_r2_t <- predict(anx_r2_t, s = bestlam_anx_r2_t,
                         newx = anx_pred_2_t[!correct_indices,])
  anx_r2_mse_t[i] <- mean((pred_anx_r2_t - data_test$anxiety_trans)^2)
  
  
  set.seed(1)
  anx_l1_t <- glmnet(anx_pred_1_t[correct_indices,], anx_resp_t[correct_indices],
                   alpha = 1, lambda = grid)
  anx_l1_cv_t <- cv.glmnet(anx_pred_1_t[correct_indices,],
                              anx_resp_t[correct_indices], alpha = 1)
  bestlam_anx_l1_t <- anx_l1_cv_t$lambda.min
  pred_anx_l1_t <- predict(anx_l1_t, s = bestlam_anx_l1_t,
                         newx = anx_pred_1_t[!correct_indices,])
  anx_l1_mse_t[i] <- mean((pred_anx_l1_t - data_test$anxiety_trans)^2)
  
  set.seed(1)
  anx_l2_t <- glmnet(anx_pred_2_t[correct_indices,], anx_resp_t[correct_indices],
                   alpha = 1, lambda = grid)
  anx_l2_cv_t <- cv.glmnet(anx_pred_2_t[correct_indices,],
                              anx_resp_t[correct_indices], alpha = 1)
  bestlam_anx_l2_t <- anx_l2_cv_t$lambda.min
  pred_anx_l2_t <- predict(anx_l2_t, s = bestlam_anx_l2,
                         newx = anx_pred_2_t[!correct_indices,])
  anx_l2_mse_t[i] <- mean((pred_anx_l2_t - data_test$anxiety_trans)^2)
  
  
  # Nonlinear
  anx_nonlin_1_t <- lm(anxiety_trans ~ poly(Q11A, 2) + poly(Q27A, 2) +
                               poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 3) + poly(Q40A, 2) + poly(Q10A, 3) + 
             poly(Q13A, 3) + poly(Q21A, 3) + bs(TIPI4, df = 5), data_tr)
  anx_nonlin_1_mse_t[i] <- mean((predict(anx_nonlin_1_t, data_test) -
                                 data_test$anxiety_trans)^2)
  
  anx_nonlin_2_t <- lm(anxiety_trans ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) +
                       poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             bs(TIPI4, df = 5), data_tr)
  anx_nonlin_2_mse_t[i] <- mean((predict(anx_nonlin_2_t, data_test) - 
                                 data_test$anxiety_trans)^2)
  
  anx_nonlin_3_t <- lm(anxiety_trans ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) +
                       poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             ns(TIPI4, df = 5), data_tr)
  anx_nonlin_3_mse_t[i] <- mean((predict(anx_nonlin_3_t, data_test) - 
                                 data_test$anxiety_trans)^2)
  
  anx_nonlin_4_t <- lm(anxiety_trans ~ poly(Q11A, 2) + Q27A + poly(Q29A, 2) + poly(Q9A, 2) +
             poly(Q28A, 2) + Q40A + Q10A + poly(Q13A, 2) + poly(Q21A, 2) +
             poly(TIPI4, 3), data_tr)
  anx_nonlin_4_mse_t[i] <- mean((predict(anx_nonlin_4_t, data_test) - 
                                 data_test$anxiety_trans)^2)
  
  # KNN
  set.seed(1)
  anx_knn_reg_t <- knn.reg(data_tr_X, test = data_te_X,
                         y = data_tr$anxiety_trans, k = 45)
  anx_knn_mse_t[i] <- mean((anx_knn_reg_t$pred - data_test$anxiety_trans)^2)
}
```

### Output

It's impossible to properly recover MSE of transformed variable to compare - we will leave it out for stress & depression (showed minimal evidence of a useful transformation), but will still have to consider through other methods for anxiety (showed real evidence).

```{r CV Dep Output}
get_se <- function(mse){
  ret = 0
  for(i in 1:10){
    ret = ret + (mse[i] - mean(mse))^2
  }
  return(sqrt(ret / 9))
}

dep_mse <- data.frame(t(c(Model = "dep_lin_1", MSE = mean(dep_lin_1_mse),
                          SD = get_se(dep_lin_1_mse),
                          MSE_red = mean(dep_lin_1_mse_red),
                          SD_red = get_se(dep_lin_1_mse_red))))
dep_mse <- rbind(dep_mse,
                 c("dep_lin_2", mean(dep_lin_2_mse), get_se(dep_lin_2_mse),
                   mean(dep_lin_2_mse_red), get_se(dep_lin_2_mse_red)),
                 c("dep_r1", mean(dep_r1_mse), get_se(dep_r1_mse),
                   mean(dep_r1_mse_red), get_se(dep_r1_mse_red)),
                 c("dep_r2", mean(dep_r2_mse), get_se(dep_r2_mse),
                   mean(dep_r2_mse_red), get_se(dep_r2_mse_red)),
                 c("dep_l1", mean(dep_l1_mse), get_se(dep_l1_mse),
                   mean(dep_l1_mse_red), get_se(dep_l1_mse_red)),
                 c("dep_l2", mean(dep_l2_mse), get_se(dep_l2_mse),
                   mean(dep_l2_mse_red), get_se(dep_l2_mse_red)),
                 c("dep_nonlin_1", mean(dep_nonlin_1_mse),
                   get_se(dep_nonlin_1_mse), mean(dep_nonlin_1_mse_red),
                   get_se(dep_nonlin_1_mse_red)),
                 c("dep_nonlin_2", mean(dep_nonlin_2_mse),
                   get_se(dep_nonlin_2_mse), mean(dep_nonlin_2_mse_red),
                   get_se(dep_nonlin_2_mse_red)),
                 c("dep_nonlin_3", mean(dep_nonlin_3_mse),
                   get_se(dep_nonlin_3_mse), mean(dep_nonlin_3_mse_red),
                   get_se(dep_nonlin_3_mse_red)),
                 c("dep_nonlin_4", mean(dep_nonlin_4_mse),
                   get_se(dep_nonlin_4_mse), mean(dep_nonlin_4_mse_red),
                   get_se(dep_nonlin_4_mse_red)),
                 c("dep_knn_reg", mean(dep_knn_mse), get_se(dep_knn_mse),
                   mean(dep_knn_mse_red), get_se(dep_knn_mse_red)))

dep_mse %>% 
  arrange(MSE, MSE_red) %>% 
  knitr::kable(digits = 3)

dep_mse %>% 
  mutate(sd_1 = as.numeric(MSE) + as.numeric(SD)) %>% 
  arrange(sd_1) # Actually flip-flops best (nonlin 2) and third-best (nonlin 4)



dep_mce <- data.frame(t(c(Model = "dep_lr_1A", MCE = mean(dep_lr_1A_mce),
                          SD = get_se(dep_lr_1A_mce), FN = mean(dep_lr_1A_fn),
                          FN_SD = get_se(dep_lr_1A_fn))))
dep_mce <- rbind(dep_mce,
                 c("dep_lr_1B", mean(dep_lr_1B_mce), get_se(dep_lr_1B_mce),
                   mean(dep_lr_1B_fn), get_se(dep_lr_1B_fn)),
                 c("dep_lr_2A", mean(dep_lr_2A_mce), get_se(dep_lr_2A_mce),
                   mean(dep_lr_2A_fn), get_se(dep_lr_2A_fn)),
                 c("dep_lr_2B", mean(dep_lr_2B_mce), get_se(dep_lr_2B_mce),
                   mean(dep_lr_2B_fn), get_se(dep_lr_2B_fn)),
                 c("dep_lr_3A", mean(dep_lr_3A_mce), get_se(dep_lr_3A_mce),
                   mean(dep_lr_3A_fn), get_se(dep_lr_3A_fn)),
                 c("dep_lr_3B", mean(dep_lr_3B_mce), get_se(dep_lr_3B_mce),
                   mean(dep_lr_3B_fn), get_se(dep_lr_3B_fn)),
                 c("dep_knn_1", mean(dep_knn_1_mce), get_se(dep_knn_1_mce),
                   mean(dep_knn_1_fn), get_se(dep_knn_1_fn)),
                 c("dep_knn_2", mean(dep_knn_2_mce), get_se(dep_knn_2_mce),
                   mean(dep_knn_2_fn), get_se(dep_knn_2_fn)))

dep_mce %>% 
  arrange(FN, MCE) %>% 
  knitr::kable(digits = 3) # KNN doesn't keep up here
```

### Comparison on New Data

The best way I can think of to compare the tree and non-tree error rates (even though I wanted to avoid this) is to test their predictions on new data. I will only compare the best 1-2 non-tree models to the tree models.

```{r New Data Dep}
dep_nonlin_4_test <- lm(depression_score ~ Q11A + Q27A + Q29A + poly(Q9A, 3) +
             Q28A + poly(Q40A, 3) + Q10A + Q13A + poly(Q21A, 3) +
             poly(TIPI4, 5), trainData)
dep_nonlin_4_mse_test <- mean((predict(dep_nonlin_4_test, testData) - 
                                 testData$depression_score)^2) # 14.310

dep_nonlin_4_test_red <- lm(depression_score_reduced ~ Q11A + Q27A + Q29A + poly(Q9A, 3) +
             Q28A + poly(Q40A, 3) + Q10A + Q13A + poly(Q21A, 3) +
             poly(TIPI4, 5), trainData)
dep_nonlin_4_mse_test_red <- mean((predict(dep_nonlin_4_test_red, testData) - 
                                 testData$depression_score_reduced)^2) # 14.310
  
  
dep_nonlin_2_test <- lm(depression_score ~ Q11A + Q27A + Q29A + poly(Q9A, 3) +
             Q28A + poly(Q40A, 3) + Q10A + Q13A + poly(Q21A, 3) +
             bs(TIPI4, df = 5), trainData)
dep_nonlin_2_mse_test <- mean((predict(dep_nonlin_2_test, testData) - 
                                 testData$depression_score)^2) # 14.310

dep_nonlin_2_test_red <- lm(depression_score_reduced ~ Q11A + Q27A + Q29A + poly(Q9A, 3) +
             Q28A + poly(Q40A, 3) + Q10A + Q13A + poly(Q21A, 3) +
             bs(TIPI4, df = 5), trainData)
dep_nonlin_2_mse_test_red <- mean((predict(dep_nonlin_2_test_red, testData) - 
                                 testData$depression_score_reduced)^2) # 14.310




dep_bag_test_mse <- mean((predict(dep_bag, testData) - testData$depression_score)^2) # 16.189

dep_bag_red_test_mse <- mean((predict(dep_bag_red, testData) -
                                testData$depression_score_reduced)^2) # 16.167

dep_rf_test_mse <- mean((predict(dep_rf, testData) - testData$depression_score)^2) # 15.147

dep_rf_red_test_mse <- mean((predict(dep_rf_red, testData) -
                               testData$depression_score_reduced)^2) # 15.122

# Literally zero evidence that these are outperforming


trainData_X <- trainSubset %>% 
  select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)

testData_X <- testData %>% 
  select(Q11A, Q27A, Q29A, Q9A, Q28A, Q40A, Q10A, Q13A, Q21A, TIPI4)
  
dep_lr_3_test <- glm(depression_bin ~ poly(Q11A, 3) + Q27A + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 2) + poly(Q40A, 3) + poly(Q10A, 3) + 
             poly(Q13A, 2) + poly(Q21A, 3) + bs(TIPI4, df = 5), trainData,
             family = "binomial")
dep_lr_3_prob_test <- predict(dep_lr_3_test, testData, type = "response")
preds_dep_lr_3B_test <- rep("Low", length(dep_lr_3_prob_test))
preds_dep_lr_3B_test[dep_lr_3_prob_test < .81] <- "High"
dep_lr_3B_mce_test <- mean(preds_dep_lr_3B_test != testData$depression_bin) # 9.694%
dep_lr_3B_fn_test <- mean(preds_dep_lr_3B_test == "Low" &
                            testData$depression_bin == "High") # 0.986% (yes, less than 1)

dep_bag_bin_pred_test <- predict(dep_bag_bin, testData)
dep_bag_bin_mce_test <- mean(dep_bag_bin_pred_test != testData$depression_bin) # 9.121%
dep_bag_bin_fn_test <- mean(dep_bag_bin_pred_test == "Low" & 
                              testData$depression_bin == "High") # 4.153%

dep_rf_bin_pred_test <- predict(dep_rf_bin, testData)
dep_rf_bin_mce_test <- mean(dep_rf_bin_pred_test != testData$depression_bin) # 8.296%, this is almost as good as the LR models that don't optimize for FN
dep_rf_bin_fn_test <- mean(dep_rf_bin_pred_test == "Low" &
                             testData$depression_bin == "High") # 3.751

set.seed(1)
class_weights <- c(100, 1)
dep_rf_bin_test <- randomForest(depression_bin ~ Q11A + Q27A + Q29A + Q9A +
                                  Q28A + Q40A + Q10A + Q13A + Q21A + TIPI4,
                                trainSubset, mtry = 4, importance = T,
                                ntree = 800, classwt = class_weights)

dep_rf_bin_test_pred <- predict(dep_rf_bin_test, testData)
dep_rf_bin_test_mce <- mean(dep_rf_bin_test_pred != testData$depression_bin) # 9.393%
dep_rf_bin_test_fn <- mean(dep_rf_bin_test_pred == "Low" &
                             testData$depression_bin == "High") # 3.007%
# It is just not possible to drive down the false negative rate as effectively in a RF model - and it doesn't outperform by too much on an even level.
```

The forests do almost (but not quite) as well here - sticking with the simpler models.


### Selection

Since two models are almost exactly a wash, but cubic splines are slightly more stable than quintic polynomials (even though, in this situation, I am not worried about the end behavior of my predictor since the input values are constrained), we will select `dep_nonlin_2` as the regression model, and keep `dep_lr_3B` with a threshold at .81 as our classification model.

```{r Dep Final}
dep_final_reg <- lm(depression_score ~ Q11A + Q27A + Q29A + poly(Q9A, 3) +
             Q28A + poly(Q40A, 3) + Q10A + Q13A + poly(Q21A, 3) +
             bs(TIPI4, df = 5), trainData)
summary(dep_final_reg) # The only insignificant predictors are the 2nd order of Q21A, and some of the higher order effects of TIPI4
# Residual SE = 3.801
# Rsq = adj rsq = 0.9047
# F(20, 29810) = 14150
plot(dep_final_reg) # Slight trends in residuals vs fitted but overall not too bad
# Q-Q is okay, not the best I've seen
# Scale-location is fine - still weird patterns
# Nothing with concerning leverage
save(dep_final_reg, file = "Depression Regression Model.RData")

dep_final_reg_red <- lm(depression_score_reduced ~ Q11A + Q27A + Q29A +
                          poly(Q9A, 3) + Q28A + poly(Q40A, 3) + Q10A + Q13A +
                          poly(Q21A, 3) + bs(TIPI4, df = 5), trainData)
summary(dep_final_reg_red)# Almost identical specification
# Rsq = 0.8417, adj rsq = 0.8416
# F(20, 29810) = 7923, p < .001
plot(dep_final_reg_red) # Definite trends in residuals vs fitted
# Q-Q, scale-location, leverage plots all the same
save(dep_final_reg_red, file = "Depression Reduced Regression Model.RData")

dep_final_class <- glm(depression_bin ~ poly(Q11A, 3) + Q27A + poly(Q29A, 2) +
             poly(Q9A, 3) + poly(Q28A, 2) + poly(Q40A, 3) + poly(Q10A, 3) + 
             poly(Q13A, 2) + poly(Q21A, 3) + bs(TIPI4, df = 5), trainData,
             family = "binomial")
# Thresholded at .81
summary(dep_final_class) # Much more sparse model
save(dep_final_class, file = "Depression Classification Model.RData")

dep_lr_3_prob_test <- predict(dep_lr_3_test, testData, type = "response")
preds_dep_lr_3B_test <- rep("Low", length(dep_lr_3_prob_test))
preds_dep_lr_3B_test[dep_lr_3_prob_test < .81] <- "High"
```


